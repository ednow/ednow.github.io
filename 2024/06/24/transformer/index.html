<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.3.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"ednow.github.io",root:"/",scheme:"Muse",version:"7.8.0",exturl:!1,sidebar:{position:"left",width:400,display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="名词解释Dominant sequence transduction models“Dominant sequence transduction models” 这个短语可以分解为几个单词来解释：  Dominant - 这个词意味着”占主导地位的”或”支配的”，在技术或学术领域中通常用来描述那些在某个领域或问题上表现最好或最常用的方法或模型。  Sequence - 这个词指的是一系列事物的排列"><meta property="og:type" content="article"><meta property="og:title" content="transformer"><meta property="og:url" content="http://ednow.github.io/2024/06/24/transformer/index.html"><meta property="og:site_name" content="ednow"><meta property="og:description" content="名词解释Dominant sequence transduction models“Dominant sequence transduction models” 这个短语可以分解为几个单词来解释：  Dominant - 这个词意味着”占主导地位的”或”支配的”，在技术或学术领域中通常用来描述那些在某个领域或问题上表现最好或最常用的方法或模型。  Sequence - 这个词指的是一系列事物的排列"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2024-06-24T15:59:58.000Z"><meta property="article:modified_time" content="2024-09-26T11:57:18.000Z"><meta property="article:author" content="ednow"><meta name="twitter:card" content="summary"><link rel="canonical" href="http://ednow.github.io/2024/06/24/transformer/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>transformer | ednow</title><script async src="https://www.googletagmanager.com/gtag/js?id=G-XQGJ63ZD9Y"></script><script>function gtag(){dataLayer.push(arguments)}CONFIG.hostname===location.hostname&&(window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-XQGJ63ZD9Y"))</script><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script"),t=(e.src="https://hm.baidu.com/hm.js?83f04257c97e81cca692d7c4c7fbbc9a",document.getElementsByTagName("script")[0]);t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">ednow</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://ednow.github.io/2024/06/24/transformer/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="ednow"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="ednow"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">transformer</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2024-06-24 15:59:58" itemprop="dateCreated datePublished" datetime="2024-06-24T15:59:58+00:00">2024-06-24</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2024-09-26 11:57:18" itemprop="dateModified" datetime="2024-09-26T11:57:18+00:00">2024-09-26</time> </span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>11k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>10 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="名词解释"><a href="#名词解释" class="headerlink" title="名词解释"></a>名词解释</h1><h2 id="Dominant-sequence-transduction-models"><a href="#Dominant-sequence-transduction-models" class="headerlink" title="Dominant sequence transduction models"></a>Dominant sequence transduction models</h2><p>“Dominant sequence transduction models” 这个短语可以分解为几个单词来解释：</p><ol><li><p><strong>Dominant</strong> - 这个词意味着”占主导地位的”或”支配的”，在技术或学术领域中通常用来描述那些在某个领域或问题上表现最好或最常用的方法或模型。</p></li><li><p><strong>Sequence</strong> - 这个词指的是一系列事物的排列或顺序，可以是时间上的连续，也可以是空间上的排列。</p></li><li><p><strong>Transduction</strong> - 这个词在机器学习和信号处理领域中，指的是将一种形式的信号转换成另一种形式的过程。在自然语言处理（NLP）中，”transduction”通常指的是将输入序列（如文本）转换成输出序列（如翻译后的文本）的过程。</p></li><li><p><strong>Models</strong> - 这个词在这里指的是用于执行特定任务或解决特定问题的数学或计算框架，它们可以基于数据进行训练以执行预测或分类等任务。</p></li></ol><p>将这些单词组合起来，”dominant sequence transduction models” 指的是在序列转换任务中表现最好或最常用的模型。这些模型能够处理如语言翻译、文本摘要、语音识别等任务，将输入序列转换为具有特定意义的输出序列。在自然语言处理的背景下，这通常涉及到将一种语言的文本转换成另一种语言的文本，或者将口头语言转换成书面文本等。</p><h2 id="Complex-recurrent"><a href="#Complex-recurrent" class="headerlink" title="Complex recurrent"></a>Complex recurrent</h2><p>“Complex recurrent” 这个短语同样可以分解为两个单词来解释：</p><ol><li><p><strong>Complex</strong> - 这个词在这里可以有几种含义，取决于上下文。它可以指”复杂的”，意味着由许多部分组成，具有复杂的结构或过程。在技术领域，它还可以指涉及复数（数学中的复数，包含实部和虚部）的系统或过程。</p></li><li><p><strong>Recurrent</strong> - 这个词来源于”recur”，意味着”再次发生”或”重复出现”。在神经网络的背景下，”recurrent”通常与”递归”有关，指的是网络中的循环或反馈机制，使得网络能够在处理数据时考虑先前的信息或状态。</p></li></ol><p>结合这两个词，”complex recurrent”可以理解为具有复杂反馈机制或循环结构的系统或模型。在神经网络领域，特别是在处理序列数据时，”complex recurrent”可能指的是那些具有复杂内部状态和反馈连接的递归神经网络（RNNs），它们能够处理时间序列数据中的动态特征和长期依赖关系。这些网络通过在时间上展开，允许信息在网络的不同层之间传递，从而捕获序列数据中的复杂模式。</p><h2 id="28-4-BLEU是什么"><a href="#28-4-BLEU是什么" class="headerlink" title="28.4 BLEU是什么"></a>28.4 BLEU是什么</h2><p>28.4 BLEU 是一个评估机器翻译质量的指标值，具体来说，它是在 WMT 2014（Workshop on Machine Translation）的英语到德语翻译任务中，Transformer 模型取得的 BLEU 分数。BLEU（Bilingual Evaluation Understudy）是一种基于短句重叠的评估方法，用于衡量机器翻译输出与一组参考翻译之间的相似度。</p><p>BLEU 分数的计算通常涉及以下几个步骤：</p><ol><li><p><strong>短句匹配（N-gram Precision）</strong>：BLEU 比较机器翻译输出与参考翻译中的 n-gram（连续的 n 个词）的匹配程度。n-gram 可以是 1-gram（单个词），2-gram（两个词的短语），依此类推。</p></li><li><p><strong>短句权重</strong>：在计算过程中，通常会给较短的 n-gram 更高的权重，因为较短的短句匹配可能更容易实现，而较长的 n-gram 匹配更能体现翻译的质量。</p></li><li><p><strong>短句修正</strong>：为了鼓励较短的翻译输出，BLEU 分数会根据机器翻译输出的长度进行修正。如果机器翻译输出比参考翻译短，那么其 BLEU 分数会受到惩罚。</p></li><li><p><strong>短句计数</strong>：计算每种长度的 n-gram 在机器翻译输出和参考翻译中的出现次数，然后取两者的最小值作为匹配的 n-gram 数。</p></li><li><p><strong>分数计算</strong>：BLEU 分数是所有考虑的 n-gram 长度的几何平均值，然后乘以一个长度修正因子。</p></li></ol><p>最终得到的 BLEU 分数是一个介于 0 到 1 之间的值，较高的 BLEU 分数表示机器翻译的质量更接近人类翻译。在机器翻译的研究和实践中，BLEU 分数是一个非常重要的评估指标。在上述论文中，Transformer 模型的 28.4 BLEU 分数表明它在该任务上取得了非常优秀的性能。</p><h2 id="codebase是什么意思"><a href="#codebase是什么意思" class="headerlink" title="codebase是什么意思"></a>codebase是什么意思</h2><p>“Codebase”是一个术语，用于描述构成软件应用程序或系统的整个代码集合。它包括所有的源代码文件、库、框架、脚本以及可能的配置文件等，这些元素共同构成了软件的运行时环境。简而言之，codebase就是软件项目中所有代码的总和。</p><p>在软件开发中，codebase可以指：</p><ul><li>一个单一的代码库，其中包含了所有的开发文件。</li><li>一个项目的版本控制系统中的所有文件，比如使用Git管理的项目。</li><li>一个由多个开发者共同工作和维护的代码集合。</li></ul><p>在提供的文档摘要中，提到了codebase可能是在讨论软件项目的开发过程，其中Llion负责了初始的codebase，并且进行了高效的推理和可视化。这表明Llion在项目的早期阶段参与了基础代码的编写，并为后续的开发工作奠定了基础。随着项目的发展，可能还会有其他的开发者参与进来，对codebase进行扩展和改进。</p><h2 id="efficient-inference-and-visualizations-这里的inference-是什么意思"><a href="#efficient-inference-and-visualizations-这里的inference-是什么意思" class="headerlink" title="efficient inference and visualizations.这里的inference 是什么意思"></a>efficient inference and visualizations.这里的inference 是什么意思</h2><p>在计算机科学和人工智能领域，”inference”（推理）通常指的是从已知信息或数据中推导出新信息或结论的过程。在机器学习和深度学习的背景下，”efficient inference”（高效推理）特别指的是在模型训练完成后，使用该模型对新的数据进行预测或分类的过程，同时注重推理过程的效率和速度。</p><p>具体来说，”efficient inference”可能包括以下几个方面：</p><ol><li><strong>快速响应</strong>：模型能够迅速对输入数据做出预测。</li><li><strong>资源优化</strong>：在推理过程中，尽可能减少对计算资源（如CPU、GPU）的需求。</li><li><strong>低延迟</strong>：减少从接收输入到输出结果的时间延迟。</li><li><strong>高吞吐量</strong>：在单位时间内处理更多的数据请求。</li></ol><p>在提供的文档摘要中，提到了”efficient inference and visualizations”（高效的推理和可视化），这可能意味着在Transformer模型的开发过程中，除了关注模型的训练效果，还注重了模型在实际应用中的推理效率，以及如何将模型的输出结果以直观的方式展示给用户，从而提高用户体验和模型的实用性。</p><h2 id="implementing-tensor2tensor-replacing-our-earlier-codebase。这里的tensor2tensor是什么"><a href="#implementing-tensor2tensor-replacing-our-earlier-codebase。这里的tensor2tensor是什么" class="headerlink" title="implementing tensor2tensor, replacing our earlier codebase。这里的tensor2tensor是什么"></a>implementing tensor2tensor, replacing our earlier codebase。这里的tensor2tensor是什么</h2><p>“Tensor2Tensor”（简称 T2T）是一个由Google开发的开源库，用于机器学习模型的快速研究和开发，特别是在序列到序列（seq2seq）学习领域。Tensor2Tensor使用TensorFlow作为其底层计算框架，它提供了一种统一的方式来处理各种类型的序列转换任务，例如机器翻译、文本摘要、语音识别等。</p><p>Tensor2Tensor库的特点包括：</p><ol><li><p><strong>模块化设计</strong>：T2T将不同的组件（如模型层、优化器、学习率调度器等）设计成模块化，使得研究人员可以轻松地交换和组合这些组件，以适应不同的任务需求。</p></li><li><p><strong>多种模型架构</strong>：库中实现了多种先进的模型架构，包括但不限于Transformer、LSTM、GRU等，用于各种序列转换任务。</p></li><li><p><strong>易于扩展</strong>：研究人员可以利用T2T提供的接口轻松地实现自己的模型架构或自定义组件。</p></li><li><p><strong>预训练模型和数据集</strong>：T2T提供了一些预训练模型和常用的数据集，方便研究人员快速开始实验和基准测试。</p></li><li><p><strong>训练和推理工具</strong>：库中包含了训练和推理的工具，使得模型训练和部署变得更加高效。</p></li></ol><p>在提供的文档摘要中，提到了”implementing tensor2tensor, replacing our earlier codebase”，这意味着在开发Transformer模型的过程中，开发者们实现了Tensor2Tensor库，并且用它来替换了他们之前使用的代码基础。这可能是为了利用Tensor2Tensor提供的灵活性和效率，以便于更好地进行模型的实验和迭代。</p><h2 id="Noam-proposed-scaled-dot-product-attention-这里的scale指什么"><a href="#Noam-proposed-scaled-dot-product-attention-这里的scale指什么" class="headerlink" title="Noam proposed scaled dot-product attention,这里的scale指什么"></a>Noam proposed scaled dot-product attention,这里的scale指什么</h2><p>在论文 “Attention Is All You Need” 中提到的 “scaled dot-product attention” 中的 “scale” 指的是对点积（dot-product）操作的结果进行缩放（scaling down）的过程。这种缩放是为了改善模型在计算注意力权重时的稳定性和效果。</p><p>具体来说，在计算注意力权重时，模型会计算查询（query）向量与键（key）向量之间的点积。点积的结果可能会非常大，特别是当向量的维度（dimensionality）较高时。如果这些值过大，那么应用 softmax 函数时可能会导致数值计算问题，因为 softmax 函数在输入值非常大或非常小的情况下可能会导致梯度非常小，从而影响模型的学习效率。</p><p>为了解决这个问题，论文中提出了一种缩放点积的方法，即在计算点积之前，先将查询和键向量的点积结果除以键向量维度（dk）的平方根。数学上，这种缩放可以表示为：</p><p>$$\text{Attention}(Q, K, V) = \text{softmax}\left(\frac{QK^T}{\sqrt{dk}}\right)V$$</p><p>其中：</p><ul><li>$Q$ 是查询矩阵。</li><li>$K$ 是键矩阵。</li><li>$V$ 是值矩阵。</li><li>$dk$ 是键向量的维度。</li><li>$\sqrt{dk}$ 是对键向量维度的平方根进行缩放。</li></ul><p>这种缩放操作有助于防止 softmax 函数在处理非常大或非常小的数值时进入梯度很小的区域，从而保持了梯度的稳定性，使得模型可以更有效地学习。这也是 Transformer 模型中的一个关键创新点，有助于提高模型在处理长距离依赖时的性能。</p><h2 id="dispensing-with-recurrence-and-convolutions-entirely。这里的dispensing-是什么意思"><a href="#dispensing-with-recurrence-and-convolutions-entirely。这里的dispensing-是什么意思" class="headerlink" title="dispensing with recurrence and convolutions entirely。这里的dispensing 是什么意思"></a>dispensing with recurrence and convolutions entirely。这里的dispensing 是什么意思</h2><p>在句子 “dispensing with recurrence and convolutions entirely” 中，”dispensing” 通常意味着“不再使用”或“完全摒弃”。在这里，它用来描述 Transformer 模型的一个关键特点：它不依赖于传统的递归（recurrence）或卷积（convolutions）机制。</p><ul><li><strong>Recurrence</strong> 在这里指的是递归神经网络（RNNs），它们通过递归地应用相同的权重来处理序列数据中的每个元素，从而捕获序列中的动态特征。</li><li><strong>Convolutions</strong> 指的是卷积神经网络（CNNs），它们使用卷积层来处理数据，通常用于图像处理任务，但也可以用来处理序列数据。</li></ul><p>“Dispensing with” 这个短语表明 Transformer 模型采用了一种全新的方法来处理序列数据，即完全基于注意力机制（attention mechanisms）。这种方法避免了传统RNNs和CNNs在处理长序列时的局限性，如难以并行化处理和对长距离依赖的捕捉能力有限。通过摒弃这些传统方法，Transformer 模型能够实现更高的并行度和灵活性，从而在某些任务上取得了更好的性能。</p><h2 id="improving-over-the-existing-best-results-including-ensembles-这里的ensemble是什么"><a href="#improving-over-the-existing-best-results-including-ensembles-这里的ensemble是什么" class="headerlink" title="improving over the existing best results, including ensembles,这里的ensemble是什么"></a>improving over the existing best results, including ensembles,这里的ensemble是什么</h2><p>在机器学习和人工智能领域，”ensemble”（集成）通常指的是集成学习方法，这是一种将多个模型结合起来以提高整体性能的策略。集成学习可以减少模型的过拟合，提高泛化能力，并在某些情况下提高预测的准确性。</p><p>在上下文中提到的 “ensembles” 指的是将多个不同的机器学习模型或相同类型的多个实例结合起来，形成一个单一的、更强大的模型。这些模型可以是独立的，也可以是多样化的，它们共同对同一个问题进行预测，然后通过某种形式的投票、平均或其他组合规则来产生最终的预测结果。</p><p>例如，在机器翻译任务中，可能会训练多个不同的神经网络模型，然后将它们的结果结合起来，以期望最终的翻译质量比任何一个单独的模型都要好。这种方法可以利用每个模型的优点，减少单个模型的弱点，从而在整体性能上取得提升。</p><p>因此，当文档中提到 “improving over the existing best results, including ensembles”，这意味着 Transformer 模型在某些任务上的性能不仅超过了单个最佳模型，而且还超过了通过集成多个模型得到的集成模型的性能。这显示了 Transformer 模型在这些任务上的显著优势。</p><h2 id="a-small-fraction-of-the-training-costs-of-the-best-models-from-the-literature-这里的literature是什么意思"><a href="#a-small-fraction-of-the-training-costs-of-the-best-models-from-the-literature-这里的literature是什么意思" class="headerlink" title="a small fraction of the training costs of the best models from the literature.这里的literature是什么意思"></a>a small fraction of the training costs of the best models from the literature.这里的literature是什么意思</h2><p>在这句话 “a small fraction of the training costs of the best models from the literature” 中，”literature” 指的是已发表的研究文献、学术论文、技术报告或其他书面作品，通常与特定领域或主题相关。在这里，它特指与机器学习和人工智能，特别是与Transformer模型或相关技术领域内的已发表研究成果。</p><p>当我们说 “the best models from the literature” 时，意味着我们讨论的是在学术文献中报告的最佳性能模型，这些模型可能已经被研究者们广泛认可，并在各种基准测试或实际应用中表现出色。</p><p>因此，整句话的意思是，Transformer模型的训练成本只是文献中报告的最佳模型训练成本的一小部分，这表明Transformer模型不仅在性能上具有竞争力，而且在训练效率上也具有显著优势。</p><h2 id="Recurrent-models-typically-factor-computation-along-the-symbol-positions-of-the-input-and-output-sequences-这里的factor是指什么"><a href="#Recurrent-models-typically-factor-computation-along-the-symbol-positions-of-the-input-and-output-sequences-这里的factor是指什么" class="headerlink" title="Recurrent models typically factor computation along the symbol positions of the input and output sequences.这里的factor是指什么"></a>Recurrent models typically factor computation along the symbol positions of the input and output sequences.这里的factor是指什么</h2><p>在这句话 “Recurrent models typically factor computation along the symbol positions of the input and output sequences.” 中，”factor” 这个词在这里的意思是“分解”或“按因素组织”。更具体地说，它指的是将计算过程分解或分配到输入和输出序列的各个符号位置上。</p><p>在递归神经网络（Recurrent Neural Networks, RNNs）中，这种分解通常意味着网络的计算是按照序列中每个元素（或称为符号）的顺序逐步进行的。每个时间步的计算都依赖于前一个时间步的输出（隐藏状态）和当前时间步的输入。这样，RNNs 能够捕捉序列数据中的时间依赖性。</p><p>具体来说，”factor computation along the symbol positions” 描述了以下过程：</p><ol><li><strong>序列化处理</strong>：RNNs 将计算分解为一系列步骤，每个步骤对应序列中的一个位置。</li><li><strong>逐步更新</strong>：在每个时间步，RNNs 更新隐藏状态，这是基于当前输入和前一时间步的隐藏状态。</li><li><strong>位置依赖性</strong>：每个步骤的计算都依赖于它在序列中的位置，因为隐藏状态携带了之前所有步骤的信息。</li></ol><p>这种按位置分解的计算方式使得RNNs能够处理序列数据中的动态特征和时间依赖性，但同时也导致了它们在处理长序列时的一些局限性，比如训练过程中的梯度消失或梯度爆炸问题，以及难以并行化处理。这也是为什么Transformer模型采用注意力机制来替代传统的递归结构，以克服这些限制。</p><h2 id="This-inherently-sequential-nature-precludes-parallelization-within-training-examples。precludes是什么"><a href="#This-inherently-sequential-nature-precludes-parallelization-within-training-examples。precludes是什么" class="headerlink" title="This inherently sequential nature precludes parallelization within training examples。precludes是什么"></a>This inherently sequential nature precludes parallelization within training examples。precludes是什么</h2><p>“Precludes” 是一个动词，意思是“排除”、“阻止”或“妨碍”某事的发生。在句子 “This inherently sequential nature precludes parallelization within training examples” 中，”precludes” 用来表达由于递归模型的内在序列特性，它们在训练样本内部进行并行化处理是不可能的。</p><p>这里的上下文是讨论递归神经网络（RNNs）的处理方式。由于RNNs在处理序列数据时必须按顺序逐步进行计算（即，每一步的输出依赖于前一步的输出），这种序列化的计算需求本质上排除了同时处理多个训练样本的可能性。换句话说，RNNs在训练时不能利用并行计算的优势，因为每个序列的元素必须按照特定的顺序来处理。</p><p>与之相对的，Transformer 模型通过使用注意力机制代替递归结构，允许模型在训练时进行更多的并行化处理，从而提高了训练效率。</p><h2 id="Recent-work-has-achieved-significant-improvements-in-computational-efficiency-through-factorization-tricks。factorization-tricks是指什么"><a href="#Recent-work-has-achieved-significant-improvements-in-computational-efficiency-through-factorization-tricks。factorization-tricks是指什么" class="headerlink" title="Recent work has achieved significant improvements in computational efficiency through factorization tricks。factorization tricks是指什么"></a>Recent work has achieved significant improvements in computational efficiency through factorization tricks。factorization tricks是指什么</h2><p>在机器学习和深度学习领域，”factorization tricks”（分解技巧）是指一种优化技术，用于简化模型的计算过程，提高效率。这些技巧通常涉及将复杂的计算分解成更小的、更容易并行处理的部分。</p><p>具体来说，”factorization tricks” 可能包括以下几种方法：</p><ol><li><p><strong>矩阵分解</strong>：将一个大矩阵分解为几个较小的矩阵的乘积，这样可以减少计算量和内存需求。</p></li><li><p><strong>稀疏表示</strong>：利用数据的稀疏性质，只存储和处理非零元素，从而减少计算。</p></li><li><p><strong>低秩近似</strong>：用低秩矩阵近似一个高秩矩阵，减少模型参数的数量，简化计算。</p></li><li><p><strong>分层计算</strong>：将计算分解成多个层次或阶段，每个阶段只处理数据的一部分。</p></li><li><p><strong>条件计算</strong>：根据条件执行计算，避免不必要的操作，减少计算量。</p></li></ol><p>在上下文中提到的 “significant improvements in computational efficiency” 可能是指通过这些分解技巧，可以减少模型训练和推理时所需的计算资源，提高模型运行的速度，同时保持或提高模型的性能。这些技巧在处理大规模数据集或复杂模型时尤其有用。</p><h2 id="Attention-mechanisms-have-become-an-integral-part-of-compelling-sequence-modeling-and-transduction-models-in-various-tasks-compelling-是什么"><a href="#Attention-mechanisms-have-become-an-integral-part-of-compelling-sequence-modeling-and-transduction-models-in-various-tasks-compelling-是什么" class="headerlink" title="Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks,compelling 是什么"></a>Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks,compelling 是什么</h2><p>在这句话 “Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks” 中，”compelling” 这个词用来形容某些东西是非常吸引人的、有说服力的或者具有显著效果的。在这里，它被用来修饰 “sequence modeling and transduction models”，意指那些在序列建模和转导任务中表现出色的模型。</p><p>具体来说，”compelling” 在这个上下文中可能指的是：</p><ul><li><strong>效果显著</strong>：这些模型在处理序列数据时能够达到非常好的效果，比如在语言翻译、文本摘要、语音识别等任务中。</li><li><strong>引人注目</strong>：这些模型因其卓越的性能或创新的方法而受到关注。</li><li><strong>有说服力</strong>：它们能够以令人信服的方式解决复杂的序列处理问题。</li></ul><p>“Attention mechanisms”（注意力机制）是这些引人注目的模型中不可或缺的一部分，因为它们允许模型在处理序列时动态地关注输入数据的不同部分，从而提高了模型的性能和适应性。注意力机制在各种序列建模任务中都显示出了其强大的能力，特别是在处理长距离依赖和复杂模式识别方面。</p><h2 id="eschewing"><a href="#eschewing" class="headerlink" title="eschewing"></a>eschewing</h2><p>回避</p><h2 id="arbitrary"><a href="#arbitrary" class="headerlink" title="arbitrary"></a>arbitrary</h2><p>任意，偶然</p><h2 id="In-these-models-the-number-of-operations-required-to-relate-signals-from-two-arbitrary-input-or-output-positions-grows-in-the-distance-between-positions-linearly-for-ConvS2S-and-logarithmically-for-ByteNet-the-distance-between-positions-是指文本的长度吗"><a href="#In-these-models-the-number-of-operations-required-to-relate-signals-from-two-arbitrary-input-or-output-positions-grows-in-the-distance-between-positions-linearly-for-ConvS2S-and-logarithmically-for-ByteNet-the-distance-between-positions-是指文本的长度吗" class="headerlink" title="In these models,the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.the distance between positions,是指文本的长度吗"></a>In these models,the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.the distance between positions,是指文本的长度吗</h2><p>在这句话中，”the distance between positions” 并不直接指代文本的长度，而是指序列中任意两个位置之间的相对距离或间隔。在序列处理模型中，比如卷积序列到序列模型（ConvS2S）或ByteNet，信号或信息在序列的不同位置之间传递时，所需的计算量会随着这些位置之间的距离增加而增加。</p><p>具体来说：</p><ul><li><p>对于 ConvS2S 模型，这种增长是线性的（linearly），意味着如果两个位置之间的距离增加，所需的操作数量将按比例增加。这是因为卷积层通常只能捕捉局部依赖关系，要连接更远的位置就需要多个卷积层的堆叠。</p></li><li><p>对于 ByteNet 模型，这种增长是对数的（logarithmically），意味着所需的操作数量增加得更慢，与位置之间的距离的对数成正比。ByteNet 使用了一种特殊的卷积结构，允许它更有效地捕捉长距离依赖。</p></li></ul><p>这里的关键是，”the distance between positions” 指的是序列中元素之间的间隔，而不是整个序列的长度。在不同的模型中，这个距离如何影响计算复杂度是模型设计和性能的关键因素之一。Transformer 模型通过注意力机制解决了这个问题，使得无论位置之间的距离有多远，计算复杂度都保持不变。</p><h2 id="counteract"><a href="#counteract" class="headerlink" title="counteract"></a>counteract</h2><p>抵消</p><h2 id="Self-attention-sometimes-called-intra-attention。-intra-attention是什么"><a href="#Self-attention-sometimes-called-intra-attention。-intra-attention是什么" class="headerlink" title="Self-attention, sometimes called intra-attention。 intra-attention是什么"></a>Self-attention, sometimes called intra-attention。 intra-attention是什么</h2><p>“Intra-attention” 是 “self-attention” 的另一种说法，它们在神经网络的上下文中指的是同一种机制。这种注意力机制涉及序列内部不同位置之间的关系，以便计算序列的表示。换句话说，它是一种机制，允许序列中的每个元素（或称为“位置”）关注序列中其他所有元素，以获取相关信息。</p><p>在自注意力（self-attention）或内注意力（intra-attention）机制中，每个位置的输出是通过对序列中所有位置的加权求和得到的，其中权重是根据当前位置与序列中其他位置的相关性动态计算的。这种机制使得模型能够捕捉序列内部的长距离依赖关系，无论这些依赖关系在序列中相隔多远。</p><p>自注意力机制的关键特点包括：</p><ol><li><p><strong>并行化</strong>：由于每个位置都可以独立地计算其对其他所有位置的注意力，因此这种机制可以高度并行化，从而提高计算效率。</p></li><li><p><strong>动态权重</strong>：注意力权重是根据输入数据动态计算的，允许模型在不同情况下灵活地关注不同的信息。</p></li><li><p><strong>长距离依赖</strong>：自注意力机制特别擅长捕捉序列中相隔较远的元素之间的关系，这对于许多序列建模任务（如语言翻译、文本摘要等）至关重要。</p></li></ol><p>在 “Attention Is All You Need” 这篇论文中，自注意力机制是 Transformer 模型的核心组成部分，它使得模型在处理序列数据时能够更加灵活和高效。</p><h2 id="At-each-step-the-model-is-auto-regressive-consuming-the-previously-generated-symbols-as-additional-input-when-generating-the-next-auto-regressive是什么意思"><a href="#At-each-step-the-model-is-auto-regressive-consuming-the-previously-generated-symbols-as-additional-input-when-generating-the-next-auto-regressive是什么意思" class="headerlink" title="At each step the model is auto-regressive , consuming the previously generated symbols as additional input when generating the next.auto-regressive是什么意思"></a>At each step the model is auto-regressive , consuming the previously generated symbols as additional input when generating the next.auto-regressive是什么意思</h2><p>“Auto-regressive”（自回归）是时间序列分析和序列生成模型中使用的一个术语，用于描述一种特定类型的模型，这种模型在生成序列中的下一个元素时，依赖于之前已经生成的元素。</p><p>在自回归模型中，每一步的输出不仅依赖于当前的输入，还依赖于模型在之前步骤的输出。换句话说，模型在生成序列的过程中是“自回归”的，因为它使用自身先前生成的信息来预测或生成序列中的下一个元素。</p><p>在上下文中提到的 “At each step the model is auto-regressive, consuming the previously generated symbols as additional input when generating the next” 描述的是：</p><ul><li><strong>自回归特性</strong>：在序列生成的过程中，模型在每一步都会考虑之前生成的符号或元素。</li><li><strong>逐步生成</strong>：序列是逐步构建的，每一步的生成都建立在之前步骤生成的内容之上。</li><li><strong>历史依赖</strong>：模型在生成下一个元素时，会利用到目前为止生成的整个序列的历史信息。</li></ul><p>这种特性在自然语言处理中的文本生成、机器翻译、语音合成等任务中非常重要，因为它允许模型生成连贯和语法正确的序列。例如，在机器翻译中，翻译模型会在生成译文的每个单词时考虑前面已经翻译的单词，以确保翻译的流畅性和准确性。</p><h2 id="The-Transformer-follows-this-overall-architecture-using-stacked-self-attention-and-point-wise-这里的point-wise是什么"><a href="#The-Transformer-follows-this-overall-architecture-using-stacked-self-attention-and-point-wise-这里的point-wise是什么" class="headerlink" title="The Transformer follows this overall architecture using stacked self-attention and point-wise,这里的point-wise是什么"></a>The Transformer follows this overall architecture using stacked self-attention and point-wise,这里的point-wise是什么</h2><p>在提到的 “The Transformer follows this overall architecture using stacked self-attention and point-wise” 这句话中，”point-wise” 通常指的是逐点操作，这是深度学习中的一种常见操作，特别是在卷积神经网络（CNNs）中。在Transformer模型的上下文中，”point-wise” 操作指的是对序列中每个位置的元素分别进行相同的操作。</p><p>具体来说，在Transformer模型中，”point-wise” 操作通常指的是：</p><ol><li><p><strong>逐点全连接层</strong>：每个位置的输入通过一个独立的线性变换（例如，一个全连接层），然后通常会跟一个非线性激活函数（如ReLU）。</p></li><li><p><strong>独立性</strong>：这种操作对序列中的每个位置是独立进行的，即每个位置的输出只依赖于该位置的输入，而与序列中其他位置的输入无关。</p></li><li><p><strong>并行化</strong>：由于每个位置的操作是独立的，这种逐点操作可以很容易地并行化，从而提高计算效率。</p></li></ol><p>在Transformer架构中，逐点操作通常与自注意力机制一起使用，以增强模型的能力。自注意力机制允许模型捕捉序列内部的长距离依赖关系，而逐点全连接层则为模型提供了学习更复杂函数的能力。这种结合使得Transformer模型能够有效地处理序列数据，并在各种序列建模任务中取得了显著的性能。</p><h2 id="为什么匿名内部类只能是final"><a href="#为什么匿名内部类只能是final" class="headerlink" title="为什么匿名内部类只能是final"></a>为什么匿名内部类只能是final</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_44299027/article/details/117333667">参考文献</a></p></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-item"><a href="/2024/04/17/12306%E6%8A%A2%E7%A5%A8/" rel="prev" title="12306抢票"><i class="fa fa-chevron-left"></i> 12306抢票</a></div><div class="post-nav-item"></div></div></footer></article></div><div class="comments" id="gitalk-container"></div><script>window.addEventListener("tabs:register",()=>{let t=CONFIG.comments["activeClass"];var e;(t=CONFIG.comments.storage?localStorage.getItem("comments_active")||t:t)&&(e=document.querySelector(`a[href="#comment-${t}"]`))&&e.click()}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{t.target.matches(".tabs-comment .tab-content .tab-pane")&&(t=t.target.classList[1],localStorage.setItem("comments_active",t))})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E5%90%8D%E8%AF%8D%E8%A7%A3%E9%87%8A"><span class="nav-text">名词解释</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#Dominant-sequence-transduction-models"><span class="nav-text">Dominant sequence transduction models</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Complex-recurrent"><span class="nav-text">Complex recurrent</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#28-4-BLEU%E6%98%AF%E4%BB%80%E4%B9%88"><span class="nav-text">28.4 BLEU是什么</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#codebase%E6%98%AF%E4%BB%80%E4%B9%88%E6%84%8F%E6%80%9D"><span class="nav-text">codebase是什么意思</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#efficient-inference-and-visualizations-%E8%BF%99%E9%87%8C%E7%9A%84inference-%E6%98%AF%E4%BB%80%E4%B9%88%E6%84%8F%E6%80%9D"><span class="nav-text">efficient inference and visualizations.这里的inference 是什么意思</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#implementing-tensor2tensor-replacing-our-earlier-codebase%E3%80%82%E8%BF%99%E9%87%8C%E7%9A%84tensor2tensor%E6%98%AF%E4%BB%80%E4%B9%88"><span class="nav-text">implementing tensor2tensor, replacing our earlier codebase。这里的tensor2tensor是什么</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Noam-proposed-scaled-dot-product-attention-%E8%BF%99%E9%87%8C%E7%9A%84scale%E6%8C%87%E4%BB%80%E4%B9%88"><span class="nav-text">Noam proposed scaled dot-product attention,这里的scale指什么</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dispensing-with-recurrence-and-convolutions-entirely%E3%80%82%E8%BF%99%E9%87%8C%E7%9A%84dispensing-%E6%98%AF%E4%BB%80%E4%B9%88%E6%84%8F%E6%80%9D"><span class="nav-text">dispensing with recurrence and convolutions entirely。这里的dispensing 是什么意思</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#improving-over-the-existing-best-results-including-ensembles-%E8%BF%99%E9%87%8C%E7%9A%84ensemble%E6%98%AF%E4%BB%80%E4%B9%88"><span class="nav-text">improving over the existing best results, including ensembles,这里的ensemble是什么</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#a-small-fraction-of-the-training-costs-of-the-best-models-from-the-literature-%E8%BF%99%E9%87%8C%E7%9A%84literature%E6%98%AF%E4%BB%80%E4%B9%88%E6%84%8F%E6%80%9D"><span class="nav-text">a small fraction of the training costs of the best models from the literature.这里的literature是什么意思</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Recurrent-models-typically-factor-computation-along-the-symbol-positions-of-the-input-and-output-sequences-%E8%BF%99%E9%87%8C%E7%9A%84factor%E6%98%AF%E6%8C%87%E4%BB%80%E4%B9%88"><span class="nav-text">Recurrent models typically factor computation along the symbol positions of the input and output sequences.这里的factor是指什么</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#This-inherently-sequential-nature-precludes-parallelization-within-training-examples%E3%80%82precludes%E6%98%AF%E4%BB%80%E4%B9%88"><span class="nav-text">This inherently sequential nature precludes parallelization within training examples。precludes是什么</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Recent-work-has-achieved-significant-improvements-in-computational-efficiency-through-factorization-tricks%E3%80%82factorization-tricks%E6%98%AF%E6%8C%87%E4%BB%80%E4%B9%88"><span class="nav-text">Recent work has achieved significant improvements in computational efficiency through factorization tricks。factorization tricks是指什么</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Attention-mechanisms-have-become-an-integral-part-of-compelling-sequence-modeling-and-transduction-models-in-various-tasks-compelling-%E6%98%AF%E4%BB%80%E4%B9%88"><span class="nav-text">Attention mechanisms have become an integral part of compelling sequence modeling and transduction models in various tasks,compelling 是什么</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#eschewing"><span class="nav-text">eschewing</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#arbitrary"><span class="nav-text">arbitrary</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#In-these-models-the-number-of-operations-required-to-relate-signals-from-two-arbitrary-input-or-output-positions-grows-in-the-distance-between-positions-linearly-for-ConvS2S-and-logarithmically-for-ByteNet-the-distance-between-positions-%E6%98%AF%E6%8C%87%E6%96%87%E6%9C%AC%E7%9A%84%E9%95%BF%E5%BA%A6%E5%90%97"><span class="nav-text">In these models,the number of operations required to relate signals from two arbitrary input or output positions grows in the distance between positions, linearly for ConvS2S and logarithmically for ByteNet.the distance between positions,是指文本的长度吗</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#counteract"><span class="nav-text">counteract</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Self-attention-sometimes-called-intra-attention%E3%80%82-intra-attention%E6%98%AF%E4%BB%80%E4%B9%88"><span class="nav-text">Self-attention, sometimes called intra-attention。 intra-attention是什么</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#At-each-step-the-model-is-auto-regressive-consuming-the-previously-generated-symbols-as-additional-input-when-generating-the-next-auto-regressive%E6%98%AF%E4%BB%80%E4%B9%88%E6%84%8F%E6%80%9D"><span class="nav-text">At each step the model is auto-regressive , consuming the previously generated symbols as additional input when generating the next.auto-regressive是什么意思</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#The-Transformer-follows-this-overall-architecture-using-stacked-self-attention-and-point-wise-%E8%BF%99%E9%87%8C%E7%9A%84point-wise%E6%98%AF%E4%BB%80%E4%B9%88"><span class="nav-text">The Transformer follows this overall architecture using stacked self-attention and point-wise,这里的point-wise是什么</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E5%8C%BF%E5%90%8D%E5%86%85%E9%83%A8%E7%B1%BB%E5%8F%AA%E8%83%BD%E6%98%AFfinal"><span class="nav-text">为什么匿名内部类只能是final</span></a></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">ednow</p><div class="site-description" itemprop="description"></div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">304</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">1</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">60</span> <span class="site-state-item-name">标签</span></a></div></nav></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">ednow</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">7.6m</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">115:35</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动</div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-divider">|</span> <span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>NexT.utils.loadComments(document.querySelector("#gitalk-container"),()=>{NexT.utils.getScript("//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js",()=>{new Gitalk({clientID:"04b9fe1c5636beb4acc4",clientSecret:"8ccb8829887eac219a8fdb018878fd0cf088a7ac",repo:"gittalk-comment",owner:"ednow",admin:["ednow"],id:"1fa05f6ed349abafaf647e85d9a3d25f",language:"zh-CN",distractionFreeMode:!0}).render("gitalk-container")},window.Gitalk)})</script><style>[bg-lazy]{background-image:none!important;background-color:#eee!important}</style><script>window.imageLazyLoadSetting={isSPA:!1,preloadRatio:1,processImages:null}</script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})})</script><script>!function(r){r.imageLazyLoadSetting.processImages=t;var e=r.imageLazyLoadSetting.isSPA,n=r.imageLazyLoadSetting.preloadRatio||1,c=a();function a(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(){e&&(c=a());for(var t,o=0;o<c.length;o++)0<=(t=(t=c[o]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(r.innerHeight*n||document.documentElement.clientHeight*n)&&function(){var t,e,n,a=c[o],i=function(){c=c.filter(function(t){return a!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(a)};(t=a).hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),i()):(e=new Image,n=t.getAttribute("data-original"),e.onload=function(){t.src=n,t.removeAttribute("data-original"),i()},t.src!==n&&(e.src=n))}()}function i(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",i),r.addEventListener("resize",i),r.addEventListener("orientationchange",i)}(this)</script></body></html>