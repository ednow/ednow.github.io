<!DOCTYPE html>
<html lang="zh-CN">
<head>
  <meta charset="UTF-8">
<meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=2">
<meta name="theme-color" content="#222">
<meta name="generator" content="Hexo 5.3.0">
  <link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png">
  <link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png">
  <link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png">
  <link rel="mask-icon" href="/images/logo.svg" color="#222">

<link rel="stylesheet" href="/css/main.css">

<link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext">
<link rel="stylesheet" href="/lib/font-awesome/css/all.min.css">

<script id="hexo-configurations">
    var NexT = window.NexT || {};
    var CONFIG = {"hostname":"ednow.github.io","root":"/","scheme":"Muse","version":"7.8.0","exturl":false,"sidebar":{"position":"left","width":400,"display":"post","padding":18,"offset":12,"onmobile":false},"copycode":{"enable":false,"show_result":false,"style":null},"back2top":{"enable":true,"sidebar":false,"scrollpercent":false},"bookmark":{"enable":false,"color":"#222","save":"auto"},"fancybox":false,"mediumzoom":false,"lazyload":false,"pangu":false,"comments":{"style":"tabs","active":null,"storage":true,"lazyload":false,"nav":null},"algolia":{"hits":{"per_page":10},"labels":{"input_placeholder":"Search for Posts","hits_empty":"We didn't find any results for the search: ${query}","hits_stats":"${hits} results found in ${time} ms"}},"localsearch":{"enable":false,"trigger":"auto","top_n_per_article":1,"unescape":false,"preload":false},"motion":{"enable":true,"async":false,"transition":{"post_block":"fadeIn","post_header":"slideDownIn","post_body":"slideDownIn","coll_header":"slideLeftIn","sidebar":"slideUpIn"}}};
  </script>

  <meta property="og:type" content="website">
<meta property="og:title" content="ednow">
<meta property="og:url" content="http://ednow.github.io/index/page/4/index.html">
<meta property="og:site_name" content="ednow">
<meta property="og:locale" content="zh_CN">
<meta property="article:author" content="ednow">
<meta name="twitter:card" content="summary">

<link rel="canonical" href="http://ednow.github.io/index/page/4/">


<script id="page-configurations">
  // https://hexo.io/docs/variables.html
  CONFIG.page = {
    sidebar: "",
    isHome : true,
    isPost : false,
    lang   : 'zh-CN'
  };
</script>

  <title>ednow</title>
  
    <script async src="https://www.googletagmanager.com/gtag/js?id=G-XQGJ63ZD9Y"></script>
    <script>
      if (CONFIG.hostname === location.hostname) {
        window.dataLayer = window.dataLayer || [];
        function gtag(){dataLayer.push(arguments);}
        gtag('js', new Date());
        gtag('config', 'G-XQGJ63ZD9Y');
      }
    </script>


  <script>
    var _hmt = _hmt || [];
    (function() {
      var hm = document.createElement("script");
      hm.src = "https://hm.baidu.com/hm.js?83f04257c97e81cca692d7c4c7fbbc9a";
      var s = document.getElementsByTagName("script")[0];
      s.parentNode.insertBefore(hm, s);
    })();
  </script>




  <noscript>
  <style>
  .use-motion .brand,
  .use-motion .menu-item,
  .sidebar-inner,
  .use-motion .post-block,
  .use-motion .pagination,
  .use-motion .comments,
  .use-motion .post-header,
  .use-motion .post-body,
  .use-motion .collection-header { opacity: initial; }

  .use-motion .site-title,
  .use-motion .site-subtitle {
    opacity: initial;
    top: initial;
  }

  .use-motion .logo-line-before i { left: initial; }
  .use-motion .logo-line-after i { right: initial; }
  </style>
</noscript>

</head>

<body itemscope itemtype="http://schema.org/WebPage">
  <div class="container use-motion">
    <div class="headband"></div>

    <header class="header" itemscope itemtype="http://schema.org/WPHeader">
      <div class="header-inner"><div class="site-brand-container">
  <div class="site-nav-toggle">
    <div class="toggle" aria-label="切换导航栏">
      <span class="toggle-line toggle-line-first"></span>
      <span class="toggle-line toggle-line-middle"></span>
      <span class="toggle-line toggle-line-last"></span>
    </div>
  </div>

  <div class="site-meta">

    <a href="/" class="brand" rel="start">
      <span class="logo-line-before"><i></i></span>
      <h1 class="site-title">ednow</h1>
      <span class="logo-line-after"><i></i></span>
    </a>
  </div>

  <div class="site-nav-right">
    <div class="toggle popup-trigger">
    </div>
  </div>
</div>




<nav class="site-nav">
  <ul id="menu" class="main-menu menu">
        <li class="menu-item menu-item-home">

    <a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a>

  </li>
        <li class="menu-item menu-item-tags">

    <a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a>

  </li>
        <li class="menu-item menu-item-categories">

    <a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a>

  </li>
        <li class="menu-item menu-item-archives">

    <a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a>

  </li>
  </ul>
</nav>




</div>
    </header>

    
  <div class="back-to-top">
    <i class="fa fa-arrow-up"></i>
    <span>0%</span>
  </div>


    <main class="main">
      <div class="main-inner">
        <div class="content-wrap">
          

          <div class="content index posts-expand">
            
      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://ednow.github.io/2021/07/26/%E7%8E%8B%E9%81%93-%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-ch4-%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ednow">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ednow">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/26/%E7%8E%8B%E9%81%93-%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-ch4-%E6%96%87%E4%BB%B6%E7%AE%A1%E7%90%86/" class="post-title-link" itemprop="url">王道-操作系统-ch4-文件管理</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-07-26 15:17:18" itemprop="dateCreated datePublished" datetime="2021-07-26T15:17:18+08:00">2021-07-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-10-14 00:04:42" itemprop="dateModified" datetime="2021-10-14T00:04:42+08:00">2021-10-14</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="王道"><a href="#王道" class="headerlink" title="王道"></a>王道</h1><h2 id="文件管理"><a href="#文件管理" class="headerlink" title="文件管理"></a>文件管理</h2><p>【考纲内容】</p>
<p>(一）文件系统基础</p>
<ol>
<li>文件的概念</li>
<li>文件的逻辑结构: $\color{red}{\text{文件元数据}}$ 和索引节点（inode）</li>
<li>文件的操作（建立，删除，打开，关闭，读，写）</li>
<li>文件的保护</li>
<li>文件的逻辑结构</li>
<li>文件的物理结构</li>
</ol>
<p>(二）目录</p>
<ol>
<li>目录的基本概念</li>
<li>树形目录</li>
<li> $\color{red}{\text{目录的操作}}$ </li>
<li>硬链接和软链接</li>
</ol>
<p>(二）文件系统</p>
<ol>
<li> $\color{red}{\text{文件系统的全局结构}}$ (layout)（文件系统在外存中的结构，文件系统在内存中的结构）</li>
<li>外存空闲空间管理办法</li>
<li> $\color{red}{\text{虚拟文件系统}}$ </li>
<li> $\color{red}{\text{文件系统挂载}}$ (mounting)</li>
</ol>
<!-- 
(三)磁盘组织与管理

磁盘的结构;磁盘调度算法;磁盘的管理 -->

<ul>
<li>概念——定义、属性、基本操作、打开与关闭</li>
<li>文件逻辑结构<ul>
<li>无结果文件（流式文件）</li>
<li>有结构文件(记录式文件)<ul>
<li>顺序文件</li>
<li>索引文件</li>
<li>索引顺序文件</li>
</ul>
</li>
</ul>
</li>
<li>目录结构<ul>
<li>文件控制块（FCB）、索引结点</li>
<li>单级目录结构、两级目录结构、树形目录结构、图形目录结构</li>
</ul>
</li>
<li>文件共享<ul>
<li>基于索引结点（硬链接)</li>
<li>利用符号链实现(软链接)</li>
</ul>
</li>
<li>文件保护————访问类型、访问控制</li>
<li>实现<ul>
<li>层次结构</li>
<li>目录实现————线性列表、哈希表</li>
<li>文件分配<ul>
<li>连续分配</li>
<li>链接分配</li>
<li>索引分配————索引链接、多层索引、混合索引</li>
</ul>
</li>
<li>文件存储空间管理<ul>
<li>空闲表法</li>
<li>空闲链表法</li>
<li>位示图法成组链接法</li>
</ul>
</li>
</ul>
</li>
<li>磁盘<ul>
<li>访问时间——寻道时间、延迟时间、传输时间</li>
<li>调度算法<ul>
<li>先来先服务（FCFS)————公平</li>
<li>最短寻找时间优先(SSTF)————“饥饿”现象</li>
<li>扫描算法(SCAN)</li>
<li>循环扫描(C-SCAN)</li>
</ul>
</li>
<li>磁盘的管理————初始化、引导块、坏块</li>
</ul>
</li>
</ul>
<p>【复习提示】</p>
<p>本章内容较为具体，要注意对概念的理解。重点掌握文件系统的结构及其实现、磁盘的相关知识点等。要掌握文件系统的文件控制块、物理分配方法、索引结构，以及磁盘特性和结构、磁盘调度算法，能分析磁盘相关的性能等。这些都是综合题易考查的内容。</p>
<h3 id="文件系统基础"><a href="#文件系统基础" class="headerlink" title="文件系统基础"></a>文件系统基础</h3><p>在学习本节时，请读者思考以下问题:</p>
<p>1）什么是文件?什么是文件系统?</p>
<p>2）文件系统要完成哪些功能?</p>
<p>本节内容较为抽象，对于初学者，推荐配合相关教材的相关章节进行学习。学习过程中要注意区分文件的逻辑结构和物理结构，不要把二者混为一谈。在读者的学习过程中，可尝试以上面的两个问题为线索，构建整个文件系统的概念，先思考有什么方法可以实现文件的共享和保护，再将自己的方法与书上的方法相比较。</p>
<p>在前面的学习中，曾经提醒过读者不要忽略对基本概念的理解。操作系统这门课程在统考中算是比较容易得分的一门课程，从历年的情况来看，大部分同学对进程管理、内存管理有较好的掌握，但对于文件管理及下一章的IO管理，往往理解不太深入，记忆不太牢固，在考试中，即使面对一些关于本章内容的基本问题也容易失分，这十分可惜。主要原因还是对概念的理解不够全面和透彻，希望各位读者能够关注这个问题。</p>
<h4 id="文件的概念"><a href="#文件的概念" class="headerlink" title="文件的概念"></a>文件的概念</h4><h5 id="文件的定义"><a href="#文件的定义" class="headerlink" title="文件的定义"></a>文件的定义</h5><p>文件(File）是操作系统中的一个重要概念。文件是以计算机硬盘为载体的存储在计算机上的信息集合，文件可以是文本文档、图片、程序等。在系统运行时，计算机以进程为基本单位进行资源的调度和分配;而在用户进行的输入、输出中，则以文件为基本单位。大多数应用程序的输入都是通过文件来实现的，其输出也都保存在文件中，以便信息的长期存储及将来的访问。当用户将文件用于应用程序的输入、输出时，还希望可以访问文件、修改文件和保存文件等，实现对文件的维护管理，这就需要系统提供一个文件管理系统，操作系统中的 $\color{green}{\text{文件系统}}$ (File System)就是用于实现用户的这些管理要求的。</p>
<p>要清晰地理解文件的概念，就要了解文件究竟由哪些东西组成。</p>
<p>首先，文件中肯定包括一块存储空间，更准确地说，是存储空间中的数据;其次，由于操作系统要管理成千上万的数据，因此必定需要对这些数据进行划分，然后贴上“标签”，以便于分类和索引，所以文件必定包含分类和索引的信息;最后，不同的用户拥有对数据的不同访问权限，因此文件中一定包含一些关于访问权限的信息。</p>
<p>再举生活中的一个直观例子来类比文件，相信读者了解这个例子后会更深入地了解文件。这个例子就是图书馆中的书，可以认为，计算机中的一个文件相当于图书馆中的一本书，操作系统管理文件，相当于图书管理员管理图书馆中的书。</p>
<p>首先，一本书的主体一定是书中的内容，相当于文件中的数据;其次，不同类别的书需要放在不同的书库，然后加上编号，再把编号登记在图书管理系统中，方便读者查阅，相当于文件的分类和查找;最后，有些已经绝版或价格比较高的外文书籍，只能借给VIP会员或权限比较高的其他读者，而有些普通的书籍可供任何人借阅，这就是文件中的访问权限。</p>
<p>所举的例子与实际操作系统中的情形并不绝对等价，读者应能找出类比中的不严谨之处，但对于某些关键的属性，图书馆管理图书和操作系统管理文件的思想却有相一致的地方，因此通过这种类比可使初学者快速认识陌生的概念。</p>
<p>从用户的角度看，文件系统是操作系统的重要部分之一。用户关心的是如何命名、分类和查找文件，如何保证文件数据的安全性及对文件可以进行哪些操作等。而对其中的细节，如文件如何存储在辅存上、如何管理文件辅存区域等关心甚少。</p>
<p>文件系统提供了与二级存储相关的资源的抽象，让用户能在不了解文件的各种属性、文件存储介质的特征及文件在存储介质上的具体位置等情况下，方便快捷地使用文件。</p>
<p>用户通过文件系统建立文件，提供应用程序的输入、输出，对资源进行管理。首先了解文件的结构，我们通过自底向上的方式来定义。</p>
<p>1）数据项。数据项是文件系统中最低级的数据组织形式，可分为以下两种类型:</p>
<ul>
<li>基本数据项。用于描述一个对象的某种属性的一个值，如姓名、日期或证件号等，是数据中可命名的最小逻辑数据单位，即原子数据。</li>
<li>组合数据项。由多个基本数据项组成。</li>
</ul>
<p>2）记录。记录是一组相关的数据项的集合，用于描述一个对象在某方面的属性，如一名考生的报名记录包括考生姓名、出生日期、报考学校代号、身份证号等一系列域。</p>
<p>3）文件。文件是指由创建者所定义的一组相关信息的集合，逻辑上可分为有结构文件和无结构文件两种。在有结构文件中，文件由一组相似的记录组成，如报考某学校的所有考生的报考信息记录，又称记录式文件;而无结构文件则被视为一个字符流，比如一个二进制文件或字符文件，又称流式文件。</p>
<p>虽然上面给出了结构化的表还，但头协上NJ大#制代码，其基太访问单元可以是字节、行或程序和数据组织成文件。文件可以定效丁、于出m中允许可控制的进程间共享访问，能够被组记录。文件可以长期存储于硬盘或其他二级存储器中，允许可控制的进程间共享访问,能够被组织成复杂的结构。</p>
<h5 id="文件的属性"><a href="#文件的属性" class="headerlink" title="文件的属性"></a>文件的属性</h5><p>文件具有一定的属性，系统不同，属性也会有所不同，但通常都包括如下属性。</p>
<p>1） $\color{green}{\text{名称}}$ 。文件名称唯一，以容易读取的形式保存。</p>
<p>2） $\color{green}{\text{标识符}}$ 。标识文件系统内文件的唯一标签，通常为数字，是对人不可读的一种内部名称。</p>
<p>3） $\color{green}{\text{类型}}$ 。被支持不同类型的文件系统所使用。</p>
<p>4） $\color{green}{\text{位置}}$ 。指向设备和设备上文件的指针。</p>
<p>5） $\color{green}{\text{大小}}$ 。文件当前大小（用字节、字或块表示)，也可包含文件允许的最大值。</p>
<p>6） $\color{green}{\text{保护}}$ 。对文件进行保护的访问控制信息。</p>
<p>7） $\color{green}{\text{时间、日期和用户标识}}$ 。文件创建、上次修改和上次访问的相关信息，用于保护和跟踪文件的使用。</p>
<p>所有文件的信息都保存在目录结构中，而目录结构保存在外存上。文件信息在需要时才调入内存。通常，目录条目包括文件名称及其唯一的标识符，而标识符定位其他属性的信息。</p>
<h5 id="文件的基本操作"><a href="#文件的基本操作" class="headerlink" title="文件的基本操作"></a>文件的基本操作</h5><p>文件属于抽象数据类型。为了恰当地定义文件，需要考虑有关文件的操作。操作系统提供系统调用，它对文件进行创建、写、读、重定位、删除和截断等操作。</p>
<p>1） $\color{green}{\text{创建文件}}$ 。创建文件有两个必要步骤:一是在文件系统中为文件找到空间;二是在目录中为新文件创建条目，该条目记录文件名称、在文件系统中的位置及其他可能的信息。</p>
<p>2） $\color{green}{\text{写文件}}$ 。为了写文件，执行一个系统调用，指明文件名称和要写入文件的内容。对于给定文件名称，系统搜索目录以查找文件位置。系统必须为该文件维护一个写位置的指针。每当发生写操作时，便更新写指针。</p>
<p>3） $\color{green}{\text{读文件}}$ 。为了读文件，执行一个系统调用，指明文件名称和要读入文件块的内存位置。同样，需要搜索目录以找到相关目录项，系统维护一个读位置的指针。每当发生读操作时，更新读指针。一个进程通常只对一个文件读或写，因此当前操作位置可作为每个进程当前文件位置的指针。由于读和写操作都使用同一指针，因此节省了空间，也降低了系统复杂度。</p>
<p>4） $\color{green}{\text{文件重定位}}$ (文件寻址)。按某条件搜索目录，将当前文件位置设为给定值，并且不会读、写文件。</p>
<p>5） $\color{green}{\text{删除文件}}$ 。先从目录中找到要删除文件的目录项，使之成为空项，然后回收该文件所占用的存储空间。</p>
<p>6） $\color{green}{\text{截断文件}}$ 。允许文件所有属性不变，并删除文件内容，即将其长度设为0并释放其空间。这6个基本操作可以组合起来执行其他文件操作。例如，一个文件的复制，可以创建新文件、从旧文件读出并写入新文件。</p>
<p>这6个基本操作可以组合起来执行其他文件操作。例如，一个文件的复制，可以创建新文件、从旧文件读出并写入新文件。</p>
<h5 id="文件的打开与关闭"><a href="#文件的打开与关闭" class="headerlink" title="文件的打开与关闭"></a>文件的打开与关闭</h5><p>因为许多文件操作都涉及为给定文件搜索相关目录条目，因此许多系统要求在首次使用文件时，使用系统调用open将指明文件的 $\color{green}{\text{属性}}$ (包括该文件在外存上的物理位置)从外存 $\color{green}{\text{复制}}$ 到内存打开文件表的一个表目中，并将该表目的编号（也称索引）返回给用户。操作系统维护一个包含所有打开文件信息的表(打开文件表，open-file table)。当用户需要一个文件操作时，可通过该表的一个索引指定文件，因此省略了搜索环节。当文件不再使用时，进程可以关闭它，操作系统从打开文件表中删除这一条目。</p>
<p>大部分操作系统要求在文件使用之前就被显式地打开。操作 open 会根据文件名搜索目录，并将目录条目复制到打开文件表。若调用open 的请求（创建、只读、读写、添加等）得到允许，则进程就可打开文件，而 open通常返回一个指向打开文件表中的一个条目的指针。通过使用该指针（而非文件名）进行所有IO操作，以简化步骤并节省资源。</p>
<p>注意，在open调用完成后，操作系统对该文件的任何操作都不再需要文件名，而只需要open调用返回的指针。</p>
<p>整个系统表包含进程相关信息，如文件在磁盘的位置、访问日期和大小。一个进程打开一个文件，系统打开文件表就会为打开的文件增加相应的条目。当另一个进程执行open时，只不过是在其进程打开表中增加一个条目，并指向整个系统表的相应条目。通常，系统打开文件表的每个文件时，还用一个文件打开计数器（Open Count)，以记录多少进程打开了该文件。每个关闭操作close使count递减，当打开计数器为0时，表示该文件不再被使用，系统将回收分配给该文件的内存空间等资源。若文件被修改过，则将文件写回外存，并将系统打开文件表中的相应条目删除，最后释放文件的文件控制块（File Control Block，FCB)。</p>
<p>每个打开文件都有如下关联信息:</p>
<ul>
<li>文件指针。系统跟踪上次的读写位置作为当前文件位置的指针，这种指针对打开文件的某个进程来说是唯一的，因此必须与磁盘文件属性分开保存。</li>
<li>文件打开计数。文件关闭时，操作系统必须重用其打开文件表条目，否则表内空间会不够用。因为多个进程可能打开同一个文件，所以系统在删除打开文件条目之前，必须等待最后一个进程关闭文件。计数器跟踪打开和关闭的数量，计数为0时，系统关闭文件，删除该条目。</li>
<li>文件磁盘位置。绝大多数文件操作都要求系统修改文件数据。该信息保存在内存中，以免为每个操作都从磁盘中读取。</li>
<li>访问权限。每个进程打开文件都需要有一个访问模式（创建、只读、读写、添加等)。该信息保存在进程的打开文件表中，以便操作系统能够允许或拒绝之后的I/O请求。</li>
</ul>
<h4 id="文件的逻辑结构"><a href="#文件的逻辑结构" class="headerlink" title="文件的逻辑结构"></a>文件的逻辑结构</h4><p>文件的逻辑结构是从用户观点出发看到的文件的组织形式。文件的物理结构(又称文件的存储结构，见4.2.1节）是从实现观点出发看到的文件在外存上的存储组织形式。文件的逻辑结构与存储介质特性无关，但文件的物理结构与存储介质的特性有很大关系。文件的逻辑结构实际上是指在文件的内部，数据逻辑上是如何组织起来的。</p>
<p>按逻辑结构，文件可划分为无结构文件和有结构文件两种。</p>
<h5 id="无结构文件（流式文件"><a href="#无结构文件（流式文件" class="headerlink" title="无结构文件（流式文件)"></a>无结构文件（流式文件)</h5><p>无结构文件是最简单的文件组织形式。无结构文件将数据按顺序组织成记录并积累、保存，它是有序相关信息项的集合，以字节(Byte）为单位。由于无结构文件没有结构，因而对记录的访问只能通过穷举搜索的方式，因此这种文件形式对大多数应用不适用。但字符流的无结构文件管理简单，用户可以方便地对其进行操作。所以，那些对基本信息单位操作不多的文件较适于采用字符流的无结构方式，如源程序文件、目标代码文件等。</p>
<h5 id="有结构文件（记录式文件"><a href="#有结构文件（记录式文件" class="headerlink" title="有结构文件（记录式文件)"></a>有结构文件（记录式文件)</h5><p>有结构文件按记录的组织形式可以分为如下几种:</p>
<p>1）顺序文件。文件中的记录一个接一个地顺序排列，记录通常是定长的，可以顺序存储或以链表形式存储，在访问时需要顺序搜索文件。顺序文件有以下两种结构:第一种是串结构，记录之间的顺序与关键字无关。通常的办法是由时间决定，即按存入时间的先后排列，最先存入的记录作为第1条记录，其次存入的为第2条记录，以此类推。第二种是顺序结构，指文件中的所有记录按关键字顺序排列。</p>
<p>在对记录进行批量操作，即每次要读或写一大批记录时，顺序文件的效率是所有逻辑文件中最高的;此外，也只有顺序文件才能存储在磁带上，并能有效地工作，但顺序文件对查找、修改、增加或删除单条记录的操作比较困难。</p>
<p>2）索引文件。索引文件示意图如图4.1所示。对于定长记录文件，要查找第i条记录,可直接根据下式计算得到第i条记录相对于第1条记录的地址:</p>
<p>$$<br>A_i= i×L<br>$$ </p>
<p>然而，对于可变长记录的文件，要查找第i条记录，必须顺序地查找前 $i$ –1条记录，从而获得相应记录的长度L，进而按下式计算出第 $i$ 条记录的首址:</p>
<p>$$<br>A_i = \sum_{i=0}^{i-1} L_i + 1<br>$$</p>
<p>注意:假定每条记录前用一个字节指明该记录的长度。</p>
<p>变长记录文件只能顺序查找，系统开销较大。为此，可以建立一张索引表以加快检索速度，索引表本身是定长记录的顺序文件。在记录很多或访问要求高的文件中，需要引入索引以提供有效的访问。实际中，通过索引可以成百上千倍地提高访问速度。</p>
<details><summary>图4.1索引文件示意图
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211006104947.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211006104947.png';" /></details>

<p>3）索引顺序文件。索引顺序文件是顺序和索引两种组织形式的结合。索引顺序文件将顺序文件中的所有记录分为若干组，为顺序文件建立一张索引表，在索引表中为每组中的第一条记录建立一个索引项，其中含有该记录的关键字值和指向该记录的指针。</p>
<p>如图4.2所示，主文件名包含姓名和其他数据项。姓名为关键字，索引表中为每组的第一条记录（不是每条记录）的关键字值，用指针指向主文件中该记录的起始位置。索引表只包含关键字和指针两个数据项，所有姓名关键字递增排列。主文件中记录分组排列，同一个组中的关键字可以无序，但组与组之间的关键字必须有序。查找一条记录时，首先通过索引表找到其所在的组，然后在该组中使用顺序查找，就能很快地找到记录。</p>
<details><summary>图4.2索引顺序文件示意图
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211006105051.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211006105051.png';" /></details>

<p>对于含有N条记录的顺序文件，查找某关键字值的记录时，平均需要查找N/2次。在索引顺序文件中，假设N条记录分为 $\sqrt{N}$ 组，索引表中有$\sqrt{N}$个表项，每组有$\sqrt{N}$条记录,在查找某关键字值的记录时，先顺序查找索引表，需要查找√N/2次，然后在主文件中对应的组中顺序查找，也需要查找$\sqrt{N}$/2次，因此共需查找$\sqrt{N}$ /2+$\sqrt{N}$ /2=$\sqrt{N}$ 次。显然，索引顺序文件提高了查找效率，若记录数很多，则可采用两级或多级索引。</p>
<p>索引文件和索引顺序文件都提高了存取的速度，但因为配置索引表而增加了存储空间。</p>
<p>4）直接文件或散列文件（Hash File)。给定记录的键值或通过散列函数转换的键值直接决定记录的物理地址。这种映射结构不同于顺序文件或索引文件，没有顺序的特性。</p>
<p>散列文件有很高的存取速度，但是会引起冲突，即不同关键字的散列函数值相同。</p>
<p>复习了数据结构的读者读到这里时，会有这样的感觉:有结构文件逻辑上的组织，是为在文件中查找数据服务的（顺序查找、索引查找、索引顺序查找、哈希查找)。</p>
<h4 id="目录结构"><a href="#目录结构" class="headerlink" title="目录结构"></a>目录结构</h4><p>与文件管理系统和文件集合相关联的是文件目录，它包含有关文件的信息如属性、位置和所有权等，这些信息主要由操作系统进行管理。首先我们来看目录管理的基本要求:从用户的角度看，目录在用户(应用程序）所需要的文件名和文件之间提供一种映射，所以目录管理要实现“按名存取”;目录存取的效率直接影响到系统的性能，所以要提高对目录的检索速度;在共享系统中，目录还需要提供用于控制访问文件的信息。此外，文件允许重名也是用户的合理和必然要求,目录管理通过树形结构来解决和实现。</p>
<p>前面介绍了文件内部的逻辑结构，下面介绍多个文件之间在逻辑上是如何组织的，这实际上是文件“外部”的逻辑结构的问题。</p>
<h5 id="文件控制块和索引结点"><a href="#文件控制块和索引结点" class="headerlink" title="文件控制块和索引结点"></a>文件控制块和索引结点</h5><p>与进程管理一样，为实现目录管理，操作系统中引入了文件控制块的数据结构。</p>
<p>1）文件控制块。文件控制块（FCB）是用来存放控制文件需要的各种信息的数据结构以实现“按名存取”。FCB的有序集合称为文件目录，一个FCB就是一个文件目录项为了创建一个新文件，系统将分配一个FCB并存放在文件目录中，成为目录项。</p>
<p>FCB主要包含以下信息:</p>
<ul>
<li>基本信息，如文件名、文件的物理位置、文件的逻辑结构、文件的物理结构等。</li>
<li>存取控制信息，如文件存取权限等。</li>
<li>使用信息，如文件建立时间、修改时间等。</li>
</ul>
<p>2）索引结点。在检索目录文件的过程中，只用到了文件名，仅当找到一个目录项（查找文件名与目录项中文件名匹配）时，才需要从该目录项中读出该文件的物理地址。也就是说，在检索目录时，文件的其他描述信息不会用到，也不需要调入内存。因此，有的系统（如UNIX，见表4.1）采用了文件名和文件描述信息分开的方法，文件描述信息单独形成一个称为索引结点的数据结构，简称i结点。在文件目录中的每个目录项仅由文件名和指向该文件所对应的i结点的指针构成。</p>
<details><summary>表4.1UNIX的文件目录结构</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211006105418.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211006105418.png';" /></details>

<p>一个FCB的大小是64B，盘块大小是1KB，因此在每个盘块中可以存放16个FCB(注意，FCB必须连续存放)。而在 UNIX系统中，一个目录项仅占16B，其中14B是文件名，2B是i结点指针。在1KB的盘块中可存放64个目录项。这样，就可使查找文件时的平均启动磁盘次数减少到原来的1/4，大大节省了系统开销。</p>
<p>存放在磁盘上的索引结点称为磁盘索引结点，UNIX中的每个文件都有一个唯一的磁盘索引结点，主要包括以下几个方面:</p>
<ul>
<li> $\color{green}{\text{文件主标识符}}$ ，拥有该文件的个人或小组的标识符。</li>
<li> $\color{green}{\text{文件类型}}$ ，包括普通文件、目录文件或特别文件。文件存取权限，各类用户对该文件的存取权限。</li>
<li> $\color{green}{\text{文件物理地址}}$ ，每个索引结点中含有13个地址项，即 iaddr(O)一iaddr(12)，它们以直接或间接方式给出数据文件所在盘块的编号。</li>
<li> $\color{green}{\text{文件长度}}$ ，以字节为单位。</li>
<li> $\color{green}{\text{文件链接计数}}$ ，在本文件系统中所有指向该文件的文件名的指针计数。</li>
<li>$\color{green}{\text{文件存取时间}}$ ，本文件最近被进程存取的时间、最近被修改的时间及索引结点最近被修改的时间。</li>
</ul>
<p>文件被打开时，磁盘索引结点复制到内存的索引结点中，以便于使用。在内存索引结点中又增加了以下内容:</p>
<ul>
<li>索引结点编号，用于标识内存索引结点。</li>
<li>状态，指示i结点是否上锁或被修改。</li>
<li>访问计数，每当有一进程要访问此i结点时，计数加1，访问结束减1。</li>
<li>逻辑设备号，文件所属文件系统的逻辑设备号。</li>
<li>链接指针，设置分别指向空闲链表和散列队列的指针。</li>
</ul>
<p>FCB或索引结点相当于图书馆中图书的索书号，我们可以在图书馆网站上找到图书的索书号，然后根据索书号找到想要的书本。</p>
<h5 id="目录结构-1"><a href="#目录结构-1" class="headerlink" title="目录结构"></a>目录结构</h5><p>在理解一个文件系统的需求前，我们首先考虑在目录这个层次上所需要执行的操作，这有助于后面文件系统的整体理解。</p>
<ul>
<li>搜索。当用户使用一个文件时，需要搜索目录，以找到该文件的对应目录项。</li>
<li>创建文件。当创建一个新文件时，需要在目录中增加-个目录项。</li>
<li>删除文件。当删除一个文件时，需要在目录中删除相应的目录项。</li>
<li>显示目录。用户可以请求显示目录的内容，如显示该用户目录中的所有文件及属性。</li>
<li>修改目录。某些文件属性保存在目录中，因而这些属性的变化需要改变相应的目录项。操作时，考虑以下几种目录结构:</li>
</ul>
<p>操作时，考虑以下几种目录结构:</p>
<p>1）$\color{green}{\text{单级目录结构}}$ 。在整个文件系统中只建立一张目录表，每个文件占一个目录项，如图4.3所示。</p>
<details><summary>图4.3单级目录结构</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211006105810.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211006105810.png';" /></details>

<p>当访问一个文件时，先按文件名在该目录中查找到相应的FCB，经合法性检查后执行相应的操作。当建立一个新文件时，必须先检索所有目录项以确保没有“重名”的情况，然后在该目录中增设一项，把FCB 的全部信息保存在该项中。当删除一个文件时，先从该目录中找到该文件的目录项，回收该文件所占用的存储空间，然后清除该目录项。</p>
<p>单级目录结构实现了“按名存取”，但是存在查找速度慢、文件不允许重名、不便于文件共享等缺点，而且对于多用户的操作系统显然是不适用的。</p>
<p>2）两级目录结构。单级目录很容易造成文件名称的混淆，因此可以考虑采用两级方案,将文件目录分成主文件目录(Master File Directory，MFD）和用户文件目录（User FileDirectory, UFD)两级，如图4.4所示。</p>
<p>主文件目录项记录用户名及相应用户文件目录所在的存储位置。用户文件目录项记录该用户文件的FCB 信息。当某用户欲对其文件进行访问时，只需搜索该用户对应的UFD，这既解决了不同用户文件的“重名”问题，又在一定程度上保证了文件的安全。</p>
<p>两级目录结构可以解决多用户之间的文件重名问题，文件系统可以在目录上实现访问限制。但是两级目录结构缺乏灵活性，不能对文件分类。</p>
<details><summary>图4.4两级目录结构</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211006105935.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211006105935.png';" /></details>

<p>3）多级目录结构（树形目录结构)。将两级目录结构的层次关系加以推广，就形成了多级目录结构，即树形目录结构，如图4.5所示。</p>
<p>用户要访问某个文件时，用文件的路径名标识文件，文件路径名是个字符串，由从根目录出发到所找文件通路上所有目录名与数据文件名用分隔符“/”链接而成。从根目录出发的路径称为绝对路径。当层次较多时，每次从根目录查询会浪费时间，于是加入了当前目录(又称工作目录)，进程对各文件的访问都是相对于当前目录进行的。当用户要访问某个文件时，使用相对路径标识文件，相对路径由从当前目录出发到所找文件通路上所有目录名与数据文件名用分隔符“/”链接而成。</p>
<details><summary>图4.5树形目录结构</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211006110043.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211006110043.png';" /></details>

<p>图4.5是Linux 操作系统的目录结构,”/dev/hda”就是一个绝对路径。若当前目录为“/bin”,则“./1s”就是一个相对路径，其中符号“.”表示当前工作目录。</p>
<p>通常，每个用户都有各自的“当前目录”，登录后自动进入该用户的“当前目录”。操作系统提供一条专门的系统调用，供用户随时改变“当前目录”。例如，在 UNIX系统中，”/etc/passwd”文件就包含有用户登录时默认的“当前目录”，可用cd命令改变“当前目录”。</p>
<p>树形目录结构可以很方便地对文件进行分类，层次结构清晰，也能够更有效地进行文件的管理和保护。但是，在树形目录中查找一个文件时，需要按路径名逐级访问中间结点，这就增加了磁盘访问次数，无疑将影响查询速度。</p>
<p>4）无环图目录结构。树形目录结构能便于实现文件分类，但不便于实现文件共享，为此在树形目录结构的基础上增加了一些指向同一结点的有向边，使整个目录成为一个有向无环图。引入无环图目录结构是为了实现文件共享，如图4.6所示。</p>
<details><summary>图4.6无环图目录结构
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211006110145.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211006110145.png';" /></details>

<p>当某用户要求删除一个共享结点时，若系统只是简单地将它删除，则当另一共享用户需要访问时，会因无法找到这个文件而发生错误。为此，可为每个共享结点设置一个共享计数器，每当图中增加对该结点的共享链时，计数器加1;每当某用户提出删除该结点时,计数器减1。仅当共享计数器为0时，才真正删除该结点，否则仅删除请求用户的共享链。共享文件（或目录）不同于文件拷贝(副本)。若有两个文件拷贝，则每个程序员看到的是拷贝而不是原件;然而，若一个文件被修改，则另一个程序员的拷贝不会改变。对于共享文件，只存在一个真正的文件，任何改变都会为其他用户所见。</p>
<p>无环图目录结构方便地实现了文件的共享，但使得系统的管理变得更加复杂。</p>
<h4 id="文件共享"><a href="#文件共享" class="headerlink" title="文件共享"></a>文件共享</h4><p>文件共享使多个用户(进程）共享同一个文件，系统中只需保留该文件的一个副本。若系统不能提供共享功能，则每个需要该文件的用户都要有各自的副本，会造成对存储空间的极大浪费。随着计算机技术的发展，文件共享的范围已由单机系统发展到多机系统，进而通过网络扩展到全球。这些文件的分享是通过分布式文件系统、远程文件系统、分布式信息系统实现的。这些系统允许多个客户通过C/S模型共享网络中的服务器文件。</p>
<p>现代常用的两种文件共享方法如下。</p>
<h5 id="基于索引结点的共享方式（硬链接"><a href="#基于索引结点的共享方式（硬链接" class="headerlink" title="基于索引结点的共享方式（硬链接)"></a>基于索引结点的共享方式（硬链接)</h5><p>在树形结构的目录中，当有两个或多个用户要共享一个子目录或文件时，必须将共享文件或子目录链接到两个或多个用户的目录中，才能方便地找到该文件，如图4.7所示。</p>
<details><summary>图4.7基于索引结点的共享方式
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211006110256.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211006110256.png';" /></details>

<p>在这种共享方式中，诸如文件的物理地址及其他的文件属性等信息，不再放在目录项中，而放在索引结点中。在文件目录中只设置文件名及指向相应索引结点的指针。在索引结点中还应有一个链接计数count，用于表示链接到本索引结点（即文件)上的用户目录项的数目。当count = 2时，表示有两个用户目录项链接到本文件上，或者说有两个用户共享此文件。</p>
<p>用户A创建一个新文件时，它便是该文件的所有者，此时将count置为1。用户B要共享此文件时，在用户B的目录中增加一个目录项，并设置一个指针指向该文件的索引结点。此时，文件主仍然是用户A，count=2。用户A不再需要此文件，不能将文件直接删除。因为若删除了该文件，则必然也删除了该文件的索引结点，这样便会使用户B的指针悬空，而用户B可能正在此文件上执行写操作，此时用户B会无法访问到文件。因此用户A不能删除此文件，只是将该文件的count减1，然后删除自己目录中的相应目录项。用户B仍可以使用该文件。当count =0时，表示没有用户使用该文件，系统将负责删除该文件。如图4.8给出了用户B链接到文件上的前、后情况。</p>
<details><summary>图4.8文件共享中的链接计数
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211006110342.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211006110342.png';" /></details>

<h5 id="利用符号链实现文件共享（软链接"><a href="#利用符号链实现文件共享（软链接" class="headerlink" title="利用符号链实现文件共享（软链接)"></a>利用符号链实现文件共享（软链接)</h5><p>为使用户B能共享用户A的一个文件F，可以由系统创建一个LINK类型的新文件也取名为F，并将文件F写入用户B的目录中，以实现用户B的目录与文件F的链接。在新文件中只包含被链接文件F的路径名。这样的链接方法被称为符号链接。</p>
<p>新文件中的路径名只被视为符号链，当用户B要访问被链接的文件F且正要读LINK类新文件时，操作系统根据新文件中的路径名去读该文件，从而实现用户B对文件F的共享。</p>
<p>在利用符号链方式实现文件共享时，只有文件的拥有者才拥有指向其索引结点的指针而共享该文件的其他用户只有该文件的路径名，并不拥有指向其索引结点的指针。这样，也就不会发生在文件主删除一个共享文件后留下一个悬空指针的情况。当文件的拥有者把一个共享文件删除后，其他用户通过符号链去访问它时，会出现访问失败，于是将符号链删除，此时不会产生任何影响。当然，利用符号链实现文件共享仍然存在问题。例如，一个文件采用符号链方式共享，当文件拥有者将其删除，而在共享的其他用户使用其符号链接访问该文件之前，又有人在同一路径下创建了另一个具有同样名称的文件，则该符号链将仍然有效，但访问的文件已经改变，从而导致错误。</p>
<p>在符号链的共享方式中，当其他用户读共享文件时，需要根据文件路径名逐个地查找目录，直至找到该文件的索引结点。因此，每次访问时，都可能要多次地读盘，使得访问文件的开销变大并增加了启动磁盘的频率。此外，符号链的索引结点也要耗费一定的磁盘空间。</p>
<p>符号链方式有一个很大的优点，即网络共享只需提供该文件所在机器的网络地址及该机器中的文件路径。</p>
<p>上述两种链接方式都存在一个共同的问题，即每个共享文件都有几个文件名。换言之，每增加一条链接，就增加一个文件名。这实质上是每个用户都使用自己的路径名去访问共享文件。当我们试图去遍历整个文件系统时，将会多次遍历到该共享文件。</p>
<p>硬链接和软链接都是文件系统中的静态共享方法，在文件系统中还存在着另外的共享需求，即两个进程同时对同一个文件进行操作，这样的共享称为 $\color{green}{\text{动态共享}}$ 。</p>
<p>可以这样说:文件共享，“软”“硬”兼施。硬链接就是多个指针指向一个索引结点，保证只要还有一个指针指向索引结点，索引结点就不能删除;软链接就是把到达共享文件的路径记录下来，当要访问文件时，根据路径寻找文件。可以想象，硬链接的查找速度要比软链接的快。</p>
<h4 id="文件保护"><a href="#文件保护" class="headerlink" title="文件保护"></a>文件保护</h4><p>为了防止文件共享可能会导致文件被破坏或未经核准的用户修改文件，文件系统必须控制用户对文件的存取，即解决对文件的读、写、执行的许可问题。为此，必须在文件系统中建立相应的文件保护机制。</p>
<p>文件保护通过 $\color{green}{\text{口令保护}}$ 、 $\color{green}{\text{加密保护}}$ 和 $\color{green}{\text{访问控制}}$ 等方式实现。其中，口令保护和加密保护是为了防止用户文件被他人存取或窃取，而访问控制则用于控制用户对文件的访问方式。</p>
<h5 id="访问类型"><a href="#访问类型" class="headerlink" title="访问类型"></a>访问类型</h5><p>对文件的保护可从限制对文件的访问类型中出发。可加以控制的访问类型主要有以下几种。</p>
<ul>
<li>读。从文件中读。</li>
<li>写。向文件中写。</li>
<li>执行。将文件装入内存并执行。</li>
<li>添加。将新信息添加到文件结尾部分。</li>
<li>删除。删除文件，释放空间。</li>
<li>列表清单。列出文件名和文件属性。</li>
</ul>
<p>此外还可以对文件的重命名、复制、编辑等加以控制。这些高层的功能可以通过系统程序调用低层系统调用来实现。保护可以只在低层提供。例如，复制文件可利用一系列的读请求来完成,这样，具有读访问权限的用户同时也就具有了复制和打印权限。</p>
<h5 id="访问控制"><a href="#访问控制" class="headerlink" title="访问控制"></a>访问控制</h5><p>解决访问控制最常用的方法是根据用户身份进行控制。而实现基于身份访问的最为普通的方法是，为每个文件和目录增加一个 $\color{green}{\text{访问控制列表}}$ (Access-Control List，ACL)，以规定每个用户名及其所允许的访问类型。</p>
<p>这种方法的优点是可以使用复杂的访问方法，缺点是长度无法预计并且可能导致复杂的空间管理，使用精简的访问列表可以解决这个问题。</p>
<p>精简的访问列表采用拥有者、组和其他三种用户类型。</p>
<p>1）拥有者。创建文件的用户。</p>
<p>2）组。一组需要共享文件且具有类似访问的用户。</p>
<p>3）其他。系统内的所有其他用户。这样，只需用三个域即可列出访问表中这三类用户的访问权限。文件拥有者在创建文件时，说明创建者用户名及所在的组名，系统在创建文件时也将文件主的名字、所属组名列在该文件的FCB中。用户访问该文件时，按照拥有者所拥有的权限访问文件，若用户和拥有者在同一个用户组，则按照同组权限访问，否则只能按其他用户权限访问。UNIX操作系统即采用此种方法。</p>
<p>口令和密码是另外两种访问控制方法。</p>
<p>口令指用户在建立一个文件时提供一个口令，系统为其建立FCB 时附上相应口令，同时告诉允许共享该文件的其他用户。用户请求访问时必须提供相应的口令。这种方法时间和空间的开销不多，缺点是口令直接存在系统内部，不够安全。</p>
<p>密码指用户对文件进行加密，文件被访问时需要使用密钥。这种方法保密性强，节省了存储空间，不过编码和译码要花费一定的时间。</p>
<p>口令和密码都是防止用户文件被他人存取或窃取，并没有控制用户对文件的访问类型。注意两个问题:</p>
<p>1）现代操作系统常用的文件保护方法是，将访问控制列表与用户、组和其他成员访问控制方案一起组合使用。</p>
<p>2）对于多级目录结构而言，不仅需要保护单个文件，而且需要保护子目录内的文件，即需要提供目录保护机制。目录操作与文件操作并不相同，因此需要不同的保护机制。</p>
<h4 id="本节小结"><a href="#本节小结" class="headerlink" title="本节小结"></a>本节小结</h4><p>本节开头提出的问题的参考答案如下。</p>
<p>1）什么是文件?什么是文件系统?</p>
<p>文件是以计算机硬盘为载体的存储在计算机上的信息集合，它的形式多样，可以是文本文档、图片、程序等。操作系统中负责管理和存储文件信息的软件机构称为文件管理系统，简称文件系统。文件系统由三部分组成:与文件管理有关的软件、被管理文件及实施文件管理所需的数据结构。</p>
<p>2）文件系统要完成哪些功能?</p>
<p>对于用户而言，文件系统最主要的功能是实现对文件的基本操作，让用户可以按名存储和查找文件，组织成合适的结构，并应当具有基本的文件共享和文件保护功能。对于操作系统本身而言，文件系统还需要管理与磁盘的信息交换，完成文件逻辑结构和物理结构上的变换，组织文件在磁盘上的存放，采取好的文件排放顺序和磁盘调度方法以提升整个系统的性能。</p>
<p>学习到这里时，读者应会有这样的一种体会:现代操作系统的管理思想中，到处能够见到面向对象程序设计的影子。本节我们学习的一个新概念————文件，实质上就是一个抽象数据类型，也就是一种 $\color{red}{\text{数据结构}}$ ，若读者在复习操作系统之前已复习完数据结构，则遇到一种新的数据结构时，一定会有这样的意识:要认识它的逻辑结构、物理结构，以及对这种数据结构的操作。本节我们已经学完文件的 $\color{green}{\text{逻辑结构}}$ ，下一节将介绍文件的实现，也就是文件的 $\color{green}{\text{物理结构}}$ 。操作系统对文件的操作不是本课程关心的问题，我们不去研究。</p>
<h3 id="文件系统实现"><a href="#文件系统实现" class="headerlink" title="文件系统实现"></a>文件系统实现</h3><p>在学习本节时，请读者思考以下问题:</p>
<p>1）在目录中查找某个文件可以使用什么方法?</p>
<p>2）文件的逻辑结构和物理结构有何区别?单个文件的逻辑结构和物理结构之间是否存在某些制约关系?上节介绍了目录和文件的逻辑结构,本节将介绍文件物理结构和目录的实现。建议读者阅读之前先回顾上节的内容，并自己思考相应功能的实现方法，在学习过程中和本节的方法进行对比，这样能更好地理解本节的内容。</p>
<details><summary>图4.9文件系统层次结构
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211006110943.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211006110943.png';" /></details>

<h4 id="文件系统层次结构"><a href="#文件系统层次结构" class="headerlink" title="文件系统层次结构"></a>文件系统层次结构</h4><p>现代操作系统有多种文件系统类型（如FAT32，NTFS,ext2，ext3，ext4等)，因此文件系统的层次结构也不尽相同。图4.9是一种合理的层次结构。</p>
<details><summary>图4.9文件系统层次结构
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211006111035.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211006111035.png';" /></details>

<h5 id="用户调用接口"><a href="#用户调用接口" class="headerlink" title="用户调用接口"></a>用户调用接口</h5><p>文件系统为用户提供与文件及目录有关的调用，如新建、打开、读写、关闭、删除文件，建立、删除目录等。此层由若干程序模块组成，每个模块对应一条系统调用，用户发出系统调用时，控制即转入相应的模块。</p>
<h5 id="文件目录系统"><a href="#文件目录系统" class="headerlink" title="文件目录系统"></a>文件目录系统</h5><p>文件目录系统的主要功能是管理文件目录，其任务有管理活跃文件目录表、管理读写状态信息表、管理用户进程的打开文件表、管理与组织存储设备上的文件目录结构、调用下一级存取控制模块。</p>
<h5 id="存取控制验证模块"><a href="#存取控制验证模块" class="headerlink" title="存取控制验证模块"></a>存取控制验证模块</h5><p>实现文件保护主要由该级软件完成，它把用户的访问要求与FCB中指示的访问控制权限进行比较，以确认访问的合法性。</p>
<h5 id="逻辑文件系统与文件信息缓冲区"><a href="#逻辑文件系统与文件信息缓冲区" class="headerlink" title="逻辑文件系统与文件信息缓冲区"></a>逻辑文件系统与文件信息缓冲区</h5><p>逻辑文件系统与文件信息缓冲区的主要功能是，根据文件的逻辑结构将用户要读写的逻辑记录转换成文件逻辑结构内的相应块号。</p>
<h5 id="物理文件系统"><a href="#物理文件系统" class="headerlink" title="物理文件系统"></a>物理文件系统</h5><p>物理文件系统的主要功能是把逻辑记录所在的相对块号转换成实际的物理地址。</p>
<h5 id="辅助分配模块"><a href="#辅助分配模块" class="headerlink" title="辅助分配模块"></a>辅助分配模块</h5><p>分配模块的主要功能是管理辅存空间，即负责分配辅存空闲空间和回收辅存空间。</p>
<h5 id="设备管理程序模块"><a href="#设备管理程序模块" class="headerlink" title="设备管理程序模块"></a>设备管理程序模块</h5><p>设备管理程序模块的主要功能是分配设备、分配设备读写用缓冲区、磁盘调度、启动设备、处理设备中断、释放设备读写缓冲区、释放设备等。</p>
<p>对于文件管理系统的层次结构我们不能忽略，因为它是重要考点之一，当然也不需要死记硬背，我们可以通过用户请求访问某个文件时发生的一系列事情来辅助记忆文件系统的层次结构。</p>
<p>例如，用户要查看文件F中的内容，对操作系统发出命令（操作系统有面向用户的接口),于是就经过了第О级的用户调用接口。</p>
<p>操作系统得到命令后，需要查找目录以查找文件F的索引信息，可能是FCB，也可能是索引结点，经过了第1级文件目录系统。</p>
<p>通过目录找到文件FCB后，需要查看文件FCB上的信息，看看那个用户有没有访问该文件的权限，于是经过了存取控制验证模块。</p>
<p>用户通过验证后，就真正开始寻址。经历第3章的学习后，我们有这样的意识:操作系统的寻址往往要先得到逻辑地址，再得到物理地址，于是在开始寻址时，操作系统经过逻辑文件系统与文件信息缓冲区，得到了相应文件的内容的逻辑地址。</p>
<p>把 $\color{green}{\text{逻辑地址}}$ 转换为 $\color{green}{\text{物理地址}}$ ，是在 $\color{green}{\text{物理文件系统}}$ 中完成的。</p>
<p>至此为止，寻址就已完成。寻址完成后，我们关心的是找到的这块空间应该如何管理，若要释放这块空间，则任务就交给辅助分配模块，若要把这块空间分配给设备用于输入/输出，则把任务交给设备管理程序模块。</p>
<h4 id="目录实现"><a href="#目录实现" class="headerlink" title="目录实现"></a>目录实现</h4><p>在读文件前，必须先打开文件。打开文件时，操作系统利用路径名找到相应目录项，目录项中提供了查找文件磁盘块所需要的信息。目录实现的基本方法有线性列表和哈希表两种，要注意目录的实现就是为了查找，因此线性列表实现对应线性查找，哈希表的实现对应散列查找。</p>
<h5 id="线性列表"><a href="#线性列表" class="headerlink" title="线性列表"></a>线性列表</h5><p>最简单的目录实现方法是使用存储文件名和数据块指针的线性表。创建新文件时，必须首先搜索目录表以确定没有同名的文件存在，然后在目录表后增加一个目录项。删除文件则根据给定的文件名搜索目录表，接着释放分配给它的空间。重用目录项有许多方法:可以将目录项标记为不再使用，或将它加到空闲目录项表上，还可以将目录表中的最后一个目录项复制到空闲位置，并降低目录表长度。采用链表结构可以减少删除文件的时间，其优点在于实现简单，不过由于线性表的特殊性，比较费时。</p>
<h5 id="哈希表"><a href="#哈希表" class="headerlink" title="哈希表"></a>哈希表</h5><p>哈希表根据文件名得到一个值，并返回一个指向线性列表中元素的指针。这种方法的优点是查找非常迅速，插入和删除也较简单，不过需要一些预备措施来避免冲突。最大的困难是哈希表长度固定以及哈希函数对表长的依赖性。</p>
<p>目录查询是通过在磁盘上反复搜索完成的，需要不断地进行I/O操作，开销较大。所以如前所述，为了减少I/O操作，把当前使用的文件目录复制到内存，以后要使用该文件时只需在内存中操作，因此降低了磁盘操作次数，提高了系统速度。</p>
<h4 id="文件实现—文件分配方式"><a href="#文件实现—文件分配方式" class="headerlink" title="文件实现—文件分配方式"></a>文件实现—文件分配方式</h4><p>前面说过，文件实际上是一种抽象数据类型，我们要研究它的逻辑结构、物理结构以及关于它的一系列操作（不是统考关注的内容)。文件的实现就是研究文件的物理结构，即文件数据在物理存储设备上是如何分布和组织的。同一个问题有两个方面的回答:一是文件的分配方式，讲的是对磁盘非空闲块的管理;二是文件存储空间管理，讲的是对磁盘空闲块的管理。</p>
<p>文件分配对应于文件的物理结构，是指如何为文件分配磁盘块。常用的磁盘空间分配方法有三种:连续分配、链接分配和索引分配。有的系统（如RDOS操作系统）对三种方法都支持，但更普遍的是一个系统只支持一种方法。对于本节的内容，读者要注意与文件的逻辑结构区分，从历年的经验来看，这是很多读者容易搞混的地方（读者复习完数据结构后，应该了解 $\color{green}{\text{线性表}}$ 、 $\color{green}{\text{顺序表}}$ 和 $\color{green}{\text{链表之间}}$ 的关系，类比到这里就不易混淆)。</p>
<h5 id="连续分配"><a href="#连续分配" class="headerlink" title="连续分配"></a>连续分配</h5><p>连续分配方法要求每个文件在磁盘上占有一组连续的块，如图4.10所示。磁盘地址定义了磁盘上的一个线性排序。这种排序使作业访问磁盘时需要的寻道数和寻道时间最小。</p>
<details><summary>图4.10连续分配</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211006111559.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211006111559.png';" /></details>

<p>文件的连续分配可以用第一块的磁盘地址和连续块的数量来定义。若文件长 $n$ 块并从位置 $b$ 开始，则该文件将占有块b,b+ 1,b+2,…,b+n-1。一个文件的目录条目包括开始块的地址和该文件所分配区域的长度。</p>
<p>连续分配支持顺序访问和直接访问。其优点是实现简单、存取速度快。缺点是文件长度不宜动态增加，因为一个文件末尾后的盘块可能已分配给其他文件，一旦需要增加，就需要大量移动盘块。此外，反复增删文件后会产生外部碎片（与内存管理分配方式中的碎片相似)，且很难确定一个文件需要的空间大小，因而只适用于 $\color{green}{\text{长度固定}}$ 的文件。</p>
<h5 id="链接分配"><a href="#链接分配" class="headerlink" title="链接分配"></a>链接分配</h5><p>链接分配采取离散分配的方式，消除了外部碎片，因此显著提高了磁盘空间的利用率;又因为根据文件的当前需求为其分配必需的盘块，当文件动态增长时，可以动态地再为它分配盘块，因此无须事先知道文件的大小。此外，对文件的增、删、改也非常方便。链接分配又可以分为隐式链接和显式链接两种形式。</p>
<p>隐式链接如图4.11所示。每个文件对应一个磁盘块的链表;磁盘块分布在磁盘的任何地方，除最后一个盘块外，每个盘块都有指向下一个盘块的指针，这些指针对用户是透明的。目录包括文件第一块的指针和最后一块的指针。</p>
<details><summary>图4.11隐式链接分配
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211006111710.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211006111710.png';" /></details>

<p>创建新文件时，目录中增加一个新条目。每个目录项都有一个指向文件首块的指针。该指针初始化为NULL 以表示空文件，大小字段为0。写文件会通过空闲空间管理系统找到空闲块，将该块链接到文件的尾部，以便写入。读文件则通过块到块的指针顺序读块。</p>
<p>$\color{green}{\text{隐式链接}}$ 分配的缺点是无法直接访问盘块，只能通过指针顺序访问文件，且盘块指针会消耗一定的存储空间。隐式链接分配的稳定性也是一个问题，系统在运行过程中由于软件或硬件错误导致链表中的指针丢失或损坏，会导致文件数据的丢失。</p>
<p>$\color{green}{\text{显式链接}}$ 是指把用于链接文件各物理块的指针，从每个物理块的块末尾中提取出来，显式地存放在内存的一张链接表中。该表在整个磁盘中仅设置一张，称为文件分配表(File AllocationTable，FAT)。每个表项中存放对应块的下一块链接指针，即下一个盘块号。文件的第一个盘块号记录在目录中，后续的盘块可通过查FAT找到。例如，某磁盘共有100个磁盘块，存放了两个文件:文件“aa”占三个盘块，依次是2→8→5;文件“bbb”占两个盘块，依次是7→1。其余盘块都是空闲盘块，则该磁盘的FAT表如图4.12所示。</p>
<details><summary>图4.12文件分配表
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211006111831.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211006111831.png';" /></details>

<p>不难看出，FAT的表项与全部磁盘块一一对应，并且可以用一个特殊的数字-1表示文件的最后一块，用-2表示这个磁盘块是空闲的（当然也可指定为-3,-4)。因此，文件分配表（FAT）不仅记录了文件各块之间的先后链接关系，同时还标记了空闲的磁盘块，操作系统也可以通过FAT对文件存储空间进行管理。当某进程请求操作系统分配一个磁盘块时，操作系统只需从FAT中找到-2的表项,并将对应的磁盘块分配给进程即可。</p>
<p>FAT表在系统启动时就会被读入内存，因此查找FAT的过程是在内存中进行的，因此不仅显著地提高了检索速度，而且明显减少了访问磁盘的次数。</p>
<h5 id="索引分配"><a href="#索引分配" class="headerlink" title="索引分配"></a>索引分配</h5><p>链接分配解决了连续分配的外部碎片和文件大小管理的问题。但是，链接分配不能有效支持直接访问(FAT 除外)。索引分配解决了这个问题，它把每个文件的所有的盘块号都集中放在一起构成索引块（表)，如图4.13所示。</p>
<details><summary>图4.13 索引分配
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211006113135.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211006113135.png';" /></details>

<p>每个文件都有其 $\color{green}{\text{索引块}}$ ，这是一个磁盘块地址的数组。索引块的第i个条目指向文件的第i个块。目录条目包括索引块的地址。要读第 i块，通过索引块的第 i个条目的指针来查找和读入所需的块。</p>
<p>创建文件时，索引块的所有指针都设为空。首次写入第i块时，先从空闲空间中取得一个块，再将其地址写到索引块的第i个条目。索引分配支持直接访问，且没有外部碎片问题。其缺点是由于索引块的分配，增加了系统存储空间的开销。索引块的大小是一个重要的问题，每个文件必须有一个索引块，因此索引块应尽可能小，但索引块太小就无法支持大文件。可以采用以下机制来处理这个问题。</p>
<ul>
<li>链接方案。一个索引块通常为一个磁盘块，因此它本身能直接读写。为了处理大文件，可以将多个索引块链接起来。</li>
<li>多层索引。多层索引使第一层索引块指向第二层的索引块，第二层索引块再指向这种方法根据最大文件大小的要求，可以继续到第三层或第四层。例如，4096B 的块，能在索引块中存入1024个4B的指针。两层索引允许1048576个数据块，即允许最大文件为4GB。</li>
<li>混合索引。将多种索引分配方式相结合的分配方式。例如，系统既采用直接地址单级索引分配方式或两级索引分配方式（混合索引是本章最综合的高频考点，可先学习本章疑难点4，然后回来接着学习)。</li>
</ul>
<p>表4.2是三种分配方式的比较。</p>
<details><summary>表4.2文件三种分配方式的比较</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211006113337.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211006113337.png';" /></details>

<p>此外，访问文件需要两次访问外存——首先要读取索引块的内容，然后访问具体的磁盘块，因而降低了文件的存取速度。为了解决这一-问题，通常将文件的索引块读入内存的缓冲区中，以加快文件的访问速度。</p>
<h4 id="文件实现————文件存储空间管理"><a href="#文件实现————文件存储空间管理" class="headerlink" title="文件实现————文件存储空间管理"></a>文件实现————文件存储空间管理</h4><p>(1）文件存储器空间的划分与初始化</p>
<p>一般来说，一个文件存储在一个文件卷中。文件卷可以是物理盘的一部分，也可以是整个物理盘，支持超大型文件的文件卷也可由多个物理盘组成，如图4.14所示。</p>
<p>在一个文件卷中，文件数据信息的空间(文件区）和存放文件控制信息FCB的空间(目录区)是分离的。由于存在很多种类的文件表示和存放格式，所以现代操作系统中一般都有很多不同的文件管理模块，通过它们可以访问不同格式的逻辑卷中的文件。逻辑卷在提供文件服务前，必须由对应的文件程序进行初始化，划分好目录区和文件区,建立空闲空间管理表格及存放逻辑卷信息的 $\color{green}{\text{超级块}}$ 。</p>
<details><summary>图4.14逻辑卷与物理盘的关系</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211006113519.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211006113519.png';" /></details>

<p>(2）文件存储器空间管理</p>
<p>文件存储设备分成许多大小相同的物理块，并以 $\color{green}{\text{块}}$ 为单位交换信息，因此，文件存储设备的管理实质上是对空闲块的组织和管理，它包括空闲块的组织、分配与回收等问题。</p>
<h5 id="空闲表法"><a href="#空闲表法" class="headerlink" title="空闲表法"></a>空闲表法</h5><p>空闲表法属于连续分配方式，它与内存的动态分配方式类似，为每个文件分配一块连续的存储空间。系统为外存上的所有空闲区建立一张空闲盘块表，每个空闲区对应于一个空闲表项，其中包括表项序号、该空闲区第一个盘块号、该区的空闲盘块数等信息。再将所有空闲区按其起始盘块号递增的次序排列，如表4.3所示。</p>
<p>空闲盘区的分配与内存的动态分配类似，同样采用首次适应算法、循环首次适应算法等。例如，在系统为某新创建的文件分配空闲盘块时，先顺序地检索空闲盘块表的各表项，直至找到第一个其大小能满足要求的空闲区，再将该盘区分配给用户，同时修改空闲盘块表。</p>
<p>系统在对用户所释放的存储空间进行回收时，也采取类似于内存回收的方法，即要考虑回收区是否与空闲表中插入点的前区和后区相邻接，对相邻接者应予以合并。</p>
<details><summary>表4.3空闲盘块表
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211006113633.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211006113633.png';" /></details>

<h5 id="空闲链表法"><a href="#空闲链表法" class="headerlink" title="空闲链表法"></a>空闲链表法</h5><p>将所有空闲盘区拉成一条空闲链,根据构成链所用的基本元素不同,可把链表分成两种形式:空闲盘块链和空闲盘区链。</p>
<p>空闲盘块链将磁盘上的所有空闲空间以盘块为单位拉成–条链。当用户因创建文件而请求分配存储空间时，系统从链首开始，依次摘下适当数目的空闲盘块分配给用户。当用户因删除文件而释放存储空间时，系统将回收的盘块依次插入空闲盘块链的末尾。这种方法的优点是分配和回收一个盘块的过程非常简单，但在为一个文件分配盘块时可能要重复多次操作。</p>
<p>空闲盘区链将磁盘上的所有空闲盘区（每个盘区可包含若干盘块）拉成一条链。在每个盘区上除含有用于指示下一个空闲盘区的指针外，还应有能指明本盘区大小(盘块数）的信息。分配盘区的方法与内存的动态分区分配类似，通常采用首次适应算法。在回收盘区时，同样也要将回收区与相邻接的空闲盘区合并。</p>
<h5 id="位示图法"><a href="#位示图法" class="headerlink" title="位示图法"></a>位示图法</h5><p>位示图利用二进制的一位来表示磁盘中一个盘块的使用情况，磁盘上所有的盘块都有一个二进制位与之对应。当其值为“0”时，表示对应的盘块空闲;当其值为“1”时，表示对应的盘块已分配。位示图法示意如图4.15所示。</p>
<details><summary>图4.15位示图法示意图
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211006113830.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211006113830.png';" /></details>

<p>盘块的分配:</p>
<p>${\textstyle\unicode{x2460}}$  顺序扫描位示图，从中找出一个或一组其值为“0”的二进制位。</p>
<p>②将找到的一个或一组二进制位，转换成与之对应的盘块号。若找到的其值为“0O”的二进制位位于位示图的第 $i$ 行、第 $j$ 列,则其相应的盘块号应按下式计算( $n$ 代表每行的位数):</p>
<p>$$<br>b= n(i- 1)+j<br>$$ </p>
<p>③修改位示图，令map[ i,j ]= 1。盘块的回收:</p>
<p>①将回收盘块的盘块号转换成位示图中的行号和列号。转换公式为</p>
<p>$$<br>i=(b -1) DIV n+1<br>$$  </p>
<p>$$<br>j=(b-1)MOD n+1<br>$$  </p>
<p>②修改位示图，令map[ i,j ]=0。</p>
<h5 id="成组链接法"><a href="#成组链接法" class="headerlink" title="成组链接法"></a>成组链接法</h5><p>空闲表法和空闲链表法都不适用于大型文件系统，因为这会使空闲表或空闲链表太大。在UNIX系统中采用的是成组链接法，这种方法结合了空闲表和空闲链表两种方法，克服了表太大的缺点。其大致思想是:把顺序的n个空闲扇区地址保存在第一个空闲扇区内，其后一个空闲扇区内则保存另一顺序空闲扇区的地址，如此继续，直至所有空闲扇区均予以链接。系统只需要保存一个指向第一个空闲扇区的指针。假设磁盘最初全为空闲扇区，其成组链接如图4.16所示。通过这种方式可以迅速找到大批空闲块地址。</p>
<details><summary>图4.16成组链接法示意图
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211013181718.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211013181718.png';" /></details>

<p>表示文件存储器空闲空间的“位向量”表或第一个成组链块，以及卷中的目录区、文件区划分信息都需要存放在辅存储器中，一般放在卷头位置，在UNIX系统中称为超级块。在对卷中的文件进行操作前，超级块需要预先读入系统空闲的主存，并且经常保持主存超级块与辅存卷中超级块的一致性。</p>
<blockquote>
<p>注意:本书如无特别提示，所使用的位示图法中行和列都从1开始编号。特别注意，若题目中指明从0开始编号，则上述计算方法要进行相应调整。</p>
</blockquote>
<h4 id="本节小结-1"><a href="#本节小结-1" class="headerlink" title="本节小结"></a>本节小结</h4><p>本节开头提出的问题的参考答案如下。</p>
<h5 id="在目录中查找某个文件可以使用什么方法"><a href="#在目录中查找某个文件可以使用什么方法" class="headerlink" title="在目录中查找某个文件可以使用什么方法?"></a>在目录中查找某个文件可以使用什么方法?</h5><p>可以采用线性列表法或哈希表法。线性列表把文件名组织成一个线性表，查找时依次与线性表中的每个表项进行比较。若把文件名按序排列，则使用折半查找法可以降低平均的查找时间，但建立新文件时会增加维护线性表的开销。哈希表用文件名通过哈希函数得到一个指向文件的指针，这种方法非常迅速，但要注意避免冲突。</p>
<h5 id="文件的逻辑结构和物理结构有何区别-单个文件的逻辑结构和物理结构之间是否存在着某些制约关系"><a href="#文件的逻辑结构和物理结构有何区别-单个文件的逻辑结构和物理结构之间是否存在着某些制约关系" class="headerlink" title="文件的逻辑结构和物理结构有何区别?单个文件的逻辑结构和物理结构之间是否存在着某些制约关系?"></a>文件的逻辑结构和物理结构有何区别?单个文件的逻辑结构和物理结构之间是否存在着某些制约关系?</h5><p>文件的逻辑结构是用户可见的结构，即用户使用文件的结构。文件的物理结构是文件在存储器上的组织结构，它表示一个文件在辅存上安置、链接、编目的方法。它和文件的存取方法以及辅存设备的特性等都有着密切的联系。单个文件的逻辑结构和物理结构之间虽无明显的制约或关联关系，但是如果物理结构选择不慎，也很难体现出逻辑结构的特点，比如一个逻辑结构是顺序结构，而物理结构是隐式链接结构的文件，即使理论上可以很快找出某条记录的地址，而实际找时仍然需要在磁盘上一块一块地找。</p>
<h3 id="磁盘组织与管理"><a href="#磁盘组织与管理" class="headerlink" title="磁盘组织与管理"></a>磁盘组织与管理</h3><p>在学习本节时，请读者思考以下问题:</p>
<p>1）在磁盘上进行一次读写操作需要哪几部分时间?其中哪部分时间最长?</p>
<p>2）存储一个文件时，当一个磁道存储不下时，剩下部分是存在同一个盘面的不同磁道好，还是存在同一个柱面上的不同盘面好?</p>
<p>本节主要介绍文件系统管理磁盘的方式，由于内容较少且属于实现部分，不需要问题来引导学习，因此本节不给出问题。学习本节时，要重点掌握计算一次磁盘操作的时间，以及对于给定访盘的磁道序列，按照特定算法求出磁头通过的总磁道数及平均寻道数。</p>
<h4 id="磁盘的结构"><a href="#磁盘的结构" class="headerlink" title="磁盘的结构"></a>磁盘的结构</h4><p>磁盘（Disk）是由表面涂有磁性物质的金属或塑料构成的圆形盘片，通过一个称为磁头的导体线圈从磁盘存取数据。在读/写操作期间，磁头固定，磁盘在下面高速旋转。如图4.17所示，磁盘盘面上的数据存储在一组同心圆中，称为 $\color{green}{\text{磁道}}$ 。每个磁道与磁头一样宽，一个盘面有上千个磁道。磁道又划分为几百个扇区，每个扇区固定存储大小(通常为512B)，一个 $\color{green}{\text{扇区}}$ 称为一个 $\color{green}{\text{盘块}}$ 。相邻磁道及相邻扇区间通过一定的间隙分隔开，以避免精度错误。注意，由于扇区按固定圆心角度划分，所以密度从最外道向里道增加，磁盘的存储能力受限于最内道的最大记录密度。</p>
<p>磁盘安装在一个磁盘驱动器中，它由磁头臂、用于旋转磁盘的主轴和用于数据输入/输出的电子设备组成。如图4.18所示，多个盘片垂直堆叠，组成 $\color{green}{\text{磁盘组}}$ ，每个盘面对应一个磁头，所有磁头固定在一起，与磁盘中心的距离相同且一起移动。所有盘片上相对位置相同的磁道组成柱面。按照这种物理结构组织，扇区就是磁盘可寻址的最小存储单位，磁盘地址用“柱面号·盘面号·扇区号(或块号)”表示。</p>
<details><summary>图4.17磁盘盘片
图4.18 磁盘驱动器
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211006114533.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211006114533.png';" /></details>

<p>磁盘按不同的方式可分为若干类型:磁头相对于盘片的径向方向固定的，称为固定头磁盘，每个磁道一个磁头;磁头可移动的，称为活动头磁盘，磁头臂可来回伸缩定位磁道;磁盘永久固定在磁盘驱动器内的，称为固定盘磁盘;可移动和替换的，称为可换盘磁盘。</p>
<p>前面说过,操作系统中几乎每介绍一类资源及对这类资源的管理时,都要涉及一类调度算法。用户访问文件，需要操作系统的服务，文件实际上存储在磁盘中，操作系统接收用户的命令后，经过一系列的检验访问权限和寻址过程后，最终都会到达磁盘，控制磁盘把相应的数据信恳读出或修改。当有多个请求同时到达时，操作系统就要决定先为哪个请求服务，这就是磁盘调度算法<br>要解决的问题。</p>
<h4 id="磁盘调度算法"><a href="#磁盘调度算法" class="headerlink" title="磁盘调度算法"></a>磁盘调度算法</h4><p>一次磁盘读写操作的时间由寻找（寻道)时间、旋转延迟时间和传输时间决定。</p>
<p>1）寻找时间 $T_s$ ,。活动头磁盘在读写信息前，将磁头移动到指定磁道所需要的时间。这个时间除跨越 $n$ 条磁道的时间外，还包括启动磁臂的时间 $s$ ，即</p>
<p>$$<br>T_s = m \times n + s<br>$$  </p>
<p>式中，m是与磁盘驱动器速度有关的常数，约为0.2ms，磁臂的启动时间约为2ms。</p>
<p>2）旋转延迟时间 $T_s$ 。磁头 $\color{green}{\text{定位}}$ 到某一磁道的扇区所需要的时间，设磁盘的旋转速度为 $r$ ,则</p>
<p>$$<br>T_r = \dfrac{1}{2r}<br>$$  </p>
<p>对于硬盘，典型的旋转速度为5400转/分，相当于一周11.1ms，则 $T_r$ 为5.55ms;对于软盘，其旋转速度为300～600转/分，则 $T_r$ 为50～100ms。</p>
<p>3）传输时间 $T_t$ 。从磁盘读出或向磁盘写入数据所经历的时间，这个时间取决于每次所读/写的字节数 $b$ 和磁盘的旋转速度:</p>
<p>$$<br>T_t = \dfrac{b}{rN}<br>$$  </p>
<p>式中， $r$ 为磁盘每秒的转数， $N$ 为一个磁道上的字节数。</p>
<p>在磁盘存取时间的计算中，寻道时间与磁盘调度算法相关，下面将会介绍分析几种算法;而延迟时间和传输时间都与磁盘旋转速度相关，且为 $\color{green}{\text{线性相关}}$ ，所以在硬件上，转速是磁盘性能的一个非常重要的参数。</p>
<p>总平均存取时间 $T_a$ 可以表示为</p>
<p>$$<br>T_a = T_s + \dfrac{1}{2r} + \dfrac{b}{rN}<br>$$  </p>
<p>虽然这里给出了总平均存取时间的公式，但是这个平均值是没有太大实际意义的，因为在实际的磁盘IO操作中，存取时间与磁盘调度算法密切相关。调度算法直接决定寻找时间从而决定总的存取时间。</p>
<p>目前常用的磁盘调度算法有以下几种。</p>
<h5 id="先来先服务（First-Come-First-Served，FCFS-算法"><a href="#先来先服务（First-Come-First-Served，FCFS-算法" class="headerlink" title="先来先服务（First Come First Served，FCFS)算法"></a>先来先服务（First Come First Served，FCFS)算法</h5><p>FCFS算法根据进程请求访问磁盘的先后顺序进行调度，这是一种最简单的调度算法，如图4.19所示。该算法的优点是具有公平性。若只有少量进程需要访问，且大部分请求都是访问簇聚的文件扇区，则有望达到较好的性能;若有大量进程竞争使用磁盘，则这种算法在性能上往往接近于随机调度。所以，实际磁盘调度中会考虑一些更为复杂的调度算法。</p>
<details><summary>图4.19 FCFS磁盘调度算法
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211006115109.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211006115109.png';" /></details>

<p>例如，磁盘请求队列中的请求顺序分别为55,58,39,18,90,160,150,38,184，磁头的初始位置是磁道100,采用FCFS算法时磁头的运动过程如图4.19所示。磁头共移动了(45＋3+19+21+72＋70＋10+112+146)= 498个磁道，平均寻找长度=498/9=55.3。</p>
<h5 id="最短寻找时间优先-Shortest-Seek-Time-First-SSTF-算法"><a href="#最短寻找时间优先-Shortest-Seek-Time-First-SSTF-算法" class="headerlink" title="最短寻找时间优先( Shortest Seek Time First, SSTF)算法"></a>最短寻找时间优先( Shortest Seek Time First, SSTF)算法</h5><p>SSTF算法选择调度处理的磁道是与当前磁头所在磁道距离最近的磁道，以便使每次的寻找时间最短。当然，总是选择最小寻找时间并不能保证平均寻找时间最小，但能提供比 FCFS算法更好的性能。这种算法会产生“饥饿”现象。如图4.20所示，若某时刻磁头正在18号磁道，而在18号磁道附近频繁地增加新的请求，则SSTF 算法使得磁头长时间在18号磁道附近工作，将使184号磁道的访问被无限期地延迟，即被“饿死”。</p>
<details><summary>4.20 SSTF磁盘调度算法
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211006115252.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211006115252.png';" /></details>

<p>例如，磁盘请求队列中的请求顺序分别为55,58,39,18,90,160,150,38,184，磁头初始位置是磁道100，采用SSTF算法时磁头的运动过程如图4.20所示。磁头共移动了10+32+3+16+1＋ 20+132+10＋24= 248个磁道，平均寻找长度=248/9 = 27.5。</p>
<h5 id="扫描（SCAN-算法（又称电梯调度算法"><a href="#扫描（SCAN-算法（又称电梯调度算法" class="headerlink" title="扫描（SCAN)算法（又称电梯调度算法)"></a>扫描（SCAN)算法（又称电梯调度算法)</h5><ul>
<li>scan求移动的磁道数的快速方法：找到「 $\color{green}{\text{枢纽}}$ 」，两边减去枢纽</li>
</ul>
<p>SCAN算法在磁头当前移动方向上选择与当前磁头所在磁道距离最近的请求作为下一次服务的对象，实际上就是在最短寻找时间优先算法的基础上规定了磁头运动的方向，如图4.21所示。由于磁头移动规律与电梯运行相似，因此又称电梯调度算法。SCAN算法对最近扫描过的区域不公平，因此它在访问局部性方面不如 FCFS 算法和SSTF算法好。</p>
<details><summary>图4.21SCAN磁盘调度算法
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211006115411.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211006115411.png';" /></details>

<p>例如，磁盘请求队列中的请求顺序分别为55,58, 39,18,90,160,150,38,184，磁头初始位置是磁道100。采用SCAN算法时，不但要知道磁头的当前位置，而且要知道磁头的移动方向，假设磁头沿磁道号增大的顺序移动，则磁头的运动过程如图4.21所示。移动磁道的顺序为100,150,160,184,200,90,58,55,39,38,18。磁头共移动了(50+10+24+16+110＋32+3+16+1+20)=282个磁道，平均寻道长度=282/9= 31.33。</p>
<h5 id="循环扫描-Circular-SCAN-C-SCAN）算法"><a href="#循环扫描-Circular-SCAN-C-SCAN）算法" class="headerlink" title="循环扫描( Circular SCAN, C-SCAN）算法"></a>循环扫描( Circular SCAN, C-SCAN）算法</h5><p>在扫描算法的基础上规定磁头单向移动来提供服务，回返时直接快速移动至起始端而不服务任何请求。由于SCAN算法偏向于处理那些接近最里或最外的磁道的访问请求，所以使用改进型的C-SCAN 算法来避免这个问题，如图4.22所示。</p>
<p>采用SCAN算法和C-SCAN 算法时，磁头总是严格地遵循从盘面的一端到另一端，显然，在实际使用时还可以改进，即磁头移动只需要到达最远端的一个请求即可返回,不需要到达磁盘端点。这种形式的SCAN算法和C-SCAN算法称为LOOK调度（见图4.23-1)和C-LOOK(见图4.23-2)调度，因为它们在朝一个给定方向移动前会查看是否有请求。</p>
<details><summary>4.22 C-SCAN磁盘调度算法
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211006115523.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211006115523.png';" /></details>

<details><summary>4.23-1 LOOK磁盘调度算法
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211006115543.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211006115543.png';" /></details>

<p>注意,若无特别说明,也可以默认SCAN算法和C-SCAN算法为LOOK和C-LOOK调度(请读者认真领悟，并通过结合后面的习题进一步加深对以上相关算法的理解)。</p>
<details><summary>4.23-2C-LOOK磁盘调度算法
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211006115610.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211006115610.png';" /></details>

<p>例如，磁盘请求队列中的请求顺序为55,58,39,18,90,160,150,38,184，磁头初始位置是磁道100。采用C-SCAN算法时，假设磁头沿磁道号增大的顺序移动，则磁头的运动过程如图4.23所示。移动磁道的顺序为100,150,160,184,200,0,18,38,39,55,58,90。磁头共移动50＋10＋24＋16+200+ 18＋20+1+16＋3+32=390个磁道，平均寻道长度=390/9= 43.33。</p>
<p>不太熟悉操作系统整体框架的读者经常混淆磁盘调度算法中的循环扫描算法和页面调度算法中的CLOCK算法，请读者注意区分。</p>
<p>对比以上几种磁盘调度算法，FCFS 算法太过简单， $\color{green}{\text{性能较差}}$ ，仅在请求队列长度接近于1时才较为理想;SSTF 算法较为通用和自然;SCAN算法和C-SCAN算法在磁盘 $\color{green}{\text{负载较大}}$ 时比较占优势。它们之间的比较见表4.4。</p>
<details><summary>表4.4磁盘调度算法比较
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211006115642.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211006115642.png';" /></details>

<p>除减少寻找时间外，减少延迟时间也是提高磁盘传输效率的重要因素。可以对盘面扇区进行交替编号，对磁盘片组中的不同盘面错位命名。假设每个盘面有8个扇区，磁盘片组共8个盘面,则可以采用如图4.24所示的编号。</p>
<p>磁盘是连续自转设备，磁头读/写一个物理块后，需要经过短暂的处理时间才能开始读/马下一块。假设逻辑记录数据连续存放在磁盘空间中，若在盘面上按扇区交替编号连续存放,则连续读/写多条记录时能减少磁头的延迟时间;同柱面不同盘面的扇区若能错位编号，连续读/写相邻两个盘面的逻辑记录时也能减少磁头延迟时间。</p>
<details><summary>图4.24 磁盘片组扇区编号
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211006115719.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211006115719.png';" /></details>

<p>以图4.24为例，在随机扇区访问情况下，定位磁道中的一个扇区平均需要转过4个扇区，这时，延迟时间是传输时间的4倍，这是一种非常低效的存取方式。理想化的情况是不需要定位而直接连续读取扇区，没有延迟时间，这样磁盘数据存取效率可以成倍提高。但由于读取扇区的顺序是不可预测的，所以延迟时间不可避免。图4.24中的编号方式是读取连续编号扇区时的一种方法。</p>
<p>磁盘寻块时间分为三个部分，即寻道时间、延迟时间和传输时间，寻道时间和延迟时间属于“找”的时间，凡是“找”的时间都可以通过一定的方法削减，但传输时间是磁盘本身性质所决定的，不能通过一定的措施减少。</p>
<h4 id="磁盘的管理"><a href="#磁盘的管理" class="headerlink" title="磁盘的管理"></a>磁盘的管理</h4><h5 id="磁盘初始化"><a href="#磁盘初始化" class="headerlink" title="磁盘初始化"></a>磁盘初始化</h5><p>一个新的磁盘只是一个含有磁性记录材料的空白盘。在磁盘能存储数据之前，它必须分成扇区以便磁盘控制器能进行读和写操作，这个过程称为 $\color{green}{\text{低级格式化}}$ （物理分区)。低级格式化为磁盘的每个扇区采用特别的数据结构。每个扇区的数据结构通常由 $\color{green}{\text{头}}$ 、 $\color{green}{\text{数据区域}}$ (通常为512B大小）和 $\color{green}{\text{尾部}}$ 组成。头部和尾部包含了一些磁盘控制器所使用的信息。</p>
<p>为了使用磁盘存储文件，操作系统还需要将自己的数据结构记录在磁盘上:第一步将磁盘分为由一个或多个柱面组成的 $\color{green}{\text{分区}}$ （即我们熟悉的C盘、D盘等形式的分区);第二步对物理分区进行 $\color{green}{\text{逻辑格式化}}$ （创建文件系统)，操作系统将初始的文件系统数据结构存储到磁盘上,这些数据结构包括空闲和已分配的空间及一个初始为空的目录。</p>
<h5 id="引导块"><a href="#引导块" class="headerlink" title="引导块"></a>引导块</h5><p>计算机启动时需要运行一个初始化程序（ $\color{green}{\text{自举程序}}$ )，它初始化CPU、寄存器、设备控制器和内存等，接着启动操作系统。为此，该自举程序应找到磁盘上的操作系统内核，装入内存，并转到起始地址，从而开始操作系统的运行。</p>
<p>自举程序通常保存在ROM 中，为了避免改变自举代码而需要改变ROM硬件的问题，因此只在ROM中保留很小的自举装入程序，将完整功能的自举程序保存在磁盘的启动块上，启动块位于磁盘的固定位。拥有启动分区的磁盘称为启动磁盘或系统磁盘。</p>
<h5 id="坏块"><a href="#坏块" class="headerlink" title="坏块"></a>坏块</h5><p>由于磁盘有移动部件且容错能力弱，因此容易导致一个或多个扇区损坏。部分磁盘甚至从出厂时就有坏扇区。根据所使用的磁盘和控制器，对这些块有多种处理方式。</p>
<p>对于简单磁盘，如电子集成驱动器(IDE)，坏扇区可手工处理，如 MS-DOS的Format 命令执行逻辑格式化时便会扫描磁盘以检查坏扇区。坏扇区在FAT表上会标明，因此程序不会使用。</p>
<p>对于复杂的磁盘，如小型计算机系统接口(SCSI)，其控制器维护一个磁盘坏块链表该链表在出厂前进行低级格式化时就已初始化，并在磁盘的整个使用过程中不断更新。低级格式化将一些块保留作为备用，对操作系统透明。控制器可用备用块来逻辑地替代坏块，这种方案称为 $\color{green}{\text{扇区备用}}$ 。</p>
<p>对坏块的处理实质上就是用某种机制，使系统不去使用坏块。坏块属于硬件故障，操作系统是不能修复坏块的。</p>
<h4 id="本节小结-2"><a href="#本节小结-2" class="headerlink" title="本节小结"></a>本节小结</h4><p>本节开头提出的问题的参考答案如下。</p>
<h5 id="在磁盘上进行一次读写操作需要哪几部分时间-其中哪部分时间最长"><a href="#在磁盘上进行一次读写操作需要哪几部分时间-其中哪部分时间最长" class="headerlink" title="在磁盘上进行一次读写操作需要哪几部分时间?其中哪部分时间最长?"></a>在磁盘上进行一次读写操作需要哪几部分时间?其中哪部分时间最长?</h5><p>在磁盘上进行一次读写操作花费的时间由寻道时间、延迟时间和传输时间决定。其中寻道时间是将磁头移动到指定磁道所需要的时间，延迟时间是磁头定位到某一磁道的扇区（块号）所需要的时间，传输时间是从磁盘读出或向磁盘写入数据所经历的时间。一般来说， $\color{green}{\text{寻道时间}}$ 因为要移动磁臂，所以占用时间最长。</p>
<h5 id="存储一个文件时，当一个磁道存储不下时，剩下部分是存在同一个盘面的不同磁道好，还是存在同一个柱面上的不同盘面好"><a href="#存储一个文件时，当一个磁道存储不下时，剩下部分是存在同一个盘面的不同磁道好，还是存在同一个柱面上的不同盘面好" class="headerlink" title="存储一个文件时，当一个磁道存储不下时，剩下部分是存在同一个盘面的不同磁道好，还是存在同一个柱面上的不同盘面好?"></a>存储一个文件时，当一个磁道存储不下时，剩下部分是存在同一个盘面的不同磁道好，还是存在同一个柱面上的不同盘面好?</h5><p>上一问已经说到，寻道时间对于一次磁盘访问的影响是最大的，若存在同一个盘面的不同磁道，则磁臂势必要移动，这样会大大增加文件的访问时间，而存在同一个柱面上的不同盘面就不需要移动磁道，所以一般情况下存在同一个柱面上的不同盘面更好。</p>
<h3 id="本章疑难点"><a href="#本章疑难点" class="headerlink" title="本章疑难点"></a>本章疑难点</h3><h4 id="磁盘结构"><a href="#磁盘结构" class="headerlink" title="磁盘结构"></a>磁盘结构</h4><p>引导控制块（Boot Control Block)包括系统从该分区引导操作系统所需要的信息。若磁盘没有操作系统，则这块的内容为空。它通常为分区的第一块。UFS称为引导块(Boot Block);NTFS称为分区引导扇区(Partition Boot Sector)。</p>
<p>分区控制块(Partition Control Block)包括分区详细信息，如分区的块数、块的大小、空闲块的数量和指针、空闲FCB 的数量和指针等。UFS称为超级块(Super Block)，而NTFS称为主控文件表(Master File Table)。</p>
<h4 id="内存结构"><a href="#内存结构" class="headerlink" title="内存结构"></a>内存结构</h4><p>内存分区表包含所有安装分区的信息。</p>
<p>内存目录结构用来保存近来访问过的目录信息。对安装分区的目录，可以包括一个指向分区表的指针。</p>
<p>系统范围的打开文件表，包括每个打开文件的FCB复制和其他信息。</p>
<p>单个进程的打开文件表，包括一个指向系统范围内已打开文件表中合适条目和其他信息的指针。</p>
<h4 id="文件系统实现概述"><a href="#文件系统实现概述" class="headerlink" title="文件系统实现概述"></a>文件系统实现概述</h4><p>为了创建一个文件，应用程序调用逻辑文件系统。逻辑文件系统知道目录结构形式，它将为文件分配一个新的 FCB，把相应目录读入内存，用新的文件名更新该目录和FCB，并将结果写回磁盘。图4.25显示了一个典型的FCB。</p>
<details><summary>图4.25典型的FCB
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211006120305.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211006120305.png';" /></details>

<p>一旦文件被创建,它就能用于IO,不过首先要打开文件。调用open将文件名传给文件系统，文件系统根据给定文件名搜索目录结构。部分目录结构通常缓存在内存中以加快目录操作。找到文件后，其FCB复制到系统范围的打开文件表。该表不但存储FCB，而且存储打开该文件的进程数量的条目。</p>
<p>然后，单个进程的打开文件表中会增加一个条目，并通过指针将系统范围的打开文件表的条目与其他域(文件当前位置的指针和文件打开模式等)相连。调用open返回的是一个指向单个进程的打开文件表中合适条目的指针，所以文件操作都是通过该指针进行的。</p>
<p>文件名不必是打开文件表的一部分，因为一旦完成对FCB在磁盘上的定位，系统就不再使用文件名。对于访问打开文件表的索引，UNIX称之为文件描述符(File Descriptor)，而Windows2000称之为文件句柄(Fiie Handle)。因此，只要文件未被关闭，所有文件操作就通过打开文件表来进行。</p>
<p>当一个进程关闭文件时，就会删除单个进程打开文件表中的一个相应条目，即目录项，系统范围内打开文件表的打开数也会递减。当打开文件的所有用户都关闭了一个文件时，更新的文件信息会复制到磁盘的目录结构中，系统范围的打开文件表的条目也将删除。</p>
<p>在实际中，系统调用open 时会首先搜索系统范围的打开文件表，以确定某文件是否已被其他进程所使用。如果是，就在单个进程的打开文件表中创建一项，并指向现有系统范围的打开文件表的相应条目。该算法在文件已打开时，能节省大量开销。</p>
<h4 id="混合索引分配的实现"><a href="#混合索引分配的实现" class="headerlink" title="混合索引分配的实现"></a>混合索引分配的实现</h4><p>混合索引分配已在UNIX系统中采用。在UNIX System V的索引结点中，共设置了13个地址项，即 iaddr(0)~iaddr(12)，如图4.26所示。在BSD UNIX 的索引结点中，共设置了13个地址项，它们都把所有的地址项分成两类，即直接地址和间接地址。</p>
<p>(1）直接地址</p>
<p>为了提高对文件的检索速度，在索引结点中可设置10个直接地址项，即用iaddr(O)一iaddr(9)来存放直接地址。换言之，这里每项中所存放的是该文件数据所在盘块的盘块号。假如每个盘块的大小为4KB，当文件不大于40KB时，便可直接从索引结点中读出该文件的全部盘块号。</p>
<p>(2)一次间接地址</p>
<p>对于大、中型文件，只采用直接地址并不现实。可再利用索引结点中的地址项iaddr(10)来提供一次间接地址。这种方式的实质就是一级索引分配方式。图中的一次间址块也就是索引块，系统将分配给文件的多个盘块号记入其中。在一次间址块中可存放1024个盘块号，因而允许文件长达 4MB。</p>
<p>(3）多次间接地址</p>
<p>当文件长度大于4MB＋40KB(一次间接地址与10个直接地址项)时，系统还须采用二次间接地址分配方式。这时，用地址项iaddr(11)提供二次间接地址。该方式的实质是两级索引分配方式。系统此时在二次间接地址块中记入所有一次间接地址块的盘号。在采用二次间接地址方式时，文件的最大长度可达4GB。同理，地址项iaddr(12)作为三次间接地址，其允许的文件最大长度可达4TB。</p>
<details><summary>4.26UNIX系统的inode结构示意图
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211006120538.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211006120538.png';" /></details>

<p>最后，我们对本章内容再进行一次宏观上的把握。贯穿本章内容的有两条主线:第一条主线是介绍一种新的抽象数据类型、文件，从逻辑结构和物理结构两个方面进行;第二条主线是操作系统是如何管理“文件”这种数据结构的，介绍了多文件的逻辑结构的组织，即目录，还介绍了如何处理用户对文件的服务请求，即磁盘管理。但宏观认识是远远不够的，从宏观上把握知识的目的是从微观上更加准确地掌控细微知识点，在考试中得到好成绩。读者要通过反复做题、对答案，不断加深自己对知识点的认知程度。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://ednow.github.io/2021/07/26/%E7%8E%8B%E9%81%93-%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-ch3-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ednow">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ednow">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/26/%E7%8E%8B%E9%81%93-%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-ch3-%E5%86%85%E5%AD%98%E7%AE%A1%E7%90%86/" class="post-title-link" itemprop="url">王道-操作系统-ch3-内存管理</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-07-26 15:17:07" itemprop="dateCreated datePublished" datetime="2021-07-26T15:17:07+08:00">2021-07-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-10-13 13:55:23" itemprop="dateModified" datetime="2021-10-13T13:55:23+08:00">2021-10-13</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="王道"><a href="#王道" class="headerlink" title="王道"></a>王道</h1><h2 id="内存管理"><a href="#内存管理" class="headerlink" title="内存管理"></a>内存管理</h2><p>【考纲内容】</p>
<p>(一）内存管理基础</p>
<ol>
<li>内存管理概念（逻辑地址与物理地址空间，地址变换， $\color{red}{\text{内存共享}}$ ，内存保护，内存分配与回收）</li>
<li>连续分配管理方式</li>
<li>非连续分配管理方式:</li>
<li>分页管理方式</li>
<li>分段管理方式</li>
<li>段页式管理方式</li>
</ol>
<p>(二）虚拟内存管理</p>
<ol>
<li>虚拟内存的基本概念</li>
<li>请求分页管理方式</li>
<li>页框分配</li>
<li>页面置换算法</li>
<li> $\color{red}{\text{内存映射文件}}$ （Memory-Mapped Files）</li>
<li>虚拟存储器性能的影响因素及改进方式</li>
</ol>
<p>【知识框架】</p>
<ul>
<li>程序执行过程<ul>
<li>编译、连接、装入</li>
<li>逻辑地址和物理地址</li>
</ul>
</li>
<li>扩充内存————覆盖与变换</li>
<li>连续分配<ul>
<li>单一连续分配</li>
<li>固定分区分配————内部碎片</li>
<li>动态分区分配<ul>
<li>外部碎片</li>
<li>分配算法：首次、最佳、最坏、邻近适应</li>
</ul>
</li>
</ul>
</li>
<li>非连续分配<ul>
<li>页式存储管理<ul>
<li>概念:页面、地址结构、页表</li>
<li>地址变化机构及变换过程</li>
<li>快表</li>
</ul>
</li>
<li>段式存储管理————段表、地址变换机构、段的共享与保护</li>
<li>段式存储管理————段表、页表</li>
</ul>
</li>
<li>虚拟内存<ul>
<li>概念<ul>
<li>局部性原理</li>
<li>特征:多次性、对换性、虚拟性</li>
</ul>
</li>
<li>请求分页<ul>
<li>组成：页表机构、缺页中断机构、地址变换机构</li>
<li>页面置换算法<ul>
<li>最佳置换(OPT)</li>
<li>先进先出(FIFO)————Belady异常</li>
<li>最近最久未使用(LRU)</li>
<li>时钟(CLOCK)算法</li>
</ul>
</li>
<li>页面分配策略</li>
<li>抖动、工作集</li>
</ul>
</li>
</ul>
</li>
</ul>
<p>【复习提示】</p>
<p>内存管理和进程管理是操作系统的核心内容，需要重点复习。本章围绕分页机制展开:通过分页管理方式在物理内存大小的基础上提高内存的利用率，再进一步引入请求分页管理方式，实现虚拟内存，使内存脱离物理大小的限制，从而提高处理器的利用率。</p>
<ul>
<li>需要注意：编译，链接，装入，执行，各个阶段的任务</li>
</ul>
<h3 id="内存管理概念"><a href="#内存管理概念" class="headerlink" title="内存管理概念"></a>内存管理概念</h3><p>在学习本节时，请读者思考以下问题:</p>
<p>1）为什么要进行内存管理?</p>
<p>2）页式管理中每个页表项大小的下限如何决定?</p>
<p>3）多级页表解决了什么问题?又会带来什么问题?</p>
<p>在学习经典的管理方法前，同样希望读者先思考，自己给出一些内存管理的想法，并在学习过程中和经典方案进行比较。注意本节给出的内存管理是循序渐进的，后一种方法通常会解决前一种方法的不足。希望读者多多思考，比较每种方法的异同，着重掌握页式管理。</p>
<h4 id="内存管理的基本原理和要求"><a href="#内存管理的基本原理和要求" class="headerlink" title="内存管理的基本原理和要求"></a>内存管理的基本原理和要求</h4><p>内存管理(Memory Management）是操作系统设计中最重要和最复杂的内容之一。虽然计算机硬件技术一直在飞速发展，内存容量也在不断增大，但仍然不可能将所有用户进程和系统所需要的全部程序与数据放入主存，因此操作系统必须对内存空间进行合理的划分和有效的动态分配。操作系统对内存的划分和动态分配，就是内存管理的概念。</p>
<p>有效的内存管理在多道程序设计中非常重要，它不仅可以方便用户使用存储器、提高内存利用率，还可以通过虚拟技术从逻辑上扩充存储器。</p>
<p>内存管理的功能有:</p>
<ul>
<li> $\color{green}{\text{内存空间的分配与回收}}$ 。由操作系统完成主存储器空间的分配和管理，使程序员摆脱存储分配的麻烦，提高编程效率。</li>
<li> $\color{green}{\text{地址转换}}$ 。在多道程序环境下，程序中的逻辑地址与内存中的物理地址不可能一致，因此存储管理必须提供地址变换功能，把逻辑地址转换成相应的物理地址。</li>
<li> $\color{green}{\text{内存空间的扩充}}$ 。利用虚拟存储技术或自动覆盖技术，从逻辑上扩充内存。</li>
<li> $\color{green}{\text{存储保护}}$ 。保证各道作业在各自的存储空间内运行，互不干扰。</li>
</ul>
<p>在进行具体的内存管理之前，需要了解进程运行的基本原理和要求。</p>
<h5 id="程序装入和链接"><a href="#程序装入和链接" class="headerlink" title="程序装入和链接"></a>程序装入和链接</h5><p>创建进程首先要将程序和数据装入内存。将用户源程序变为可在内存中执行的程序，通常需要以下几个步骤:</p>
<ul>
<li> $\color{green}{\text{编译}}$ 。由编译程序将用户源代码编译成若干目标模块。</li>
<li> $\color{green}{\text{链接}}$ 。由链接程序将编译后形成的一组目标模块及所需的库函数链接在一起，形成一个完整的装入模块。</li>
<li> $\color{green}{\text{装入}}$ 。由装入程序将装入模块装入内存运行。</li>
</ul>
<p>这三步过程如图3.1所示。</p>
<details><summary>图3.1将用户程序变为可在内存中执行的程序的步骤</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210926001932.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210926001932.png';" /></details>

<p>程序的链接有以下三种方式。</p>
<ul>
<li> $\color{green}{\text{静态链接}}$ 。在程序运行之前，先将各目标模块及它们所需的库函数链接成一个完整的可执行程序，以后不再拆开。</li>
<li> $\color{green}{\text{装入时动态链接}}$ 。将用户源程序编译后所得到的一组目标模块，在装入内存时，采用边装入边链接的方式。</li>
<li> $\color{green}{\text{运行时动态链接}}$ 。对某些目标模块的链接，是在程序执行中需要该目标模块时才进行的。其优点是便于修改和更新，便于实现对目标模块的共享。</li>
</ul>
<p>内存的装入模块在装入内存时，同样有以下三种方式:</p>
<p>1） $\color{green}{\text{绝对装入}}$ 。在编译时，若知道程序将驻留在内存的某个位置，则编译程序将产生绝对地址的目标代码。绝对装入程序按照装入模块中的地址，将程序和数据装入内存。由于程序中的逻辑地址与实际内存地址完全相同，因此不需对程序和数据的地址进行修改。绝对装入方式只适用于单道程序环境。另外，程序中所用的绝对地址，可在编译或汇编时给出，也可由程序员直接赋予。而通常情况下在程序中采用的是符号地址，编译或汇编时再转换为绝对地址。</p>
<p>2） $\color{green}{\text{可重定位装入}}$ 。在多道程序环境下，多个目标模块的起始地址（简称 $\color{green}{\text{始址}}$ ）通常都从0开始，程序中的其他地址都是相对于始址的，此时应采用可重定位装入方式。根据内存的当前情况，将装入模块装入内存的适当位置。装入时对目标程序中指令和数据的修改﹒过程称为 $\color{green}{\text{重定位}}$ ,地址变换通常是在装入时一次完成的,所以又称静态重定位,如图3.2(a)所示。</p>
<p>静态重定位的特点是，一个作业装入内存时，必须给它分配要求的全部内存空间，若没有足够的内存，则不能装入该作业。此外，作业一旦进入内存，整个运行期间就不能在内存中移动，也不能再申请内存空间。</p>
<p>3） $\color{green}{\text{动态运行时装入}}$ ，也称动态重定位。程序在内存中若发生移动，则需要采用动态的装入方式。装入程序把装入模块装入内存后，并不立即把装入模块中的相对地址转换为绝对地址，而是把这种地址转换推迟到程序真正要执行时才进行。因此，装入内存后的所有地址均为相对地址。这种方式需要一个重定位寄存器的支持，如图3.2(b)所示。</p>
<p>动态重定位的特点如下:可以将程序分配到不连续的存储区中;在程序运行之前可以只装入它的部分代码即可投入运行，然后在程序运行期间，根据需要动态申请分配内存;便于程序段的共享，可以向用户提供一个比存储空间大得多的地址空间。</p>
<details><summary>图3.2 重定位类型</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210926002103.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210926002103.png';" /></details>

<h5 id="逻辑地址空间与物理地址空间"><a href="#逻辑地址空间与物理地址空间" class="headerlink" title="逻辑地址空间与物理地址空间"></a>逻辑地址空间与物理地址空间</h5><p>编译后,每个目标模块都从0号单元开始编址,这称为该目标模块的相对地址(或逻辑地址)。当链接程序将各个模块链接成一个完整的可执行目标程序时，链接程序顺序依次按各个模块的相对地址构成统一的从0号单元开始编址的逻辑地址空间。用户程序和程序员只需知道逻辑地址，而内存管理的具体机制则是完全透明的，只有系统编程人员才会涉及内存管理的具体机制。不同进程可以有相同的逻辑地址，因为这些相同的逻辑地址可以映射到主存的不同位置。</p>
<p>物理地址空间是指内存中物理单元的集合，它是地址转换的最终地址，进程在运行时执行指令和访问数据，最后都要通过物理地址从主存中存取。当装入程序将可执行代码装入内存时，必须通过地址转换将逻辑地址转换成物理地址，这个过程称为 $\color{green}{\text{地址重定位}}$ 。</p>
<h5 id="内存保护"><a href="#内存保护" class="headerlink" title="内存保护"></a>内存保护</h5><p>内存分配前，需要保护操作系统不受用户进程的影响，同时保护用户进程不受其他用户进程的影响。内存保护可采取两种方法:</p>
<p>1）在CPU 中设置一对上、下限寄存器，存放用户作业在主存中的下限和上限地址，每当CPU要访问一个地址时，分别和两个寄存器的值相比，判断有无越界。</p>
<p>2）采用 $\color{green}{\text{重定位寄存器}}$ （或基址寄存器）和 $\color{green}{\text{界地址寄存器}}$ （又称限长寄存器）来实现这种保护。重定位寄存器含最小的物理地址值，界地址寄存器含逻辑地址的最大值。每个逻辑地址值必须小于界地址寄存器;内存管理机构动态地将逻辑地址与界地址寄存器进行比较，若未发生地址越界，则加上重定位寄存器的值后映射成物理地址，再送交内存单元,如图3.3所示。</p>
<details><summary>图3.3 重定位和界地址寄存器的硬件支持</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210926002456.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210926002456.png';" /></details>

<p>实现内存保护需要重定位寄存器和界地址寄存器，因此要注意两者的区别。重定位寄存器是用来“加”的，逻辑地址加上重定位寄存器中的值就能得到物理地址;界地址寄存器是用来“比”的，通过比较界地址寄存器中的值与逻辑地址的值来判断是否越界。</p>
<h4 id="覆盖与交换"><a href="#覆盖与交换" class="headerlink" title="*覆盖与交换"></a>*覆盖与交换</h4><p>覆盖与交换技术是在多道程序环境下用来扩充内存的两种方法。</p>
<h5 id="覆盖"><a href="#覆盖" class="headerlink" title="覆盖"></a>覆盖</h5><p>早期的计算机系统中，主存容量很小，虽然主存中仅存放一道用户程序，但存储空间放不下用户进程的现象也经常发生，这一矛盾可以用覆盖技术来解决。</p>
<p>覆盖的基本思想如下:由于程序运行时并非任何时候都要访问程序及数据的各个部分（尤其是大程序)，因此可把用户空间分成一个固定区和若干覆盖区。将经常活跃的部分放在固定区，其余部分按调用关系分段。首先将那些即将要访问的段放入覆盖区，其他段放在外存中，在需要调用前，系统再将其调入覆盖区，替换覆盖区中原有的段。</p>
<p>覆盖技术的特点是，打破了必须将一个进程的全部信息装入主存后才能运行的限制，但当同时运行程序的代码量大于主存时仍不能运行，此外，内存中能够更新的地方只有覆盖区的段，不在覆盖区中的段会常驻内存。</p>
<h5 id="交换"><a href="#交换" class="headerlink" title="交换"></a>交换</h5><p>交换（对换）的基本思想是，把处于等待状态（或在CPU 调度原则下被剥夺运行权利）的程序从内存移到辅存，把内存空间腾出来，这一过程又称换出;把准备好竞争CPU运行的程序从辅存移到内存，这一过程又称换入。第2章介绍的中级调度采用的就是交换技术。</p>
<p>例如，有一个CPU采用时间片轮转调度算法的多道程序环境。时间片到，内存管理器将刚刚执行过的进程换出，将另一进程换入刚刚释放的内存空间。同时，CPU调度器可以将时间片分配给其他已在内存中的进程。每个进程用完时间片都与另一进程交换。在理想情况下，内存管理器的交换过程速度足够快，总有进程在内存中可以执行。</p>
<p>有关交换，需要注意以下几个问题:</p>
<ul>
<li>交换需要备份存储，通常是快速磁盘。它必须足够大，并提供对这些内存映像的直接访问。</li>
<li>为了有效使用CPU，需要使每个进程的执行时间比交换时间长，而影响交换时间的主要是转移时间。转移时间与所交换的内存空间成正比。</li>
<li>若换出进程，则必须确保该进程完全处于空闲状态。</li>
<li>交换空间通常作为磁盘的一整块，且独立于文件系统，因此使用起来可能很快。</li>
<li>交换通常在有许多进程运行且内存空间吃紧时开始启动，而在系统负荷降低时就暂停。</li>
<li>普通的交换使用不多，但交换策略的某些变体在许多系统(如UNIX系统)中仍发挥作用。</li>
</ul>
<p>交换技术主要在不同进程（或作业)之间进行，而覆盖则用于同一个程序或进程中。由于覆盖技术要求给出程序段之间的覆盖结构，使得其对用户和程序员不透明，所以对于主存无法存放用户程序的矛盾，现代操作系统是通过虚拟内存技术来解决的，覆盖技术则已成为历史;而交换技术在现代操作系统中仍具有较强的生命力。</p>
<h4 id="连续分配管理方式"><a href="#连续分配管理方式" class="headerlink" title="连续分配管理方式"></a>连续分配管理方式</h4><p>连续分配方式是指为一个用户程序分配一个连续的内存空间，譬如某用户需要1GB的内存空间，连续分配方式就在内存空间中为用户分配一块连续的1GB空间。连续分配方式主要包括 $\color{green}{\text{单一连续分配}}$ 、 $\color{green}{\text{固定分区分配}}$ 和 $\color{green}{\text{动态分区分配}}$ 。</p>
<h5 id="单一连续分配"><a href="#单一连续分配" class="headerlink" title="单一连续分配"></a>单一连续分配</h5><p>内存在此方式下分为 $\color{green}{\text{系统区}}$ 和 $\color{green}{\text{用户区}}$ ，系统区仅供操作系统使用，通常在低地址部分;用户区是为用户提供的、除系统区之外的内存空间。这种方式无须进行内存保护。因为内存中永远只有一道程序，因此肯定不会因为访问越界而干扰其他程序。</p>
<p>这种方式的优点是简单、无外部碎片，可以采用覆盖技术，不需要额外的技术支持。缺点是只能用于单用户、单任务的操作系统中，有内部碎片，存储器的利用率极低。</p>
<h5 id="固定分区分配"><a href="#固定分区分配" class="headerlink" title="固定分区分配"></a>固定分区分配</h5><p>固定分区分配是最简单的一种多道程序存储管理方式，它将用户内存空间划分为若干固定大小的区域，每个分区只装入一道作业。当有空闲分区时，便可再从外存的后备作业队列中选择适当大小的作业装入该分区，如此循环。</p>
<p>固定分区分配在划分分区时有两种不同的方法，如图3.4所示。</p>
<details><summary>图3.4 固定分区分配的两种方法</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210926002917.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210926002917.png';" /></details>

<p>为便于内存分配,通常将分区按大小排队，并为之建立一张分区说明表，其中各表项包括每个分区的始址、大小及状态（是否已分配)，如图3.5(a)所示。当有用户程序要装入时，便检索该表，以找到合适的分区给予分配并将其状态置为“已分配”，未找到合适分区时，则拒绝为该用户程序分配内存。存储空间的分配情况如图3.5(b)所示。</p>
<details><summary>图3.5 固定分区说明表和内存分配情况</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210926002959.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210926002959.png';" /></details>

<p>这种分区方式存在两个问题:一是程序可能太大而放不进任何一个分区中，这时用户不得不使用覆盖技术来使用内存空间;二是主存利用率低，当程序小于固定分区大小时，也占用一个完整的内存分区空间，这样分区内部就存在空间浪费，这种现象称为内部碎片。</p>
<p>固定分区是可用于多道程序设计的最简单的存储分配，无外部碎片，但不能实现多进程共享一个主存区，所以存储空间利用率低。固定分区分配很少用于现在通用的操作系统中，但在某些用于控制多个相同对象的控制系统中仍发挥着一定的作用。</p>
<h5 id="动态分区分配"><a href="#动态分区分配" class="headerlink" title="动态分区分配"></a>动态分区分配</h5><p>动态分区分配又称可变分区分配，是一种动态划分内存的分区方法。这种分区方法不预先划分内存，而是在进程装入内存时，根据进程的大小动态地建立分区，并使分区的大小正好适合进程的需要。因此，系统中分区的大小和数目是可变的。</p>
<p>如图3.6所示，系统有64MB内存空间，其中低8MB固定分配给操作系统，其余为用户可用内存。开始时装入前三个进程，它们分别分配到所需的空间后，内存只剩下4MB，进程4无法装入。在某个时刻，内存中没有一个就绪进程，CPU出现空闲，操作系统就换出进程2，换入进程4。由于进程4比进程2小，这样在主存中就产生了一个6MB 的内存块。之后CPU又出现空闲，而主存无法容纳进程2，操作系统就换出进程1，换入进程2。</p>
<details><summary>图3.6动态分区</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210926003108.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210926003108.png';" /></details>

<p>动态分区在开始分配时是很好的，但之后会导致内存中出现许多小的内存块。随着时间的推移，内存中会产生越来越多的碎片(（图3.6中最后的4MB和中间的6MB，且随着进程的换入/换出，很可能会出现更多、更小的内存块)，内存的利用率随之下降。这些小的内存块称为外部碎片，指在所有分区外的存储空间会变成越来越多的碎片，这与固定分区中的内部碎片正好相对。克服外部碎片可以通过紧凑(Compaction）技术来解决，即操作系统不时地对进程进行移动和整理。但这需要动态重定位寄存器的支持，且相对费时。紧凑的过程实际上类似于Windows系统中的磁盘整理程序，只不过后者是对外存空间的紧凑。</p>
<p>在进程装入或换入主存时，若内存中有多个足够大的空闲块，则操作系统必须确定分配哪个内存块给进程使用，这就是动态分区的分配策略。考虑以下几种算法:</p>
<p>1） $\color{green}{\text{首次适应}}$ (First Fit）算法。空闲分区以地址递增的次序链接。分配内存时顺序查找，找到大小能满足要求的第一个空闲分区。</p>
<p>2） $\color{green}{\text{最佳适应}}$ （Best Fit）算法。空闲分区按容量递增的方式形成分区链，找到第一个能满足要求的空闲分区。</p>
<p>3） $\color{green}{\text{最坏适应}}$ (Worst Fit）算法。又称最大适应（Largest Fit）算法，空闲分区以容量递减的次序链接，找到第一个能满足要求的空闲分区，即挑选出最大的分区。</p>
<p>4） $\color{green}{\text{邻近适应}}$ (Next Fit）算法。又称循环首次适应算法，由首次适应算法演变而成。不同之处是，分配内存时从上次查找结束的位置开始继续查找。</p>
<p>在这几种方法中，首次适应算法不仅是最简单的，而且通常也是最好和最快的。在UNIX系统的最初版本中，就是使用首次适应算法为进程分配内存空间的，它使用数组的数据结构（而非链表）来实现。不过，首次适应算法会使得内存的低地址部分出现很多小的空闲分区，而每次分配查找时，都要经过这些分区，因此增加了查找的开销。</p>
<p>邻近适应算法试图解决这个问题。但实际上，它常常导致在内存的末尾分配空间（因为在一遍扫描中，内存前面部分使用后再释放时，不会参与分配）分裂成小碎片。它通常比首次适应算法的结果要差。</p>
<p>最佳适应算法虽然称为“最佳”，但是性能通常很差，因为每次最佳的分配会留下很小的难以利用的内存块，会产生最多的外部碎片。</p>
<p>最坏适应算法与最佳适应算法相反，它选择最大的可用块，这看起来最不容易产生碎片，但是却把最大的连续内存划分开，会很快导致没有可用的大内存块，因此性能也非常差。</p>
<p>Knuth 和 Shore分别就前三种方法对内存空间的利用情况做了模拟实验，结果表明:首次适应算法可能比最佳适应法效果好，而它们两者一定比最大适应法效果好。另外要注意，在算法实现时，分配操作中最佳适应法和最大适应法需要对可用块进行排序或遍历查找，而首次适应法和邻近适应法只需要简单查找;在回收操作中，当回收的块与原来的空闲块相邻时（有三种相邻的情况，比较复杂)，需要将这些块合并。在算法实现时，使用数组或链表进行管理。除了内存的利用率，这里的算法开销也是操作系统设计需要考虑的一个因素。</p>
<p>三种内存分区管理方式的比较见表3.1。</p>
<details><summary>表3.1 三种内存分区管理方式的比较</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210926003302.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210926003302.png';" /></details>

<p>以上三种内存分区管理方法有一个共同特点，即用户进程（或作业）在主存中都是连续存放的。这里对它们进行比较和总结。</p>
<h4 id="非连续分配管理方式"><a href="#非连续分配管理方式" class="headerlink" title="非连续分配管理方式"></a>非连续分配管理方式</h4><p>非连续分配允许一个程序分散地装入不相邻的内存分区。在连续分配管理方式中，我们发现，即使内存有超过1GB的空闲空间，但若没有连续的1GB空间，则需要1GB空间的作业仍然是无法运行的;但若采用非连续分配管理方式，则作业所要求的1GB内存空间可以分散地分配在内存的各个区域，当然，这也需要额外的空间去存储它们（分散区域）的索引，使得非连续分配方式的存储密度低于连续存储方式的。</p>
<p>非连续分配管理方式根据分区的大小是否固定，分为 $\color{green}{\text{分页存储管理方式}}$ 和 $\color{green}{\text{分段存储管理方式}}$ 。</p>
<p>在分页存储管理方式中，又根据运行作业时是否要把作业的所有页面都装入内存才能运行，分为 $\color{green}{\text{基本分页存储管理方式}}$ 和 $\color{green}{\text{请求分页存储管理方式}}$ 。下面介绍基本分页存储管理方式。</p>
<h5 id="基本分页存储管理方式"><a href="#基本分页存储管理方式" class="headerlink" title="基本分页存储管理方式"></a>基本分页存储管理方式</h5><p>固定分区会产生内部碎片，动态分区会产生外部碎片,这两种技术对内存的利用率都比较低。我们希望内存的使用能尽量避免碎片的产生，这就引入了分页的思想:把主存空间划分为大小相等且固定的块，块相对较小，作为主存的基本单位。每个进程也以块为单位进行划分，进程在执行时，以块为单位逐个申请主存中的块空间。</p>
<p>分页的方法从形式上看，像分区相等的固定分区技术，分页管理不会产生外部碎片。但它又有本质的不同点:块的大小相对分区要小很多，而且进程也按照块进行划分，进程运行时按块申请主存可用空间并执行。这样，进程只会在为最后一个不完整的块申请一个主存块空间时，才产生主存碎片，所以尽管会产生内部碎片，但这种碎片相对于进程来说也是很小的，每个进程平均只产生半个块大小的内部碎片（也称页内碎片)。</p>
<h6 id="分页存储的几个基本概念"><a href="#分页存储的几个基本概念" class="headerlink" title="分页存储的几个基本概念"></a>分页存储的几个基本概念</h6><p>① $\color{green}{\text{页面和页面大小}}$ 。进程中的块称为 $\color{green}{\text{页}}$ (Page)，内存中的块称为 $\color{green}{\text{页框}}$ (Page Frame，或页帧)。外存也以同样的单位进行划分，直接称为 $\color{green}{\text{块}}$ （Block)。进程在执行时需要申请主存空间，即要为每个页面分配主存中的可用页框，这就产生了页和页框的一一对应。为方便地址转换，页面大小应是2的整数幂。同时页面大小应该适中，页面太小会使进程的页面数过多，这样页表就会过长，占用大量内存，而且也会增加硬件地址转换的开销，降低页面换入/换出的效率;页面过大又会使页内碎片增多，降低内存的利用率。所以页面的大小应该适中，要在空间效率和时间效率之间权衡。</p>
<p>② $\color{green}{\text{地址结构}}$ 。分页存储管理的逻辑地址结构如图3.7所示。</p>
<details><summary>图3.7 分页存储管理的逻辑地址结构</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210926131515.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210926131515.png';" /></details>

<p>地址结构包含两部分:前一部分为页号P，后一部分为页内偏移量W。地址长度为32位，其中 0～11位为页内地址，即每页大小为4KB;12~31位为页号，地址空间最多允许 $2^{20}$ 页。</p>
<p>注意，地址结构决定了虚拟内存的寻址空间有多大。在实际问题中，页号、页内偏移、逻辑地址大多都是用十进制数给出的。题目用二进制地址的形式给出时，读者要会转换。</p>
<p>③页表。为了便于在内存中找到进程的每个页面所对应的物理块，系统为每个进程建立张页表，它记录页面在内存中对应的物理块号，页表一般存放在内存中。页表是由页表项组成的，初学者容易混淆页表项与地址结构，页表项与地址都由两部构成，而且第一部分都是页号，但页表项的第二部分是物理内存中的块号，而地址的;二部分是页内偏移;页表项的第二部分与地址的第二部分共同组成物理地址。<br>在配置页表后，进程执行时，通过查找该表，即可找到每页在内存中的物理块号。可见页表的作用是实现从页号到物理块号的地址映射，如图3.8所示。</p>
<details><summary>图3.8 页表的作用</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210926131629.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210926131629.png';" /></details>

<h6 id="基本地址变换机构"><a href="#基本地址变换机构" class="headerlink" title="基本地址变换机构"></a>基本地址变换机构</h6><p>地址变换机构的任务是将逻辑地址转换为内存中的物理地址。地址变换是借助于页表实现的。图3.9给出了分页存储管理系统中的地址变换机构。</p>
<details><summary>图3.9 分页存储管理系统中的地址变换机构</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210926131714.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210926131714.png';" /></details>

<p>在系统中通常设置一个页表寄存器（PTR)，存放页表在内存的起始地址F和页表长度M。进程未执行时，页表的始址和长度存放在进程控制块中，当进程执行时，才将页表始址和长度存入页表寄存器。设页面大小为L，逻辑地址A到物理地址E的变换过程如下（逻辑地址、页号、每页的长度都是十进制数):</p>
<p>①计算页号P(P=AIL）和页内偏移量W(W=A%L)。</p>
<p>${\textstyle\unicode{x2461}}$  比较页号Р和页表长度M，若P≥M，则产生越界中断，否则继续执行。</p>
<p>${\textstyle\unicode{x2462}}$  页表中页号Р对应的页表项地址=页表始址F＋页号Px页表项长度，取出该页表项内容b，即为物理块号。要注意区分页表长度和页表项长度。页表长度的值是指一共有多少页，页表项长度是指页地址占多大的存储空间。</p>
<p>④计算E=b×L＋W，用得到的物理地址E去访问内存。</p>
<p>以上整个地址变换过程均是由硬件自动完成的。例如，若页面大小L为1KB，页号2对应的物理块为b=8，计算逻辑地址A=2500的物理地址E的过程如下:P=2500/1K=2，W = 2500%1K=452，查找得到页号2对应的物理块的块号为8，E=8×1024+452=8644。</p>
<p>要再次提醒读者的是，题目中条件用十进制数给出和用二进制数给出的处理过程会稍有不同。同时读者会发现，页式管理只需给出一个整数就能确定对应的物理地址，因为页面大小L是固定的。因此，页式管理中地址空间是 $\color{green}{\text{一维的}}$ 。</p>
<p>页表项的大小不是随意规定的，而是有所约束的。如何确定页表项的大小?</p>
<p>页表项的作用是找到该页在内存中的位置。以32位逻辑地址空间、字节编址单位、一页4KB为例，地址空间内一共有 $2^{32}$ B/4KB= 1M页，因此需要 $log_21M$  = 20位才能保证表示范围能容纳所有页面，又因为以字节作为编址单位，即页表项的大小≥ $\lceil 20/81 \rceil$  =3B。所以在这个条件下，为了保证页表项能够指向所有页面，页表项的大小应该大于3B，当然，也可选择更大的页表项让一个页面能够正好容下整数个页表项,进而方便存储(如取成4B，一页正好可以装下1K个页表项)，或增加一些其他信息。</p>
<p>下面讨论分页管理方式存在的两个主要问题:①每次访存操作都需要进行逻辑地址到物理地址的转换，地址转换过程必须足够快，否则访存速度会降低;②每个进程引入页表，用于存储映射机制，页表不能太大，否则内存利用率会降低。</p>
<h6 id="具有快表的地址变换机构"><a href="#具有快表的地址变换机构" class="headerlink" title="具有快表的地址变换机构"></a>具有快表的地址变换机构</h6><p>由上面介绍的地址变换过程可知，若页表全部放在内存中，则存取一个数据或一条指令至少要访问两次内存:第一次是访问页表，确定所存取的数据或指令的物理地址;第二次是根据该地址存取数据或指令。显然，这种方法比通常执行指令的速度慢了一半。</p>
<p>为此，在地址变换机构中增设一个具有并行查找能力的高速缓冲存储器—快表，又称相联存储器(TLB)，用来存放当前访问的若干页表项，以加速地址变换的过程。与此对应，主存中的页表常称为慢表。具有快表的地址变换机构如图3.10所示。</p>
<details><summary>图3.10 具有快表的地址变换机构</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210926132136.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210926132136.png';" /></details>

<p>在具有快表的分页机制中，地址的变换过程如下:</p>
<p>${\textstyle\unicode{x2460}}$  CPU给出逻辑地址后，由硬件进行地址转换，将页号送入高速缓存寄存器，并将此页号<br>与快表中的所有页号进行比较。</p>
<p>②若找到匹配的页号，说明所要访问的页表项在快表中，则直接从中取出该页对应的页框<br>号，与页内偏移量拼接形成物理地址。这样，存取数据仅一次访存便可实现。</p>
<p>${\textstyle\unicode{x2462}}$ 若未找到匹配的页号，则需要访问主存中的页表，在读出页表项后，应同时将其存入快表，以便后面可能的再次访问。但若快表已满，则必须按照一定的算法对旧的页表项进行替换。</p>
<blockquote>
<p>注意:有些处理机设计为快表和慢表同时查找，若在快表中查找成功则终止慢表的查找。</p>
</blockquote>
<p>一般快表的命中率可达90%以上，这样分页带来的速度损失就可降低至10%以下。快表的有效性基于著名的局部性原理，后面讲解虚拟内存时将会具体讨论它。</p>
<h6 id="两级页表"><a href="#两级页表" class="headerlink" title="两级页表"></a>两级页表</h6><p>由于引入了分页管理，进程在执行时不需要将所有页调入内存页框，而只需将保存有映射关系的页表调入内存。但是，我们仍然需要考虑页表的大小。以32位逻辑地址空间、页面大小4KB、页表项大小4B为例，若要实现进程对全部逻辑地址空间的映射，则每个进程需要 $2^{20}$ 即约100万个页表项。也就是说，每个进程仅页表这一项就需要4MB 主存空间，这显然是不切实际的。即便不考虑对全部逻辑地址空间进行映射的情况，一个逻辑地址空间稍大的进程，其页表大小也可能是过大的。以一个40MB的进程为例，页表项共40KB (40MB/4KBx4B)，若将所有页表项内容保存在内存中，则需要10个内存页框来保存整个页表。整个进程大小约为1万个页面，而实际执行时只需要几十个页面进入内存页框就可运行，但若要求10个页面大小的页表必须全部进入内存，则相对实际执行时的几十个进程页面的大小来说，肯定降低了内存利用率;从另一方面来说，这10页的页表项也并不需要同时保存在内存中，因为在大多数情况下，映射所需要的页表项都在页表的同一个页面中。</p>
<p>为了压缩页表，我们进一步延伸页表映射的思想，就可得到二级分页，即使用层次结构的页表:将页表的10页空间也进行地址映射，建立上一级页表，用于存储页表的映射关系。这里对页表的10个页面进行映射只需要10个页表项，所以上一级页表只需要1页就已足够（可以存储210=1024个页表项)。在进程执行时，只需要将这一页的上一级页表调入内存即可，进程的页表和进程本身的页面可在后面的执行中再调入内存。根据上面提到的条件（32位逻辑地址空间、页面大小4KB、页表项大小4B，以字节为编址单位)，我们来构造一个适合的页表结构。页面大小为4KB，页内偏移地址为 $log_2 4K$ =12位，页号部分为20位，若不采用分级页表，则仅页表就要占用 $2^{20}$ ×4B/4KB = 1024页，这大大超过了许多进程自身需要的页面，对于内存来说是非常浪费资源的，而且查询页表工作也会变得十分不便、试想若把这些页表放在连续的空间内，查询对应页的物理页号时可以通过页表首页地址＋页号×4B的形式得到，而这种方法查询起来虽然相对方便，但连续的1024页对于内存的要求实在太高，并且上面也说到了其中大多数页面都是不会用到的，所以这种方法并不具有可行性。若不把这些页表放在连续的空间里，则需要一张索引表来告诉我们第几张页表该上哪里去找，这能解决页表的查询问题，且不用把所有的页表都调入内存，只在需要它时才调入(下节介绍的虚拟存储器思想)，因此能解决占用内存空间过大的问题。读者也许发现这个方案就和当初引进页表机制的方式一模一样，实际上就是构造一个页表的页表，也就是二级页表。为查询方便，顶级页表最多只能有1个页面(一定要记住这个规定)，因此顶级页表总共可以容纳4KB/4B= 1K个页表项，它占用的地址位数为 $log_2 lK$ = 10位，而之前已经计算出页内偏移地址占用了12位，因此一个32位的逻辑地址空间就剩下了10位，正好使得二级页表的大小在一页之内，这样就得到了逻辑地址空间的格式，如图3.11所示。</p>
<details><summary>图3.11 逻辑地址空间的格式</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210926132611.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210926132611.png';" /></details>

<p>二级页表实际上是在原有页表结构上再加上一层页表，示意结构如图3.12所示。</p>
<details><summary>图3.12 二级页表结构示意图</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210926132635.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210926132635.png';" /></details>

<p>建立多级页表的目的在于建立索引，以便不用浪费主存空间去存储无用的页表项，也不用盲目地顺序式查找页表项。</p>
<h5 id="基本分段存储管理方式"><a href="#基本分段存储管理方式" class="headerlink" title="基本分段存储管理方式"></a>基本分段存储管理方式</h5><p>分页管理方式是从计算机的角度考虑设计的,目的是提高内存的利用率，提升计算机的性能。分页通过硬件机制实现，对用户完全透明。分段管理方式的提出则考虑了用户和程序员，以满足方便编程、信息保护和共享、动态增长及动态链接等多方面的需要。</p>
<p>1）分段。段式管理方式按照用户进程中的自然段划分逻辑空间。例如，用户进程由主程序、两个子程序、栈和一段数据组成，于是可以把这个用户进程划分为5段，每段从0开始编址，并分配一段连续的地址空间（段内要求连续，段间不要求连续，因此整个作业的地址空间是二维的)，其逻辑地址由段号S与段内偏移量w两部分组成。</p>
<p>在图3.13中，段号为16位，段内偏移量为16位，因此一个作业最多有 $2^{16}$ =65536段,最大段长为64KB。</p>
<details><summary>图3.13 分段系统中的逻辑地址结构</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210926144214.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210926144214.png';" /></details>

<p>在页式系统中，逻辑地址的页号和页内偏移量对用户是透明的，但在段式系统中，段号和段内偏移量必须由用户显式提供，在高级程序设计语言中，这个工作由编译程序完成。</p>
<p>2）段表。每个进程都有一张逻辑空间与内存空间映射的段表，其中每个段表项对应进程的一段，段表项记录该段在内存中的始址和长度。段表的内容如图3.14所示。</p>
<details><summary>图3.14 段表的内容</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210926144307.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210926144307.png';" /></details>

<p>配置段表后，执行中的进程可通过查找段表，找到每段所对应的内存区。可见，段表用于实现从逻辑段到物理内存区的映射，如图3.15所示。</p>
<p>3）地址变换机构。分段系统的地址变换过程如图3.16所示。为了实现进程从逻辑地址到物理地址的变换功能，在系统中设置了段表寄存器，用于存放段表始址F和段表长度M。从逻辑地址A到物理地址E之间的地址变换过程如下:</p>
<details><summary>图3.15 利用段表实现物理内存区映射</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210926144352.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210926144352.png';" /></details>

<details><summary>图3.16 分段系统的地址变换过程</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210926144411.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210926144411.png';" /></details>

<p>①从逻辑地址A中取出前几位为段号S，后几位为段内偏移量W，注意在段式存储管理的题目中，逻辑地址一般以二进制数给出，而在页式存储管理中，逻辑地址一般以十进制数给出，读者要具体问题具体分析。</p>
<p>②比较段号S和段表长度M，若S≥M，则产生越界中断，否则继续执行。</p>
<p>③段表中段号S对应的段表项地址=段表始址F＋段号S×段表项长度，取出该段表项的前几位得到段长C。若段内偏移量≥C，则产生越界中断，否则继续执行。从这句话我们可以看出，段表项实际上只有两部分，前几位是段长，后几位是始址。</p>
<p>${\textstyle\unicode{x2463}}$  取出段表项中该段的始址b，计算E=b＋W，用得到的物理地址E去访问内存。</p>
<p>4）段的共享与保护。在分段系统中，段的共享是通过两个作业的段表中相应表项指向被共享的段的同一个物理副本来实现的。当一个作业正从共享段中读取数据时，必须防止另一个作业修改此共享段中的数据。不能修改的代码称为纯代码或可重入代码（它不属于临界资源)，这样的代码和不能修改的数据可以共享，而可修改的代码和数据不能共享。</p>
<p>与分页管理类似，分段管理的保护方法主要有两种:一种是存取控制保护，另一种是地址越界保护。地址越界保护将段表寄存器中的段表长度与逻辑地址中的段号比较，若段号大于段表长度，则产生越界中断;再将段表项中的段长和逻辑地址中的段内偏移进行比较，若段内偏移大于段长，也会产生越界中断。分页管理中的地址越界保护只需要判断页号是否越界，页内偏移是不可能越界的。</p>
<p>与页式管理不同，段式管理不能通过给出一个整数便确定对应的物理地址，因为每段的长度是不固定的，无法通过整数除法得出段号，无法通过求余得出段内偏移，所以段号和段内偏移一定要显式给出（段号，段内偏移)，因此分段管理的地址空间是 $\color{green}{\text{二维的}}$ 。</p>
<h5 id="段页式管理方式"><a href="#段页式管理方式" class="headerlink" title="段页式管理方式"></a>段页式管理方式</h5><p>页式存储管理能有效地提高内存利用率，而分段存储管理能反映程序的逻辑结构并有利于段的共享。将这两种存储管理方法结合起来，便形成了段页式存储管理方式。</p>
<p>在段页式系统中，作业的地址空间首先被分成若干逻辑段，每段都有自己的段号，然后将每段分成若干大小固定的页。对内存空间的管理仍然和分页存储管理一样，将其分成若干和页面大小相同的存储块，对内存的分配以存储块为单位，如图3.17所示。</p>
<p>在段页式系统中，作业的逻辑地址分为三部分:段号、页号和页内偏移量，如图3.18所示。</p>
<details><summary>图3.18 段页式系统的逻辑地址结构</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210926150756.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210926150756.png';" /></details>

<p>为了实现地址变换，系统为每个进程建立一张段表，每个分段有一张页表。段表表项中至少包括段号、页表长度和页表始址，页表表项中至少包括页号和块号。此外，系统中还应有一个段表寄存器，指出作业的段表始址和段表长度（段表寄存器和页表寄存器的作用都有两个，一是在段表或页表中寻址，二是判断是否越界)。</p>
<blockquote>
<p>注意:在一个进程中，段表只有 $\color{green}{\text{一个}}$ ，而页表可能有 $\color{green}{\text{多个}}$ 。</p>
</blockquote>
<p>在进行地址变换时，首先通过段表查到页表始址，然后通过页表找到页帧号，最后形成物理地址。如图3.19所示，进行一次访问实际需要三次访问主存，这里同样可以使用快表来加快查找速度，其关键字由段号、页号组成，值是对应的页帧号和保护码。</p>
<details><summary>图3.19 段页式系统的地址变换机构</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210926150852.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210926150852.png';" /></details>

<p>结合上面对段式管理和页式管理的地址空间的分析，可以得出结论:段页式管理的地址空间是 $\color{green}{\text{二维的}}$ 。</p>
<h4 id="本节小结"><a href="#本节小结" class="headerlink" title="本节小结"></a>本节小结</h4><p>本节开头提出的问题的参考答案如下。</p>
<h5 id="为什么要进行内存管理"><a href="#为什么要进行内存管理" class="headerlink" title="为什么要进行内存管理?"></a>为什么要进行内存管理?</h5><p>在单道批处理系统阶段，一个系统在一个时间段内只执行一个程序，内存的分配极其简单，即仅分配给当前运行的进程。引入多道程序的并发执行后，进程之间共享的不仅仅是处理机，还有主存储器。然而，共享主存会形成一些特殊的挑战。若不对内存进行管理，则容易导致内存数据的混乱，以至于限制进程的并发执行。因此，为了更好地支持多道程序并发执行，必须进行内存管理。</p>
<h5 id="页式管理中每个页表项大小的下限如何决定"><a href="#页式管理中每个页表项大小的下限如何决定" class="headerlink" title="页式管理中每个页表项大小的下限如何决定?"></a>页式管理中每个页表项大小的下限如何决定?</h5><p>页表项的作用是找到该页在内存中的位置。以32位逻辑地址空间、字节编址单位、一页4KB为例，地址空间内共含有 $2^{32}$ B/4KB= 1M页，需要 $log_2 1M$  = 20位才能保证表示范围能容纳所有页面，又因为以字节作为编址单位，即页表项的大小≥  $\lceil 20/8 \rceil$  =3B。所以在这个条件下，为了保证页表项能够指向所有页面，页表项的大小应该大于3B;当然，也可选择更大的页表项大小，让一个页面能够正好容下整数个页表项，以方便存储（例如取成4B，一页正好可以装下1K个页表项)，或增加一些其他信息。</p>
<h5 id="多级页表解决了什么问题-又会带来什么问题"><a href="#多级页表解决了什么问题-又会带来什么问题" class="headerlink" title="多级页表解决了什么问题?又会带来什么问题?"></a>多级页表解决了什么问题?又会带来什么问题?</h5><p>多级页表解决了当逻辑地址空间过大时，页表的长度会大大增加的问题。而采用多级页表时，一次访盘需要多次访问内存甚至磁盘，会大大增加一次访存的时间。</p>
<p>不少读者表示本节的内容难以掌握，实际上本节的内容并不难，只要抓住下列几个关键的线索，本节的所有知识点就能了然于胸。</p>
<p>无论是段式管理、页式管理还是段页式管理，读者都只需要关注三个问题:①逻辑地址结构，②表项结构，③寻址过程。搞清楚这三个问题，就相当于搞清楚了上面几种存储管理方式。再次提醒读者区分逻辑地址结构和表项结构。</p>
<h3 id="虚拟内存管理"><a href="#虚拟内存管理" class="headerlink" title="虚拟内存管理"></a>虚拟内存管理</h3><p>在学习本节时，请读者思考以下问题:</p>
<p>1）为什么要引入虚拟内存?</p>
<p>2）虚拟内存空间的大小由什么因素决定?</p>
<p>3）虚拟内存是怎么解决问题的?会带来什么问题?</p>
<p>读者要掌握虚拟内存解决问题的思想，并了解几种替换算法的优劣，熟练掌握虚实地址的变换方法。</p>
<h4 id="虚拟内存的基本概念"><a href="#虚拟内存的基本概念" class="headerlink" title="虚拟内存的基本概念"></a>虚拟内存的基本概念</h4><h5 id="传统存储管理方式的特征"><a href="#传统存储管理方式的特征" class="headerlink" title="传统存储管理方式的特征"></a>传统存储管理方式的特征</h5><p>3.1节讨论的各种内存管理策略都是为了同时将多个进程保存在内存中，以便允叶进仃多道程序设计。它们都具有以下两个共同的特征:</p>
<p>1） $\color{green}{\text{一次性}}$ 。作业必须一次性全部装入内存后，才能开始运行。这会导致两种情况:①当作业很大而不能全部被装入内存时，将使该作业无法运行;②当大量作业要求运行时，由于内存不足以容纳所有作业，只能使少数作业先运行，导致多道程序度的下降。</p>
<p>2） $\color{green}{\text{驻留性}}$ 。作业被装入内存后，就一直驻留在内存中，其任何部分都不会被换出，直至作业运行结束。运行中的进程会因等待IO而被阻塞，可能处于长期等待状态。</p>
<p>由以上分析可知，许多在程序运行中不用或暂时不用的程序（数据)占据了大量的内存空间，而一些需要运行的作业又无法装入运行，显然浪费了宝贵的内存资源。</p>
<h5 id="局部性原理"><a href="#局部性原理" class="headerlink" title="局部性原理"></a>局部性原理</h5><p>要真正理解虚拟内存技术的思想，首先须了解著名的局部性原理。Bill Joy (SUN公司CEO)说过:“在研究所时，我经常开玩笑地说高速缓存是计算机科学中唯一重要的思想。事实上，高速缓存技术确实极大地影响了计算机系统的设计。”快表、页高速缓存及虚拟内存技术从广义上讲，都属于高速缓存技术。这个技术所依赖的原理就是局部性原理。局部性原理既适用于程序结构，又适用于数据结构（更远地讲，Dijkstra 关于“goto 语句有害”的著名论文也出于对程序局部性原理的深刻认识和理解)。</p>
<p>局部性原理表现在以下两个方面:</p>
<p>1） $\color{green}{\text{时间局部性}}$ 。程序中的某条指令一旦执行，不久后该指令可能再次执行;某数据被访问过，不久后该数据可能再次被访问。产生时间局部性的典型原因是程序中存在着大量的循环操作。</p>
<p>2） $\color{green}{\text{空间局部性}}$ 。一旦程序访问了某个存储单元，在不久后，其附近的存储单元也将被访问，即程序在一段时间内所访问的地址，可能集中在一定的范围之内，因为指令通常是顺序存放、顺序执行的，数据也一般是以向量、数组、表等形式簇聚存储的。</p>
<p>时间局部性通过将近来使用的指令和数据保存到高速缓冲存储器中，并使用高速缓存的层次结构实现。空间局部性通常使用较大的高速缓存，并将预取机制集成到高速缓存控制逻辑中实现。虚拟内存技术实际上建立了“内存-外存”的两级存储器结构，利用局部性原理实现高速缓存。</p>
<h5 id="虚拟存储器的定义和特征"><a href="#虚拟存储器的定义和特征" class="headerlink" title="虚拟存储器的定义和特征"></a>虚拟存储器的定义和特征</h5><p>基于局部性原理，在程序装入时，将程序的一部分装入内存，而将其余部分留在外存，就可启动程序执行。在程序执行过程中，当所访问的信息不在内存时，由操作系统将所需要的部分调入内存，然后继续执行程序。另一方面，操作系统将内存中暂时不使用的内容换出到外存上，从而腾出空间存放将要调入内存的信息。这样，系统好像为用户提供了一个比实际内存大得多的存储器，称为 $\color{green}{\text{虚拟存储器}}$ 。</p>
<p>之所以将其称为虚拟存储器，是因为这种存储器实际上并不存在，只是由于系统提供了部分装入、请求调入和置换功能后（对用户完全透明)，给用户的感觉是好像存在一个比实际物理内存大得多的存储器。虚拟存储器的大小由计算机的地址结构决定，并不是内存和外存的简单相加。虚拟存储器有以下三个主要特征:</p>
<p>1） $\color{green}{\text{多次性}}$ 。多次性是指无须在作业运行时一次性地全部装入内存，而允许被分成多次调入内存运行。</p>
<p>2） $\color{green}{\text{对换性}}$ 。对换性是指无须在作业运行时一直常驻内存，而允许在作业的运行过程中，进行换进和换出。</p>
<p>3） $\color{green}{\text{虚拟性}}$ 。虚拟性是指从逻辑上扩充内存的容量，使用户所看到的内存容量远大于实际的内存容量。</p>
<h5 id="虚拟内存技术的实现"><a href="#虚拟内存技术的实现" class="headerlink" title="虚拟内存技术的实现"></a>虚拟内存技术的实现</h5><p>虚拟内存技术允许将一个作业分多次调入内存。采用连续分配方式时，会使相当一部分内存空间都处于暂时或“永久”的空闲状态，造成内存资源的严重浪费，而且也无法从逻辑上扩大内存容量。因此，虚拟内存的实现需要建立在离散分配的内存管理方式的基础上。</p>
<p>虚拟内存的实现有以下三种方式:</p>
<ul>
<li>请求分页存储管理。</li>
<li>请求分段存储管理。</li>
<li>请求段页式存储管理。</li>
</ul>
<p>不管哪种方式，都需要有一定的硬件支持。一般需要的支持有以下几个方面:</p>
<ul>
<li>一定容量的 $\color{green}{\text{内存}}$ 和 $\color{green}{\text{外存}}$ 。</li>
<li> $\color{green}{\text{页表机制}}$ （或段表机制)，作为主要的数据结构。</li>
<li> $\color{green}{\text{中断机构}}$ ，当用户程序要访问的部分尚未调入内存时，则产生中断。</li>
<li> $\color{green}{\text{地址变换机构}}$ ，逻辑地址到物理地址的变换。</li>
</ul>
<h4 id="请求分页管理方式"><a href="#请求分页管理方式" class="headerlink" title="请求分页管理方式"></a>请求分页管理方式</h4><p>请求分页系统建立在基本分页系统基础之上，为了支持虚拟存储器功能而增加了请求调页功能和页面置换功能。请求分页是目前最常用的一种实现虚拟存储器的方法。</p>
<p>在请求分页系统中，只要求将当前需要的一部分页面装入内存，便可以启动作业运行。在作业执行过程中，当所要访问的页面不在内存中时，再通过调页功能将其调入，同时还可通过置换功能将暂时不用的页面换出到外存上，以便腾出内存空间。</p>
<p>为了实现请求分页，系统必须提供一定的硬件支持。除了需要一定容量的内存及外存的计算机系统，还需要有页表机制、缺页中断机构和地址变换机构。</p>
<h5 id="页表机制"><a href="#页表机制" class="headerlink" title="页表机制"></a>页表机制</h5><p>请求分页系统的页表机制不同于基本分页系统，请求分页系统在一个作业运行之前不要求全部一次性调入内存，因此在作业的运行过程中，必然会出现要访问的页面不在内存中的情况，如何发现和处理这种情况是请求分页系统必须解决的两个基本问题。为此，在请求页表项中增加了4个字段，如图3.20所示。</p>
<details><summary>图3.20 请求分页系统中的页表项</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210926151836.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210926151836.png';" /></details>

<p>增加的4个字段说明如下:</p>
<ul>
<li>状态位P。用于指示该页是否已调入内存，供程序访问时参考。</li>
<li>访问字段A。用于记录本页在一段时间内被访问的次数，或记录本页最近已有多长时间未被访问，供置换算法换出页面时参考。</li>
<li>修改位M。标识该页在调入内存后是否被修改过。</li>
<li>外存地址。用于指出该页在外存上的地址，通常是物理块号，供调入该页时参考。</li>
</ul>
<h5 id="缺页中断机构"><a href="#缺页中断机构" class="headerlink" title="缺页中断机构"></a>缺页中断机构</h5><p>在请求分页系统中，每当所要访问的页面不在内存中时，便产生一个缺页中断，请求操作系统将所缺的页调入内存。此时应将缺页的进程阻塞（调页完成唤醒)，若内存中有空闲块,则分配一个块，将要调入的页装入该块，并修改页表中的相应页表项，若此时内存中没有空闲块，则要淘汰某页（若被淘汰页在内存期间被修改过，则要将其写回外存)。</p>
<p>缺页中断作为中断，同样要经历诸如保护CPU环境、分析中断原因、转入缺页中断处理程序、恢复CPU环境等几个步骤。但与一般的中断相比，它有以下两个明显的区别:</p>
<ul>
<li>在指令执行期间而非一条指令执行完后产生和处理中断信号，属于内部中断。</li>
<li>一条指令在执行期间，可能产生多次缺页中断。</li>
</ul>
<h5 id="地址变换机构"><a href="#地址变换机构" class="headerlink" title="地址变换机构"></a>地址变换机构</h5><p>请求分页系统中的地址变换机构，是在分页系统地址变换机构的基础上，为实现虚拟内存，又增加了某些功能而形成的。</p>
<p>如图3.21所示，在进行地址变换时，先检索快表:</p>
<details><summary>图3.21 请求分页中的地址变换过程</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210926152033.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210926152033.png';" /></details>

<ul>
<li>若找到要访问的页，则修改页表项中的访问位（写指令还需要重置修改位)，然后利用页表项中给出的物理块号和页内地址形成物理地址。</li>
<li>若未找到该页的页表项，则应到内存中去查找页表，再对比页表项中的状态位P，看该页是否已调入内存，未调入则产生缺页中断，请求从外存把该页调入内存。</li>
</ul>
<h4 id="页面置换算法（决定应该换入哪页、换出哪页"><a href="#页面置换算法（决定应该换入哪页、换出哪页" class="headerlink" title="页面置换算法（决定应该换入哪页、换出哪页)"></a>页面置换算法（决定应该换入哪页、换出哪页)</h4><p>进程运行时，若其访问的页面不在内存中而需将其调入，但内存已无空闲空间时，就需要从内存中调出一页程序或数据，送入磁盘的对换区。</p>
<p>选择调出页面的算法就称为页面置换算法。好的页面置换算法应有较低的页面更换频率也就是说，应将以后不会再访问或以后较长时间内不会再访问的页面先调出。</p>
<p>常见的置换算法有以下4种。</p>
<h5 id="最佳-OPT-置换算法"><a href="#最佳-OPT-置换算法" class="headerlink" title="最佳(OPT)置换算法"></a>最佳(OPT)置换算法</h5><p>最佳(Optimal，OPT)置换算法选择的被淘汰页面是以后永不使用的页面，或是在最长时间内不再被访问的页面，以便保证获得最低的缺页率。然而，由于人们目前无法预知进程在内存下的若干页面中哪个是未来最长时间内不再被访问的，因而该算法无法实现。</p>
<p>最佳置换算法可用来评价其他算法。假定系统为某进程分配了三个物理块，并考虑有页面号引用串7,0,1,2,0,3,0,4,2,3,0,3,2,1,2,0,1,7,0,1。进程运行时，先将7,0,1三个页面依次装入内存。进程要访问页面2时，产生缺页中断，根据最佳置换算法，选择将第18次访问才需调入的页面7淘汰。然后，访问页面0时，因为它已在内存中，所以不必产生缺页中断。访问页面3时，又会根据最佳置换算法将页面1淘汰……以此类推，如图3.22所示，从图中可以看出采用最佳置换算法时的情况。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210926152417.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210926152417.png';" /></details>

<p>最长时间不被访问和以后被访问次数最小是不同的概念，初学者在理解OPT 算法时千万不要混淆。</p>
<p>可以看到，发生缺页中断的次数为9，页面置换的次数为6。</p>
<h5 id="先进先出-FIFO-页面置换算法"><a href="#先进先出-FIFO-页面置换算法" class="headerlink" title="先进先出(FIFO)页面置换算法"></a>先进先出(FIFO)页面置换算法</h5><p>优先淘汰最早进入内存的页面，即在内存中驻留时间最久的页面。该算法实现简单，只需把调入内存的页面根据先后次序链接成队列，设置一个指针总指向最早的页面。但该算法与进程实际运行时的规律不适应，因为在进程中，有的页面经常被访问。</p>
<p>这里仍用上面的实例采用FIFO算法进行页面置换。进程访问页面2时，把最早进入内存的页面7换出。然后访问页面3时，把2,0,1中最先进入内存的页面0换出。由图3.23可以看出，利用FIFO算法时进行了12次页面置换，比最佳置换算法正好多一倍。</p>
<p>FIFO算法还会产生所分配的物理块数增大而页故障数不减反增的异常现象，这由Belady于1969年发现，因此称为Belady异常。只有FIFO算法可能出现  $\color{green}{\text{Belady异常}}$ ，LRU和OPT算法永远不会出现Belady 异常。</p>
<details><summary>图3.23 利用FIFO置换算法时的置换图</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210926152522.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210926152522.png';" /></details>

<p>如图3.24所示，页面访问顺序为3,2,1,0,3,2,4,3,2,1,0,4。若采用FIFO置换算法，当分配的物理块为3个时，缺页次数为9次;当分配的物理块为4个时，缺页次数为10次。分配给进程的物理块增多，但缺页次数不减反增。</p>
<details><summary>图3.24 Belady异常</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210926152549.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210926152549.png';" /></details>

<h5 id="最近最久未使用-LRU-置换算法"><a href="#最近最久未使用-LRU-置换算法" class="headerlink" title="最近最久未使用(LRU)置换算法"></a>最近最久未使用(LRU)置换算法</h5><p>选择最近最长时间未访问过的页面予以淘汰，它认为过去一段时间内未访问过的页面，在最近的将来可能也不会被访问。该算法为每个页面设置一个访问字段，来记录页面自上次被访问以来所经历的时间，淘汰页面时选择现有页面中值最大的予以淘汰。</p>
<p>再对上面的实例采用LRU算法进行页面置换，如图3.25所示。进程第一次对页面2访问时，将最近最久未被访问的页面7置换出去。然后在访问页面3时，将最近最久未使用的页面1换出。</p>
<details><summary>3.25 LRU页面置换算法时的置换图</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210926152649.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210926152649.png';" /></details>

<p>在图3.25中，前5次置换的情况与最佳置换算法相同，但两种算法并无必然联系。实际上，LRU算法根据各页以前的情况，是“向前看”的，而最佳置换算法则根据各页以后的使用情况，是“向后看”的。</p>
<p>LRU算法的性能较好，但需要寄存器和栈的硬件支持。LRU是堆栈类的算法。理论上可以证明， $\color{green}{\text{堆栈类算法}}$ 不可能出现Belady异常。FIFO算法基于队列实现，不是堆栈类算法。</p>
<h5 id="时钟-CLOCK-置换算法"><a href="#时钟-CLOCK-置换算法" class="headerlink" title="时钟(CLOCK)置换算法"></a>时钟(CLOCK)置换算法</h5><p>LRU算法的性能接近于OPT算法，但实现起来比较困难，且开销大;FIFO算法实现简单，但性能差。因此，操作系统的设计者尝试了很多算法，试图用比较小的开销接近LRU算法的性能，这类算法都是CLOCK算法的变体。因为算法要循环扫描缓冲区，像时钟的指针一样转动，所以称为CLOCK算法。</p>
<p>简单的CLOCK算法给每帧关联一个附加位，称为 $\color{green}{\text{使用位}}$ 。当某页首次装入主存时，将该帧的使用位设置为 1;当该页随后再被访问到时，其使用位也被置为1。对于页替换算法，用于替换的候选帧集合可视为一个循环缓冲区，并有一个指针与之相关联。当某一页被替换时，该指针被设置成指向缓冲区中的下一帧。当需要替换一页时，操作系统扫描缓冲区，以查找使用位被置为0的一帧。每当遇到一个使用位为1的帧时，操作系统就将该位重新置为0;若在这个过程开始时，缓冲区中所有帧的使用位均为0，则选择遇到的第一个帧替换;若所有帧的使用位均为1,则指针在缓冲区中完整地循环一周，把所有使用位都置为0，并停留在最初的位置上，替换该帧中的页。由于该算法循环检查各页面的情况，因此称 $\color{red}{\text{CLOCK算法}}$ ，又称 $\color{green}{\text{最近未用}}$ (Not RecentlyUsed，NRU）算法。</p>
<p>CLOCK算法的性能比较接近LRU算法，而通过增加使用的位数目，可以使得CLOCK算法更加高效。在使用位的基础上再增加一个修改位，则得到改进型CLOCK置换算法。这样，每帧都处于以下4种情况之一:</p>
<ul>
<li>最近未被访问，也未被修改（u = 0,m = 0)</li>
<li>最近被访问，但未被修改(u = 1,m = 0)</li>
<li>最近未被访问，但被修改（u = 0, m = 1)</li>
<li>最近被访问，被修改(u = 1, m= 1)</li>
</ul>
<p>算法执行如下操作步骤:</p>
<p>1）从指针的当前位置开始，扫描帧缓冲区。在这次扫描过程中，对使用位不做任何修改。选择遇到的第一个帧（u =0, m = 0）用于替换。</p>
<p>2）若第1）步失败，则重新扫描，查找(u=0,m= 1)的帧。选择遇到的第一个这样的帧用于替换。在这个扫描过程中，对每个跳过的帧，把它的使用位设置成0。</p>
<p>3）若第2）步失败，则指针将回到它的最初位置，且集合中所有帧的使用位均为0。重复第1）步，并且若有必要，重复第2）步，以便可以找到供替换的帧。</p>
<p>改进型CLOCK算法优于简单CLOCK算法的地方在于替换时首选没有变化的页。由于修改过的页在被替换之前必须写回，因而这样做会节省时间。</p>
<p>有些读者会认为CLOCK算法和改进型CLOCK算法记忆起来不易。为方便记忆，我们将其总结如下。</p>
<p>操作系统中任何经过优化而有效的页面置换算法都有一个原则，即尽可能保留曾经使用过的页面，而淘汰未使用的页面，认为这样可以在总体上减少换页次数。CLOCK 算法只考虑到是否被访问过，因此被访问过的当然尽可能留下，未使用过的就淘汰;而改进型CLOCK算法对使用过的页面又做了细分，分为使用过但未修改过和使用过且修改过。因此，若有未使用过的页面，则当然首先把它换出，若全部页面都使用过，则当然优先把未修改过的页面换出。</p>
<ul>
<li>替换优先级（在前面的优先被替换）： $\color{green}{\text{未修改}}$ &gt; $\color{green}{\text{未使用}}$ &gt;  $\color{green}{\text{修改}}$ </li>
</ul>
<p>为帮助读者理解，这里举一个例子。假设系统给某进程分配了5个页框，刚开始，进程依次访问1,3,4,2,5号页面，系统会将这些页面连成一个循环队列，刚开始扫描指针指向第一个被访问的页面(即1号页)，如图3.26所示。</p>
<details><summary>图3.26 刚开始扫描时指针指向的页面</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210926152915.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210926152915.png';" /></details>

<p>图3.26中，小括号内的数字就是使用位。接下来，若进程请求访问6号页面，则由于此时分配给进程的5个页框都被使用，因此必须选择一个页面置换出去。按照CLOCK置换算法的规则，在第一轮扫描中，指针扫过的页面的使用位应置为0。第一轮扫描的过程如图3.27所示。</p>
<details><summary>图3.27 第一轮扫描的过程</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210926152935.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210926152935.png';" /></details> 

<p>第一轮扫描中，未找到使用位为0的页面，因此需要进行第二轮扫描。第二轮扫描中，1号页面的使用位为0，因此将1号页面换出，将6号页面换入，将6号页的访问位设置为1，并将扫描指针后移（若下次需要换出页面，则从3号页面开始扫描)，如图3.28所示。</p>
<p>注意一个小细节:假设1号页面原先占有的是 $x$ 号物理块(页框)，则6号页面换入内存后也放在 $x$ 号物理块中。</p>
<details><summary>图3.28 第二轮扫描后指针指向的页面</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210926153014.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210926153014.png';" /></details>


<h4 id="页面分配策略"><a href="#页面分配策略" class="headerlink" title="页面分配策略"></a>页面分配策略</h4><h5 id="驻留集大小"><a href="#驻留集大小" class="headerlink" title="驻留集大小"></a>驻留集大小</h5><p>对于分页式的虚拟内存，在进程准备执行时，不需要也不可能把一个进程的所有页都读入主存。因此，操作系统必须决定读取多少页，即决定给特定的进程分配几个页框。给一个进程分配的物理页框的集合就是这个进程的 $\color{green}{\text{驻留集}}$ 。需要考虑以下几点:</p>
<p>1）分配给一个进程的存储量越小，任何时候驻留在主存中的进程数就越多，从而可以提高处理机的时间利用效率。</p>
<p>2）若一个进程在主存中的页数过少，则尽管有局部性原理，页错误率仍然会相对较高。</p>
<p>3）若页数过多，则由于局部性原理，给特定的进程分配更多的主存空间对该进程的错误率没有明显的影响。</p>
<p>基于这些因素，现代操作系统通常采用三种策略:</p>
<p>1） $\color{green}{\text{固定分配局部置换}}$ 。它为每个进程分配一定数目的物理块，在整个运行期间都不改变。若进程在运行中发生缺页，则只能从该进程在内存中的页面中选出一页换出，然后调入需要的页面。实现这种策略时，难以确定应为每个进程分配的物理块数目:太少会频繁出现缺页中断，太多又会使CPU和其他资源利用率下降。</p>
<p>2） $\color{green}{\text{可变分配全局置换}}$ 。这是最易于实现的物理块分配和置换策略，它为系统中的每个进程分配一定数目的物理块，操作系统自身也保持一个空闲物理块队列。当某进程发生缺页时，系统从空闲物理块队列中取出一个物理块分配给该进程，并将欲调入的页装入其中。这种方法比固定分配局部置换更加灵活，可以动态增加进程的物理块，但也存在弊端如它会盲目地给进程增加物理块，从而导致系统多道程序的并发能力下降。</p>
<p>3） $\color{green}{\text{可变分配局部置换}}$ 。它为每个进程分配一定数目的物理块，当某个进程发生缺页时，只允许从该进程在内存的页面中选出一页换出，因此不会影响其他进程的运行。若进程在运行中频繁地缺页，则系统再为该进程分配若干物理块，直至该进程缺页率趋于适当程度;反之，若进程运行中的缺页率特别低，则可适当减少分配给该进程的物理块。比起可变分配全局置换，这种方法不仅可以动态增加进程物理块的数量，还能动态减少进程物理块的数量，在保证进程不会过多地调页的同时，也保持了系统的多道程序并发能力。当然它需要更复杂的实现，也需要更大的开销，但对比频繁地换入/换出所浪费的计算机资源，这种牺牲是值得的。</p>
<p>页面分配策略在2015年的统考选择题中出现过，考查的是这三种策略的名称。往年很多读者看到这里时，由于认为不是重点，复习时便一带而过，最后在考试中失分。在这种基础题上失分是十分可惜的。再次提醒读者，考研成功的秘诀在于“反复多次”和“全面”。</p>
<h5 id="调入页面的时机"><a href="#调入页面的时机" class="headerlink" title="调入页面的时机"></a>调入页面的时机</h5><p>为确定系统将进程运行时所缺的页面调入内存的时机，可采取以下两种调页策略:</p>
<p>1）预调页策略。根据局部性原理，一次调入若干相邻的页可能会比一次调入一页更高效。但若调入的一批页面中大多数都未被访问，则又是低效的。因此，需要采用以预测为基础的预调页策略，将预计在不久之后便会被访问的页面预先调入内存。但目前预调页的成功率仅约50%。因此这种策略主要用于进程的首次调入，由程序员指出应先调入哪些页。</p>
<p>2）请求调页策略。进程在运行中需要访问的页面不在内存而提出请求，由系统将所需页面调入内存。由这种策略调入的页一定会被访问，且这种策略比较易于实现，因此在目前的虚拟存储器中大多采用此策略。它的缺点是每次只调入一页，调入/调出页面数多时会花费过多的IO开销。</p>
<p>预调入实际上就是运行前的调入，请求调页实际上就是运行期间调入。一般情况下，两种调页策略会同时使用。</p>
<h5 id="从何处调入页面"><a href="#从何处调入页面" class="headerlink" title="从何处调入页面"></a>从何处调入页面</h5><p>请求分页系统中的外存分为两部分:用于存放文件的文件区和用于存放对换页面的对换区。对换区通常采用连续分配方式，而文件区采用离散分配方式，因此对换区的磁盘IO速度比文件区的更快。这样，从何处调入页面就存在三种情况:</p>
<p>1）系统拥有足够的对换区空间。可以全部从对换区调入所需页面，以提高调页速度。为此，在进程运行前，需将与该进程有关的文件从文件区复制到对换区。</p>
<p>2）系统缺少足够的对换区空间。凡不会被修改的文件都直接从文件区调入;而当换出这些页面时，由于它们未被修改而不必再将它们换出。但对于那些可能被修改的部分，在将它们换出时须调到对换区，以后需要时再从对换区调入（因为读的速度比写的速度快)</p>
<p>3）UNIX方式。与进程有关的文件都放在文件区，因此未运行过的页面都应从文件区调入曾经运行过但又被换出的页面，由于放在对换区，因此下次调入时应从对换区调入。进程请求的共享页面若被其他进程调入内存，则无须再从对换区调入。</p>
<h4 id="抖动"><a href="#抖动" class="headerlink" title="抖动"></a>抖动</h4><p>在页面置换过程中，一种最糟糕的情形是，刚刚换出的页面马上又要换入主存，刚刚换入的页面马上又要换出主存，这种频繁的页面调度行为称为抖动或颠簸。若一个进程在换页上用的时间多于执行时间，则这个进程就在颠簸。</p>
<p>频繁发生缺页中断(抖动）的主要原因是，某个进程频繁访问的页面数目高于可用的物理页帧数目。虚拟内存技术可在内存中保留更多的进程以提高系统效率。在稳定状态，几乎主存的所有空间都被进程块占据，处理机和操作系统可以直接访问到尽可能多的进程。然而，如果管理不当，那么处理机的大部分时间都将用于交换块，即请求调入页面的操作，而不是执行进程的指令，因此会大大降低系统效率。</p>
<h4 id="工作集"><a href="#工作集" class="headerlink" title="工作集"></a>工作集</h4><p>工作集是指在某段时间间隔内，进程要访问的页面集合。基于局部性原理，可以用最近访问过的页面来确定工作集。一般来说，工作集W可由时间t和工作集窗口大小△来确定。例如，某进程对页面的访问次序如下:</p>
<div style='text-align:center'><details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210926153350.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210926153350.png';" /></details></div>



<h4 id="地址翻译"><a href="#地址翻译" class="headerlink" title="地址翻译"></a>地址翻译</h4><p>本小节引入一个实例来说明虚实地址的变换过程，考虑到统考试题近来出现了学科综合的趋势，这里结合“计算机组成原理”中的Cache部分进行讲解。对于不参加统考的读者，可以看到翻译出实地址为止，对于参加统考却还没有复习计算机组成原理的读者，可在复习完“计算机组成原理”后，再回来看本章的内容。</p>
<p>设某系统满足以下条件:</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211012201423.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211012201423.png';" /></details>

<ul>
<li>$\color{red}{\text{Q：}}$ 为什么虚拟地址和物理地址的位数可以不一致</li>
<li>$\color{green}{\text{A：}}}$ 确实可以，只要按照低位对齐就好了</li>
</ul>
<h4 id="本章小结"><a href="#本章小结" class="headerlink" title="本章小结"></a>本章小结</h4><h5 id="为什么要引入虚拟内存"><a href="#为什么要引入虚拟内存" class="headerlink" title="为什么要引入虚拟内存?"></a>为什么要引入虚拟内存?</h5><p>上一节提到过，多道程序并发执行不仅使进程之间共享了处理器，而且同时共享了主存。然而，随着对处理器需求的增长，进程的执行速度会以某种合理平滑的方式慢下来。但是，若同时运行的进程太多，则需要很多的内存，当一个程序没有内存空间可用时，那么它甚至无法运行。所以，在物理上扩展内存相对有限的条件下，应尝试以一些其他可行的方式在逻辑上扩充内存。</p>
<h5 id="虚拟内存（虚存）空间的大小由什么因素决定"><a href="#虚拟内存（虚存）空间的大小由什么因素决定" class="headerlink" title="虚拟内存（虚存）空间的大小由什么因素决定?"></a>虚拟内存（虚存）空间的大小由什么因素决定?</h5><p>虚存的容量要满足以下两个条件:</p>
<p>①虚存的实际容量≤内存容量和外存容量之和，这是硬件的硬性条件规定的，若虚存的实际容量超过了这个容量，则没有相应的空间来供虚存使用。</p>
<p>${\textstyle\unicode{x2461}}$  虚存的最大容量≤计算机的地址位数能容纳的最大容量。假设地址是32位的，按字节编址，一个地址代表1B存储空间，则虚存的最大容量≤4GB( $2^{32}$ B)。这是因为若虚存的最大容量超过4GB，则32位的地址将无法访问全部虚存，也就是说4GB 以后的空间被浪费了，相当于没有一样，没有任何意义。</p>
<p>实际虚存的容量是取条件①和②的交集，即两个条件都要满足，仅满足一个条件是不行的。</p>
<h5 id="虚拟内存是怎么解决问题的-会带来什么问题"><a href="#虚拟内存是怎么解决问题的-会带来什么问题" class="headerlink" title="虚拟内存是怎么解决问题的?会带来什么问题?"></a>虚拟内存是怎么解决问题的?会带来什么问题?</h5><p>虚拟内存使用外存上的空间来扩充内存空间，通过一定的换入/换出，使得整个系统在逻辑上能够使用一个远远超出其物理内存大小的内存容量。因为虚拟内存技术调换页面时需要访问外存，会导致平均访存时间增加，若使用了不合适的替换算法，则会大大降低系统性能。</p>
<p>本节学习了4种页面置换算法，要把它们与处理机调度算法区分开。当然，这些调度算法之间也是有联系的，它们都有一个共同点，即通过一定的准则决定资源的分配对象。在处理机调度算法中这些准则比较多，有优先级、响应比、时间片等，而在页面调度算法中就比较简单，即是否被用到过或近段时间内是否经常使用。在操作系统中，几乎每类资源都会有相关的调度算法，读者通过将这些调度算法作为线索，可把整个操作系统的课程连成一个整体。</p>
<h3 id="本章疑难点"><a href="#本章疑难点" class="headerlink" title="本章疑难点"></a>本章疑难点</h3><p>分页管理方式和分段管理方式在很多地方是相似的，比如在内存中都是不连续的、都有地址变换机构来进行地址映射等。但两者也存在许多区别，表3.7列出了分页管理方式和分段管理方式各方面的对比。</p>
<details><summary>表3.7分页管理方式和分段管理方式的比较
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211013135503.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211013135503.png';" /></details>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://ednow.github.io/2021/07/26/%E7%8E%8B%E9%81%93-%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-ch2-%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ednow">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ednow">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/26/%E7%8E%8B%E9%81%93-%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-ch2-%E8%BF%9B%E7%A8%8B%E7%AE%A1%E7%90%86/" class="post-title-link" itemprop="url">王道-操作系统-ch2-进程管理</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-07-26 15:16:47" itemprop="dateCreated datePublished" datetime="2021-07-26T15:16:47+08:00">2021-07-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-10-12 11:02:50" itemprop="dateModified" datetime="2021-10-12T11:02:50+08:00">2021-10-12</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="王道"><a href="#王道" class="headerlink" title="王道"></a>王道</h1><h2 id="进程管理"><a href="#进程管理" class="headerlink" title="进程管理"></a>进程管理</h2><p>【考纲内容】</p>
<p>(一）进程与线程</p>
<ol>
<li>进程与 $\color{green}{\text{线程}}$ 的基本概念</li>
<li>进程/ $\color{red}{\text{线程的状态与转换}}$</li>
<li> $\color{green}{\text{线程的实现}}$ （内核支持的线程，线程库支持的线程）</li>
<li>进程与 $\color{red}{\text{线程的组织与控制}}$ </li>
<li>进程间的通信（共享内存，消息传递，管道）</li>
</ol>
<p>(二）CPU调度与上下文切换</p>
<ol>
<li>调度的基本概念</li>
<li> $\color{green}{\text{调度的目标}}$ </li>
<li> $\color{green}{\text{调度的实现}}$ （ $\color{red}{\text{调度器/调度程序}}$ （scheduler），调度的时机与调度方式（抢占式/非抢占式）， $\color{red}{\text{闲逛进程}}$ ， $\color{red}{\text{内核级线程与用户级线程调度}}$ ）</li>
<li> 典型调度算法（先来先服务调度算法；短作业（短进程、短线程)优先调度算法;时间片轮转调度算法;优先级调度算法;高响应比优先调度算法; $\color{red}{\text{多级队列调度算法}}$ ;多级反馈队列调度算法。）</li>
<li>  $\color{green}{\text{上下文及其切换机制}}$ </li>
</ol>
<p>(三）进程同步</p>
<ol>
<li>进程同步的基本概念</li>
<li>基本实现方法（软件方法，硬件方法）</li>
<li> $\color{red}{\text{锁}}$ </li>
<li>信号量</li>
<li> $\color{red}{\text{条件变量}}$ </li>
<li>经典同步问题（生产者-消费者问题；读者-写者问题；哲学家进餐问题）</li>
</ol>
<p>(四）死锁</p>
<ol>
<li>死锁的概念</li>
<li>死锁处理策略</li>
<li>死锁预防</li>
<li>死锁避免</li>
<li>死锁的检测和解除</li>
</ol>
<p>【知识框架】</p>
<ul>
<li>进程<ul>
<li>概念:与程序的区别</li>
<li>特征:动态性、并发性、独立性、异步性、结构性</li>
<li>状态:运行、就绪、阻塞、创建、结束</li>
<li>控制:创建、终止、阻塞和唤醒、切换</li>
<li>组织:进程控制块PCB、程序段、数据段</li>
<li>通信:共享存储、消息传递、管道通信</li>
</ul>
</li>
<li>线程<ul>
<li>概念、与进程的比较、属性</li>
<li>线程的实现方式</li>
</ul>
</li>
<li>处理机调度<ul>
<li>概念、三级调度:作业调度、中级调度、进程调度调度方式:剥夺式、非剥夺式</li>
<li>调度准则:CPU利用率、吞吐量、周转时间、等待时间、响应时间</li>
<li>算法:先来先服务、短作业(SJF）优先、优先级、高响应比优先、时间片轮转、多级反馈队列</li>
</ul>
</li>
<li>进程同步<ul>
<li>概念:临界资源、同步、互斥</li>
<li>实现方法:软件实现的几种算法、硬件实现</li>
<li>信号量:整型、记录型</li>
<li>经典问题:生产者-消费者问题、读者-写者问题、哲学家进餐问题、吸烟者问题</li>
</ul>
</li>
<li>死锁<ul>
<li>定义</li>
<li>原因:系统资源竞争、进程推进顺序非法</li>
<li>条件:互斥、不剥夺、请求和保持、循环等待</li>
<li>策略:预防死锁、避免死锁、死锁的检测与解除</li>
</ul>
</li>
</ul>
<p>【复习提示】</p>
<p>进程管理是操作系统的核心，也是每年必考的重点。其中，进程的概念、进程调度、信号量机制实现同步和互斥、进程死锁等更是重中之重，必须深入掌握。需要注意的是，除选择题外，本章还容易出综合题，其中信号量机制实现同步和互斥、进程调度算法和银行家算法都是可能出现的综合题考点，如利用信号量进行进程同步就在往年的统考中频繁出现。</p>
<p>进程:process</p>
<h3 id="进程与线程"><a href="#进程与线程" class="headerlink" title="进程与线程"></a>进程与线程</h3><p>在学习本节时，请读者思考以下问题:</p>
<p>1）为什么要引入进程?</p>
<p>2）什么是进程?进程由什么组成?</p>
<p>3）进程是如何解决问题的?</p>
<p>希望读者带着上述问题去学习本节内容，并在学习的过程中多思考，从而更深入地理解本节内容。进程本身是一个比较抽象的概念，它不是实物，看不见、摸不着，初学者在理解进程概念时存在一定困难，在介绍完进程的相关知识后，我们会用比较直观的例子帮助大家理解。</p>
<h4 id="进程的概念和特征"><a href="#进程的概念和特征" class="headerlink" title="进程的概念和特征"></a>进程的概念和特征</h4><h5 id="进程的概念"><a href="#进程的概念" class="headerlink" title="进程的概念"></a>进程的概念</h5><p>在多道程序环境下，允许多个程序并发执行，此时它们将失去封闭性，并具有间断性及不可再现性的特征。为此引入了进程（Process）的概念，以便更好地描述和控制程序的并发执行，实现操作系统的并发性和共享性（最基本的两个特性)。</p>
<p>为了使参与并发执行的程序（含数据）能独立地运行，必须为之配置一个专门的数据结构，称为进程控制块（Process Control Block，PCB)。系统利用PCB来描述进程的基本情况和运行状态，进而控制和管理进程。相应地，由程序段、相关数据段和PCB三部分构成了进程映像（进程实体)。所谓创建进程，实质上是创建进程映像中的PCB;而撤销进程,实质上是撤销进程的PCB。值得注意的是，进程映像是静态的，进程则是动态的。</p>
<p>注意:PCB是进程存在的唯一标志!</p>
<p>从不同的角度，进程可以有不同的定义，比较典型的定义有:</p>
<p>1）进程是程序的一次执行过程。</p>
<p>2）进程是一个程序及其数据在处理机上顺序执行时所发生的活动。</p>
<p>3）进程是具有独立功能的程序在一个数据集合上运行的过程，它是系统进行资源分配和调度的一个独立单位。</p>
<p>引入进程实体的概念后，我们可以把传统操作系统中的进程定义为:“进程是进程实体的运行过程，是系统进行资源分配和调度的一个独立单位。”</p>
<p>读者要准确理解这里说的系统资源。它指处理机、存储器和其他设备服务于某个进程的“时间”，例如把处理机资源理解为处理机的时间片才是准确的。因为进程是这些资源分配和调度的独立单位，即“时间片”分配的独立单位，这就决定了进程一定是一个动态的、过程性的概念。</p>
<h5 id="进程的特征"><a href="#进程的特征" class="headerlink" title="进程的特征"></a>进程的特征</h5><p>进程是由多道程序的并发执行而引出的，它和程序是两个截然不同的概念。进程的基本特征是对比单个程序的顺序执行提出的，也是对进程管理提出的基本要求。</p>
<p>1）动态性。进程是程序的一次执行，它有着创建、活动、暂停、终止等过程，具有一定的生命周期，是动态地产生、变化和消亡的。动态性是进程最基本的特征。</p>
<p>2）并发性。指多个进程实体同时存于内存中，能在一段时间内同时运行。并发性是进程的重要特征，同时也是操作系统的重要特征。引入进程的目的就是使程序能与其他进程的程序并发执行，以提高资源利用率。</p>
<p>3）独立性。指进程实体是一个能独立运行、独立获得资源和独立接受调度的基本单位。凡未建立PCB的程序，都不能作为一个独立的单位参与运行。</p>
<p>4）异步性。由于进程的相互制约，使得进程具有执行的间断性，即进程按各自独立的、不可预知的速度向前推进。异步性会导致执行结果的不可再现性，为此在操作系统中必须配置相应的进程同步机制。</p>
<p>5）结构性。每个进程都配置一个PCB对其进行描述。从结构上看，进程实体是由程序段、数据段和进程控制块三部分组成的。</p>
<p>通常不会直接考查进程有什么特性，所以读者对上面的5个特性不求记忆，只求理解。</p>
<blockquote>
<p><a target="_blank" rel="noopener" href="https://www.zhihu.com/question/67644873/answer/829627104">并发和异步的区别</a></p>
</blockquote>
<h4 id="进程的状态与转换"><a href="#进程的状态与转换" class="headerlink" title="进程的状态与转换"></a>进程的状态与转换</h4><p>进程在其生命周期内，由于系统中各进程之间的相互制约关系及系统的运行环境的变化，使得进程的状态也在不断地发生变化(一个进程会经历若干不同状态)。通常进程有以下5种状态，前3种是进程的基本状态。</p>
<p>1）运行态。进程正在处理机上运行。在单处理机环境下，每个时刻最多只有一个进程处于运行态。</p>
<p>2）就绪态。进程获得了除处理机外的一切所需资源，一旦得到处理机，便可立即运行。系统中处于就绪状态的进程可能有多个，通常将它们排成一个队列，称为就绪队列。</p>
<p>3）阻塞态，又称等待态。进程正在等待某一事件而暂停运行，如等待某资源为可用（不包括处理机）或等待输入/输出完成。即使处理机空闲，该进程也不能运行。</p>
<p>4）创建态。进程正在被创建，尚未转到就绪态。创建进程通常需要多个步骤:首先申请一个空白的 PCB，并向PCB中填写一些控制和管理进程的信息;然后由系统为该进程分配运行时所必需的资源;最后把该进程转入就绪态。</p>
<p>5）结束态。进程正从系统中消失，可能是进程正常结束或其他原因中断退出运行。进程需要结束运行时，系统首先必须将该进程置为结束态，然后进一步处理资源释放和回收等工作。</p>
<p>注意区别就绪态和等待态:就绪态是指进程仅缺少处理机，只要获得处理机资源就立即运行;而等待态是指进程需要其他资源（除了处理机）或等待某一事件。之所以把处理机和其他资源划分开，是因为在分时系统的时间片轮转机制中，每个进程分到的时间片是若干毫秒。也就是说，进程得到处理机的时间很短且非常频繁，进程在运行过程中实际上是频繁地转换到就绪态的;而其他资源（如外设）的使用和分配或某一事件的发生（如IO操作的完成）对应的时间相对来说很长，进程转换到等待态的次数也相对较少。这样来看，就绪态和等待态是进程生命周期中两个完全不同的状态，显然需要加以区分。</p>
<p>图2.1说明了5种进程状态的转换，而3种基本状态之间的转换如下:</p>
<ul>
<li>就绪态→运行态:处于就绪态的进程被调度后，获得处理机资源（分派处理机时间片)，于是进程由就绪态转换为运行态。 </li>
<li>运行态→就绪态:处于运行态的进程在时间片用完后，不得不让出处理机，从而进程由运行态转换为就绪态。此外，在可剥夺的操作系统中，当有更高优先级的进程就绪时，调度程序将正在执行的进程转换为就绪态，让更高优先级的进程执行。</li>
<li>运行态→阻塞态:进程请求某一资源（如外设）的使用和分配或等待某一事件的发生(如I/O操作的完成）时，它就从运行态转换为阻塞态。进程以系统调用的形式请求操作系统提供服务，这是一种特殊的、由运行用户态程序调用操作系统内核过程的形式。</li>
<li>阻塞态→就绪态:进程等待的事件到来时，如IO操作结束或中断结束时，中断处理程序必须把相应进程的状态由阻塞态转换为就绪态。</li>
</ul>
<details><summary>图2.1 5种进程状态的转换</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210808131002.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210808131002.png';" /></details>

<p>需要注意的是，一个进程从运行态变成阻塞态是 $\color{green}{\text{主动}}$ 的行为，而从阻塞态变成就绪态是 $\color{green}{\text{被动}}$ 的行为，需要其他相关进程的协助。</p>
<h4 id="进程控制"><a href="#进程控制" class="headerlink" title="进程控制"></a>进程控制</h4><p>进程控制的主要功能是对系统中的所有进程实施有效的管理，它具有创建新进程、撤销已有进程、实现进程状态转换等功能。在操作系统中，一般把进程控制用的程序段称为原语，原语的特点是执行期间不允许中断，它是一个不可分割的基本单位。</p>
<h5 id="进程的创建"><a href="#进程的创建" class="headerlink" title="进程的创建"></a>进程的创建</h5><p>允许一个进程创建另一个进程。此时创建者称为父进程，被创建的进程称为子进程。子进程可以继承父进程所拥有的资源。当子进程被撤销时，应将其从父进程那里获得的资源归还给程。此外，在撤销父进程时，必须同时撤销其所有的子进程。</p>
<p>在操作系统中，终端用户登录系统、作业调度、系统提供服务、用户程序的应用请求等都会引起进程的创建。操作系统创建一个新进程的过程如下（创建原语):<br>1）为新进程分配一个唯一的进程标识号，并申请一个空白的 PCB (PCB是有限的)。若申请失败，则创建失败。</p>
<p>2）为进程分配资源，为新进程的程序和数据及用户栈分配必要的内存空间（在PCB中体现)。注意，若资源不足（如内存空间)，则并不是创建失败，而是处于阻塞态，等待内存资源。</p>
<p>3）初始化 PCB，主要包括初始化标志信息、初始化处理机状态信息和初始化处理机控制信息，以及设置进程的优先级等。</p>
<p>4）若进程就绪队列能够接纳新进程，则将新进程插入就绪队列，等待被调度运行。</p>
<h5 id="进程的终止"><a href="#进程的终止" class="headerlink" title="进程的终止"></a>进程的终止</h5><p>引起进程终止的事件主要有:①正常结束，表示进程的任务已完成并准备退出运行。②异常结束，表示进程在运行时，发生了某种异常事件，使程序无法继续运行，如存储区越界、保护错、非法指令、特权指令错、运行超时、算术运算错、I/O故障等。③外界干预，指进程应外界的请求而终止运行，如操作员或操作系统干预、父进程请求和父进程终止。</p>
<p>操作系统终止进程的过程如下（撤销原语):</p>
<p>1）根据被终止进程的标识符，检索PCB，从中读出该进程的状态。</p>
<p>2）若被终止进程处于执行状态，立即终止该进程的执行，将处理机资源分配给其他进程。</p>
<p>3）若该进程还有子孙进程，则应将其所有子孙进程终止。</p>
<p>4）将该进程所拥有的全部资源，或归还给其父进程，或归还给操作系统。</p>
<p>5）将该PCB从所在队列（链表）中删除。</p>
<h5 id="进程的阻塞和唤醒"><a href="#进程的阻塞和唤醒" class="headerlink" title="进程的阻塞和唤醒"></a>进程的阻塞和唤醒</h5><p>正在执行的进程，由于期待的某些事件未发生，如请求系统资源失败、等待某种操作的完成、新数据尚未到达或无新工作可做等，由系统自动执行阻塞原语（Block)，使自己由运行态变为阻塞态。可见，进程的阻塞是进程自身的一种主动行为，也因此只有处于运行态的进程(获得CPU才可能将其转为阻塞态。阻塞原语的执行过程如下:</p>
<p>1）找到将要被阻塞进程的标识号对应的PCB。</p>
<p>2）若该进程为运行态，则保护其现场，将其状态转为阻塞态，停止运行。3）把该PCB插入相应事件的等待队列，将处理机资源调度给其他就绪进程。</p>
<p>当被阻塞进程所期待的事件出现时,如它所启动的IO操作已完成或其所期待的数据已到达，由有关进程（比如，释放该IO 设备的进程，或提供数据的进程）调用唤醒原语（Wakeup)，将等待该事件的进程唤醒。唤醒原语的执行过程如下:</p>
<p>1）在该事件的等待队列中找到相应进程的PCB。</p>
<p>2）将其从等待队列中移出，并置其状态为就绪态。</p>
<p>3）把该PCB插入就绪队列，等待调度程序调度。</p>
<p>需要注意的是，Block原语和 Wakeup原语是一对作用刚好相反的原语，必须成对使用。Block原语是由被阻塞进程自我调用实现的，而 Wakeup原语则是由一个与被唤醒进程合作或被其他相关的进程调用实现的。</p>
<h5 id="进程切换"><a href="#进程切换" class="headerlink" title="进程切换"></a>进程切换</h5><p>对于通常的进程而言，其创建、撤销及要求由系统设备完成的I/O操作，都是利用系统调用而进入内核，再由内核中的相应处理程序予以完成的。进程切换同样是在内核的支持下实现的，因此可以说，任何进程都是在操作系统内核的支持下运行的，是与内核紧密相关的。</p>
<p>进程切换是指处理机从一个进程的运行转到另一个进程上运行，在这个过程中，进程的运行环境产生了实质性的变化。进程切换的过程如下:</p>
<p>1））保存处理机上下文，包括程序计数器和其他寄存器。</p>
<p>2）更新PCB信息。</p>
<p>3）把进程的PCB移入相应的队列，如就绪、在某事件阻塞等队列。</p>
<p>4）选择另一个进程执行，并更新其PCB。</p>
<p>5）更新内存管理的数据结构。</p>
<p>6）恢复处理机上下文。</p>
<blockquote>
<p>注意，进程切换与处理机模式切换是不同的，模式切换时，处理机逻辑上可能还在同一进程中运行。若进程因中断或异常进入核心态运行，执行完后又回到用户态刚被中断的程序运行，则操作系统只需恢复进程进入内核时所保存的CPU现场，而无须改变当前进程的环境信息。但若要切换进程，当前运行进程改变了，则当前进程的环境信息也需要改变。</p>
</blockquote>
<h4 id="进程的组织"><a href="#进程的组织" class="headerlink" title="进程的组织"></a>进程的组织</h4><p>进程是一个独立的运行单位，也是操作系统进行资源分配和调度的基本单位。它由以下三部分组成，其中最核心的是进程控制块（PCB)。</p>
<h5 id="进程控制块"><a href="#进程控制块" class="headerlink" title="进程控制块"></a>进程控制块</h5><p>进程创建时，操作系统为它新建一个PCB，该结构之后常驻内存，任意时刻都可以存取，并在进程结束时删除。PCB是进程实体的一部分，是进程存在的唯一标志。</p>
<p>进程执行时，系统通过其 PCB 了解进程的现行状态信息，以便对其进行控制和管理;进程结束时，系统收回其PCB，该进程随之消亡。操作系统通过PCB表来管理和控制进程。</p>
<p>当操作系统欲调度某进程运行时，要从该进程的PCB中查出其现行状态及优先级;在调度到某进程后，要根据其PCB中所保存的处理机状态信息，设置该进程恢复运行的现场，并根据其PCB中的程序和数据的内存始址，找到其程序和数据;进程在运行过程中，当需要和与之合作的进程实现同步、通信或访问文件时，也需要访问PCB;当进程由于某种原因而暂停运行时，又需将其断点的处理机环境保存在PCB中。可见，在进程的整个生命期中，系统总是通过PCB对进程进行控制的，亦即系统唯有通过进程的PCB才能感知到该进程的存在。</p>
<p>表2.1是一个PCB 的实例。PCB主要包括进程描述信息、进程控制和管理信息、资源分配清单和处理机相关信息等。各部分的主要说明如下:</p>
<details><summary>表2.1 PCB通常包含的内容</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210808131744.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210808131744.png';" /></details>

<p>1）进程描述信息。进程标识符:标志各个进程，每个进程都有一个唯一的标识号。用户标识符:进程归属的用户，用户标识符主要为共享和保护服务。</p>
<p>2）进程控制和管理信息。进程当前状态:描述进程的状态信息，作为处理机分配调度的依据。进程优先级:描述进程抢占处理机的优先级，优先级高的进程可优先获得处理机。</p>
<p>3）资源分配清单，用于说明有关内存地址空间或虚拟地址空间的状况，所打开文件的列表和所使用的输入/输出设备信息。</p>
<p>4）处理机相关信息，主要指处理机中各寄存器的值，当进程被切换时，处理机状态信息都必须保存在相应的PCB中，以便在该进程重新执行时，能从断点继续执行。</p>
<p>在一个系统中，通常存在着许多进程的PCB，有的处于就绪态，有的处于阻塞态，而且阻塞的原因各不相同。为了方便进程的调度和管理，需要将各进程的PCB用适当的方法组织起来。目前，常用的组织方式有链接方式和索引方式两种。链接方式将同一状态的PCB链接成一个队列,不同状态对应不同的队列，也可把处于阻塞态的进程的PCB，根据其阻塞原因的不同，排成多个阻塞队列。索引方式将同一状态的进程组织在一个索引表中，索引表的表项指向相应的PCB，不同状态对应不同的索引表，如就绪索引表和阻塞索引表等。</p>
<h5 id="程序段"><a href="#程序段" class="headerlink" title="程序段"></a>程序段</h5><p>程序段就是能被进程调度程序调度到CPU执行的程序代码段。注意，程序可被多个进程共享，即多个进程可以运行同一个程序。</p>
<h5 id="数据段"><a href="#数据段" class="headerlink" title="数据段"></a>数据段</h5><p>一个进程的数据段，可以是进程对应的程序加工处理的原始数据，也可以是程序执行时产生的中间或最终结果。</p>
<h4 id="进程的通信"><a href="#进程的通信" class="headerlink" title="进程的通信"></a>进程的通信</h4><p>进程通信是指进程之间的信息交换。PV操作是低级通信方式，高级通信方式是指以较高的效率传输大量数据的通信方式。高级通信方法主要有以下三类。</p>
<h5 id="共享存储"><a href="#共享存储" class="headerlink" title="共享存储"></a>共享存储</h5><p>在通信的进程之间存在一块可直接访问的共享空间,通过对这片共享空间进行写/读操作实现进程之间的信息交换，如图2.2所示。在对共享空间进行写/读操作时，需要使用同步互斥工具(如Р操作、V操作)，对共享空间的写/读进行控制。共享存储又分为两种:低级方式的共享是基于数据结构的共享;高级方式的共享则是基于存储区的共享。操作系统只负责为通信进程提供可共享使用的存储空间和同步互斥工具，而数据交换则由用户自己安排读/写指令完成。</p>
<p>注意，用户进程空间一般都是独立的，进程运行期间一般不能访问其他进程的空间，要想让两个用户进程共享空间，必须通过特殊的系统调用实现，而进程内的线程是自然共享进程空间的。</p>
<p>简单理解就是，甲和乙中间有一个大布袋，甲和乙交换物品是通过大布袋进行的，甲把物品放在大布袋里，乙拿走。但乙不能直接到甲的手中拿东西，甲也不能直接到乙的手中拿东西。</p>
<h5 id="消息传递"><a href="#消息传递" class="headerlink" title="消息传递"></a>消息传递</h5><p>在消息传递系统中，进程间的数据交换是以格式化的消息（Message）为单位的。若通信的进程之间不存在可直接访问的共享空间，则必须利用操作系统提供的消息传递方法实现进程通信。进程通过系统提供的发送消息和接收消息两个原语进行数据交换。</p>
<p>1）直接通信方式。发送进程直接把消息发送给接收进程，并将它挂在接收进程的消息缓冲队列上，接收进程从消息缓冲队列中取得消息，如图2.3所示。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210808132052.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210808132052.png';" /></details>

<p>2）间接通信方式。发送进程把消息发送到某个中间实体，接收进程从中间实体取得消息。</p>
<p>这种中间实体一般称为信箱，这种通信方式又称信箱通信方式。该通信方式广泛应用于计算机网络中，相应的通信系统称为电子邮件系统。<br>简单理解就是，甲要告诉乙某些事情，就要写信，然后通过邮差送给乙。直接通信就是邮差把信直接送到乙的手上;间接通信就是乙家门口有一个邮箱，邮差把信放到邮箱里。</p>
<h5 id="管道通信"><a href="#管道通信" class="headerlink" title="管道通信"></a>管道通信</h5><p>管道通信是消息传递的一种特殊方式（见图2.4)。所谓“管道”，是指用于连接一个读进程和一个写进程以实现它们之间的通信的一个共享文件，又名pipe文件。向管道(共享文件)提供输入的发送进程（即写进程)，以字符流形式将大量的数据送入(写）管道;而接收管道输出的接收进程（即读进程）则从管道中接收（读）数据。为了协调双方的通信，管道机制必须提供以下三方面的协调能力:互斥、同步和确定对方的存在。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210808132250.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210808132250.png';" /></details>

<p>下面以Linux中的管道为例进行说明。在Linux 中，管道是一种使用非常频繁的通信机制。从本质上说，管道也是一种文件，但它又和一般的文件有所不同，管道可以克服使用文件进行通信的两个问题，具体表现如下:</p>
<p>1）限制管道的大小。实际上，管道是一个固定大小的缓冲区。在 Linux中，该缓冲区的大小为4KB，这使得它的大小不像文件那样不加检验地增长。使用单个固定缓冲区也会带来问题，比如在写管道时可能变满，这种情况发生时，随后对管道的 write()调用将默认地被阻塞，等待某些数据被读取，以便腾出足够的空间供 write()调用写。</p>
<p>2）读进程也可能工作得比写进程快。当所有当前进程数据已被读取时，管道变空。当这种情况发生时，一个随后的read()调用将默认地被阻塞,等待某些数据被写入,这解决了read()调用返回文件结束的问题。</p>
<blockquote>
<p>注意:从管道读数据是一次性操作，数据一旦被读取，它就从管道中被抛弃，释放空间以便写更多的数据。管道只能采用半双工通信，即某一时刻只能单向传输。要实现父子进程双方互动通信，需要定义两个管道。</p>
</blockquote>
<p>管道可以理解为共享存储的优化和发展,因为在共享存储中，若某进程要访问共享存储空间则必须没有其他进程在该共享存储空间中进行写操作，否则访问行为就会被阻塞。而管道通信中存储空间进化成了缓冲区，缓冲区只允许一边写入、另一边读出，因此只要缓冲区中有数据，进程就能从缓冲区中读出，而不必担心会因为其他进程在其中进行写操作而遭到阻塞，因为写进程会先把缓冲区写满，然后才让读进程读，当缓冲区中还有数据时，写进程不会往缓冲区写数据。当然，这也决定了管道通信必然是半双工通信。</p>
<h4 id="线程概念和多线程模型"><a href="#线程概念和多线程模型" class="headerlink" title="线程概念和多线程模型"></a>线程概念和多线程模型</h4><h5 id="线程的基本概念"><a href="#线程的基本概念" class="headerlink" title="线程的基本概念"></a>线程的基本概念</h5><p>引入进程的目的是更好地使多道程序并发执行，提高资源利用率和系统吞吐量;而引入线程的目的则是减小程序在并发执行时所付出的时空开销，提高操作系统的并发性能。</p>
<p>线程最直接的理解就是“轻量级进程”，它是一个基本的CPU执行单元，也是程序执行流的最小单元，由线程ID、程序计数器、寄存器集合和堆栈组成。线程是进程中的一个实体，是被系统独立调度和分派的基本单位，线程自己不拥有系统资源，只拥有一点儿在运行中必不可少的资源，但它可与同属一个进程的其他线程共享进程所拥有的全部资源。一个线程可以创建和撤销另一个线程，同一进程中的多个线程之间可以并发执行。由于线程之间的相互制约，致使线程在运行中呈现出间断性。线程也有就绪、阻塞和运行三种基本状态。</p>
<p>引入线程后，进程的内涵发生了改变，进程只作为除CPU外的系统资源的分配单元，而线程则作为处理机的分配单元。由于一个进程内部有多个线程，若线程的切换发生在同一个进程内部，则只需要很少的时空开销。</p>
<h5 id="线程与进程的比较"><a href="#线程与进程的比较" class="headerlink" title="线程与进程的比较"></a>线程与进程的比较</h5><p>1）调度。在传统的操作系统中，拥有资源和独立调度的基本单位都是进程。在引入线程的操作系统中，线程是独立调度的基本单位，进程是拥有资源的基本单位。在同一进程中,线程的切换不会引起进程切换。在不同进程中进行线程切换，如从一个进程内的线程切换到另一个进程中的线程时，会引起进程切换。</p>
<p>2）拥有资源。不论是传统操作系统还是设有线程的操作系统，进程都是拥有资源的基本单位，而线程不拥有系统资源（也有一点儿必不可少的资源)，但线程可以访问其隶属进程的系统资源。要知道，若线程也是拥有资源的单位，则切换线程就需要较大的时空开销，线程这个概念的提出就没有意义。</p>
<p>3）并发性。在引入线程的操作系统中，不仅进程之间可以并发执行，而且多个线程之间也可以并发执行，从而使操作系统具有更好的并发性，提高了系统的吞吐量。</p>
<p>4）系统开销。由于创建或撤销进程时，系统都要为之分配或回收资源，如内存空间、IO设备等，因此操作系统所付出的开销远大于创建或撤销线程时的开销。类似地，在进行进程切换时，涉及当前执行进程CPU 环境的保存及新调度到进程CPU环境的设置，而线程切换时只需保存和设置少量寄存器内容，开销很小。此外，由于同一进程内的多个线程共享进程的地址空间，因此这些线程之间的同步与通信非常容易实现，甚至无须操作系统的干预。</p>
<p>5）地址空间和其他资源（如打开的文件)。进程的地址空间之间互相独立，同一进程的各线程间共享进程的资源，某进程内的线程对于其他进程不可见。</p>
<p>6）通信方面。进程间通信（IPC）需要进程同步和互斥手段的辅助，以保证数据的一致性，而线程间可以直接读/写进程数据段（如全局变量）来进行通信。</p>
<h5 id="线程的属性"><a href="#线程的属性" class="headerlink" title="线程的属性"></a>线程的属性</h5><p>多线程操作系统把线程作为独立运行（或调度）的基本单位，此时的进程已不再是一个基本的可执行实体，但它仍具有与执行相关的状态。所谓进程处于“执行”状态，实际上是指该进程中的某线程正在执行。线程的主要属性如下:</p>
<p>1）线程是一个轻型实体，它不拥有系统资源，但每个线程都应有一个唯一的标识符和一个线程控制块，线程控制块记录了线程执行的寄存器和栈等现场状态。</p>
<p>2）不同的线程可以执行相同的程序，即同一个服务程序被不同的用户调用时，操作系统把它们创建成不同的线程。</p>
<p>3）同一进程中的各个线程共享该进程所拥有的资源。</p>
<p>4）线程是处理机的独立调度单位，多个线程是可以并发执行的。在单CPU的计算机系统中，各线程可交替地占用CPU;在多CPU的计算机系统中，各线程可同时占用不同的CPU,若各个CPU同时为一个进程内的各线程服务，则可缩短进程的处理时间。</p>
<p>5）一个线程被创建后，便开始了它的生命周期，直至终止。线程在生命周期内会经历阻塞态、就绪态和运行态等各种状态变化。</p>
<p>为什么线程的提出有利于提高系统并发性?可以这样来理解:由于有了线程，线程切换时，有可能会发生进程切换，也有可能不发生进程切换，平均而言每次切换所需的开销就变小了，因此能够让更多的线程参与并发，而不会影响到响应时间等问题。</p>
<h5 id="线程的实现方式"><a href="#线程的实现方式" class="headerlink" title="线程的实现方式"></a>线程的实现方式</h5><p>线程的实现可以分为两类:用户级线程(User-Level Thread，ULT)和内核级线程(Kernel-LevelThread，KLT)。内核级线程又称内核支持的线程。</p>
<p>在用户级线程中，有关线程管理（线程的创建、撤销和切换等）的所有工作都由应用程序完成，内核意识不到线程的存在。应用程序可以通过使用线程库设计成多线程程序。通常，应用程序从单线程开始，在该线程中开始运行，在其运行的任何时刻，可以通过调用线程库中的派生例程创建一个在相同进程中运行的新线程。图2.5(a)说明了用户级线程的实现方式。</p>
<p>在内核级线程中，线程管理的所有工作由内核完成，应用程序没有进行线程管理的代码，只有一个到内核级线程的编程接口。内核为进程及其内部的每个线程维护上下文信息，调度也在内核基于线程架构的基础上完成。图2.5(b)说明了内核级线程的实现方式。</p>
<p>有些系统中使用组合方式的多线程实现。线程创建完全在用户空间中完成，线程的调度和同步也在应用程序中进行。一个应用程序中的多个用户级线程被映射到一些(小于等于用户级线程的数目）内核级线程上。图2.5(c)说明了用户级与内核级的组合实现方式。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210808155557.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210808155557.png';" /></details>


<h5 id="多线程模型"><a href="#多线程模型" class="headerlink" title="多线程模型"></a>多线程模型</h5><p>有些系统同时支持用户线程和内核线程，由此产生了不同的多线程模型，即实现用户级线程和内核级线程的连接方式。</p>
<p>1）多对一模型。将多个用户级线程映射到一个内核级线程，线程管理在用户空间完成。此模式中，用户级线程对操作系统不可见（即透明)。</p>
<p>优点:线程管理是在用户空间进行的，因而效率比较高。</p>
<p>缺点:一个线程在使用内核服务时被阻塞，整个进程都会被阻塞;多个线程不能并行地运行在多处理机上。</p>
<p>2）一对一模型。将每个用户级线程映射到一个内核级线程。</p>
<p>优点:当一个线程被阻塞后，允许另一个线程继续执行，所以并发能力较强。</p>
<p>缺点:每创建一个用户级线程都需要创建一个内核级线程与其对应，这样创建线程的开销比较大，会影响到应用程序的性能。</p>
<p>3）多对多模型。将n个用户级线程映射到m个内核级线程上，要求m≤n。</p>
<p>特点:多对多模型是多对一模型和一对一模型的折中，既克服了多对一模型并发度不高的缺点，又克服了一对一模型的一个用户进程占用太多内核级线程而开销太大的缺点。</p>
<p>此外，还拥有多对一模型和一对一模型各自的优点，可谓集两者之所长。</p>
<h4 id="本节小结"><a href="#本节小结" class="headerlink" title="本节小结"></a>本节小结</h4><h5 id="为什么要引入进程"><a href="#为什么要引入进程" class="headerlink" title="为什么要引入进程?"></a>为什么要引入进程?</h5><p>在多道程序同时运行的背景下，进程之间需要共享系统资源，因此会导致各程序在执行过程中出现相互制约的关系，程序的执行会表现出间断性的特征。这些特征都是在程序的执行过程中发生的，是动态的过程，而传统的程序本身是一组指令的集合，是一个静态的概念，无法描述程序在内存中的执行情况，即我们无法从程序的字面上看出它何时执行、何时停顿，也无法看出它与其他执行程序的关系，因此，程序这个静态概念已不能如实反映程序并发执行过程的特征。为了深刻描述程序动态执行过程的性质乃至更好地支持和管理多道程序的并发执行，人们引入了进程的概念。</p>
<h5 id="什么是进程-进程由什么组成"><a href="#什么是进程-进程由什么组成" class="headerlink" title="什么是进程?进程由什么组成?"></a>什么是进程?进程由什么组成?</h5><p>进程是一个具有独立功能的程序关于某个数据集合的一次运行活动。它可以申请和拥有系统资源，是一个动态的概念，是一个活动的实体。它不只是程序的代码本身，还包括当前的活动，通过程序计数器的值和处理寄存器的内容来表示。</p>
<p>一个进程实体由程序段、相关数据段和 PCB三部分构成，其中 PCB是标志一个进程存在的唯一标识，程序段是进程运行的程序的代码，数据段则存储程序运行过程中相关的一些数据。</p>
<h5 id="进程是如何解决问题的"><a href="#进程是如何解决问题的" class="headerlink" title="进程是如何解决问题的?"></a>进程是如何解决问题的?</h5><p>进程把能够识别程序运行态的一些变量存放在 PCB中，通过这些变量系统能够更好地了解进程的状况，并在适当时进行进程的切换，以避免一些资源的浪费，甚至划分为更小的调度单位一线程来提高系统的并发度。</p>
<p>本节主要介绍什么是进程，并围绕这个问题进行一些阐述和讨论，为下一节讨论的内容做铺垫，但之前未学过相关课程的读者可能会比较费解，到现在为止对进程这个概念还未形成比较清晰的认识。接下来，我们再用一个比较熟悉的概念来类比进程，以便大家能彻底理解本节的内容到底在讲什么，到底解决了什么问题。</p>
<p>我们用“人的生命历程”来类比进程。首先，人的生命历程一定是一个动态的、过程性的概念，要研究人的生命历程，先要介绍经历这个历程的主体是什么。主体当然是人，相当于经历进程的主体是进程映像，人有自己的身份，相当于进程映像里有PCB;人生历程会经历好几种状态:出生的时候、弥留的时候、充满斗志的时候、发奋图强的时候及失落的时候，相当于进程有创建、撤销、就绪、运行、阻塞等状态，这几种状态会发生改变，人会充满斗志而转向发奋图强，发奋图强获得进步之后又会充满斗志预备下一次发奋图强，或者发奋图强后遇到阻碍会进入失落状态，然后在别人的开导之下又重新充满斗志。类比进程，会由就绪态转向运行态，运行态转向就绪态，或者运行态转向阻塞态，然后在别的进程帮助下返回就绪态。</p>
<p>若我们用“人生历程”这个过程的概念去类比进程，则对进程的理解就会更深一层。前面生活化的例子可以帮我们理解进程的实质，但它毕竟有不严谨的地方。一种较好的方式是，在类比进程和“人生历程”后，再看一遍前面较为严谨的书面阐述和讨论，这样对知识的掌握会更加准确而全面。</p>
<p>这里再给出一些学习计算机科学知识的建议。学习科学知识时，很多同学会陷入一个误区，即只注重对定理、公式的应用，而忽视对基础概念的理解。这是我们从小到大为了应付考试而培养出的一个毛病，因为熟练应用公式和定理对考试有立竿见影的效果。公式、定理的应用固然重要，但基础概念的理解能让我们透彻地理解一门学科，更利于我们产生兴趣，培养创造性思维。</p>
<h3 id="处理机调度"><a href="#处理机调度" class="headerlink" title="处理机调度"></a>处理机调度</h3><p>在学习本节时，请读者思考以下问题:</p>
<p>1）为什么要进行处理机调度?</p>
<p>2）调度算法有哪几种?结合第1章学习的分时操作系统和实时操作系统，思考哪种调度算法比较适合这两种操作系统。</p>
<p>希望读者能够在学习调度算法前，先自己思考一些调度算法，在学习的过程中注意把自己的想法与这些经典的算法进行比对，并学会计算一些调度算法的周转时间。</p>
<h4 id="调度的概念"><a href="#调度的概念" class="headerlink" title="调度的概念"></a>调度的概念</h4><h5 id="调度的基本概念"><a href="#调度的基本概念" class="headerlink" title="调度的基本概念"></a>调度的基本概念</h5><p>在多道程序系统中，进程的数量往往多于处理机的个数，因此进程争用处理机的情况在所难免。处理机调度是对处理机进行分配，即从就绪队列中按照一定的算法（公平、高效）选择一个进程并将处理机分配给它运行，以实现进程并发地执行。</p>
<p>处理机调度是多道程序操作系统的基础，是操作系统设计的核心问题。</p>
<h5 id="调度的层次"><a href="#调度的层次" class="headerlink" title="调度的层次"></a>调度的层次</h5><p>一个作业从提交开始直到完成，往往要经历以下三级调度，如图2.6所示。</p>
<p>1） $\color{green}{\text{作业调度}}$ 。又称高级调度，其主要任务是按一定的原则从外存上处于后备状态的作业中挑选一个(或多个）作业，给它（们）分配内存、输入/输出设备等必要的资源，并建立相应的进程，以使它（们）获得竞争处理机的权利。简言之，作业调度就是内存与辅存之间的调度。对于每个作业只调入一次、调出一次。<br>多道批处理系统中大多配有作业调度，而其他系统中通常不需要配置作业调度。作业调度的执行频率较低，通常为几分钟一次。</p>
<p>2） $\color{green}{\text{中级调度}}$ 。又称内存调度，其作用是提高内存利用率和系统吞吐量。为此，应将那些暂时不能运行的进程调至外存等待，把此时的进程状态称为挂起态。当它们已具备运行条件且内存又稍有空闲时，由中级调度来决定把外存上的那些已具备运行条件的就绪进程再重新调入内存，并修改其状态为就绪态，挂在就绪队列上等待。</p>
<p>3） $\color{green}{\text{进程调度}}$ 。又称低级调度，其主要任务是按照某种方法和策略从就绪队列中选取一个进程，将处理机分配给它。进程调度是操作系统中最基本的一种调度，在一般的操作系统中都必须配置进程调度。进程调度的频率很高，一般几十毫秒一次。</p>
<details><summary>图2.6处理机的三级调度</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210924232351.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210924232351.png';" /></details>

<h5 id="三级调度的联系"><a href="#三级调度的联系" class="headerlink" title="三级调度的联系"></a>三级调度的联系</h5><p>作业调度从外存的后备队列中选择一批作业进入内存，为它们建立进程，这些进程被送入就绪队列，进程调度从就绪队列中选出一个进程，并把其状态改为运行态，把CPU分配给它。中级调度是为了提高内存的利用率，系统将那些暂时不能运行的进程挂起来。当内存空间宽松时，通过中级调度选择具备运行条件的进程，将其唤醒。</p>
<p>1）作业调度为进程活动做准备，进程调度使进程正常活动起来，中级调度将暂时不能运行的进程挂起，中级调度处于作业调度和进程调度之间。</p>
<p>2）作业调度次数少，中级调度次数略多，进程调度频率最高。</p>
<p>3）进程调度是最基本的，不可或缺。</p>
<h4 id="调度的时机、切换与过程"><a href="#调度的时机、切换与过程" class="headerlink" title="调度的时机、切换与过程"></a>调度的时机、切换与过程</h4><p>进程调度和切换程序是操作系统内核程序。请求调度的事件发生后，才可能运行进程调度程序，调度了新的就绪进程后，才会进行进程间的切换。理论上这三件事情应该顺序执行，但在实际设计中，操作系统内核程序运行时，若某时发生了引起进程调度的因素，则不一定能够马上进行调度与切换。</p>
<p>现代操作系统中，不能进行进程的调度与切换的情况有以下几种:</p>
<p>1）在 $\color{green}{\text{处理中断}}$ 的过程中。中断处理过程复杂，在实现上很难做到进程切换，而且中断处理是系统工作的一部分，逻辑上不属于某一进程，不应被剥夺处理机资源。</p>
<p>2）进程在操作系统 $\color{red}{\text{内核程序}}$  $\color{green}{\text{临界区}}$ 中。进入临界区后，需要独占式地访问共享数据，理论上必须加锁，以防止其他并行程序进入，在解锁前不应切换到其他进程运行，以加快该共享数据的释放。</p>
<p>3）其他需要完全屏蔽中断的 $\color{green}{\text{原子操作}}$ 过程中。如加锁、解锁、中断现场保护、恢复等原子操作。在原子过程中，连中断都要屏蔽，更不应该进行进程调度与切换。</p>
<p>若在上述过程中发生了引起调度的条件，则不能马上进行调度和切换，应置系统的请求调度标志，直到上述过程结束后才进行相应的调度与切换。</p>
<p>应该进行进程调度与切换的情况如下:</p>
<p>1）发生引起调度条件且当前进程无法继续运行下去时，可以马上进行调度与切换。若操作系统只在这种情况下进行进程调度，则是 $\color{green}{\text{非剥夺调度}}$ 。</p>
<p>2）中断处理结束或自陷处理结束后，返回被中断进程的用户态程序执行现场前，若置上请求调度标志，即可马上进行进程调度与切换。若操作系统支持这种情况下的运行调度程序，则实现了 $\color{green}{\text{剥夺方式}}$ 的调度。</p>
<p>进程切换往往在调度完成后立刻发生，它要求保存原进程当前切换点的现场信息，恢复被调度进程的现场信息。现场切换时，操作系统内核将原进程的现场信息推入当前进程的内核堆栈来保存它们，并更新堆栈指针。内核完成从新进程的内核栈中装入新进程的现场信息、更新当前运行进程空间指针、重设PC寄存器等相关工作之后，开始运行新的进程。</p>
<h4 id="进程调度方式"><a href="#进程调度方式" class="headerlink" title="进程调度方式"></a>进程调度方式</h4><p>所谓进程调度方式，是指当某个进程正在处理机上执行时，若有某个更为重要或紧迫的进程需要处理，即有优先权更高的进程进入就绪队列，此时应如何分配处理机。</p>
<p>通常有以下两种进程调度方式:</p>
<p>1）非剥夺调度方式，又称非抢占方式。非剥夺调度方式是指当一个进程正在处理机上执行时，即使有某个更为重要或紧迫的进程进入就绪队列，仍然让正在执行的进程继续执行直到该进程完成或发生某种事件而进入阻塞态时，才把处理机分配给更为重要或紧迫的进程。</p>
<p>在非剥夺调度方式下，一旦把CPU分配给一个进程，该进程就会保持CPU直到终止或转换到等待态。这种方式的优点是实现简单、系统开销小，适用于大多数的批处理系统但它不能用于分时系统和大多数的实时系统。</p>
<p>2）剥夺调度方式，又称抢占方式。剥夺调度方式是指当一个进程正在处理机上执行时，若有某个更为重要或紧迫的进程需要使用处理机，则立即暂停正在执行的进程，将处理机分配给这个更为重要或紧迫的进程。</p>
<p>采用剥夺式的调度，对提高系统吞吐率和响应效率都有明显的好处。但“剥夺”不是一种任意性行为，必须遵循一定的原则，主要有优先权、短进程优先和时间片原则等。</p>
<h4 id="调度的基本准则"><a href="#调度的基本准则" class="headerlink" title="调度的基本准则"></a>调度的基本准则</h4><p>不同的调度算法具有不同的特性，在选择调度算法时，必须考虑算法的特性。为了比较处理机调度算法的性能，人们提出了很多评价准则，下面介绍其中主要的几种:</p>
<p>1） $\color{green}{\text{CPU利用率}}$ 。CPU是计算机系统中最重要和昂贵的资源之一，所以应尽可能使CPU保持“忙”状态，使这一资源利用率最高。</p>
<p>2） $\color{green}{\text{系统吞吐量}}$ 。表示单位时间内CPU完成作业的数量。长作业需要消耗较长的处理机时间,因此会降低系统的吞吐量。而对于短作业，它们所需要消耗的处理机时间较短，因此能提高系统的吞吐量。调度算法和方式的不同，也会对系统的吞吐量产生较大的影响。</p>
<p>3） $\color{green}{\text{周转时间}}$ 。周转时间是指从作业提交到作业完成所经历的时间，是作业等待、在就绪队列中排队、在处理机上运行及进行输入/输出操作所花费时间的总和。</p>
<p>作业的周转时间可用公式表示如下:</p>
<p style='text-align:center'>周转时间=作业完成时间-作业提交时间</p>

<p>平均周转时间是指多个作业周转时间的平均值:</p>
<p style='text-align:center'>平均周转时间=（作业1的周转时间+…+作业 $n$ 的周转时间）/ $n$ </p>

<p> $\color{red}{\text{带权周转时间}}$ 是指 $\color{green}{\text{作业周转时间}}$ 与 $\color{green}{\text{作业实际运行时间}}$ 的比值:</p>
<p style='text-align:center'> $\text{带权周转时间}=\dfrac{\text{作业周转时间}}{作业实际运行时间}$ </p>

<p> $\color{red}{\text{平均带权周转时间}}$ 是指多个作业带权周转时间的平均值:</p>
<p style='text-align:center'>平均带权周转时间=（作业1的带权周转时间+..+作业 $n$ 的带权周转时间）/ $n$ </p>

<p>4）等待时间。等待时间指进程处于等处理机状态的时间之和，等待时间越长，用户满意度越低。处理机调度算法实际上并不影响作业执行或输入/输出操作的时间，只影响作业在就绪队列中等待所花的时间。因此，衡量一个调度算法的优劣，常常只需简单地考察等待时间。</p>
<p>5）响应时间。响应时间指从用户提交请求到系统首次产生响应所用的时间。在交互式系统中，周转时间不可能是最好的评价准则，一般采用响应时间作为衡量调度算法的重要准则之一。从用户角度来看，调度策略应尽量降低响应时间，使响应时间处在用户能接受的范围之内。</p>
<p>要想得到一个满足所有用户和系统要求的算法几乎是不可能的。设计调度程序，一方面要满足特定系统用户的要求（如某些实时和交互进程的快速响应要求)，另一方面要考虑系统整体效率（如减少整个系统的进程平均周转时间)，同时还要考虑调度算法的开销。</p>
<h4 id="典型的调度算法"><a href="#典型的调度算法" class="headerlink" title="典型的调度算法"></a>典型的调度算法</h4><h5 id="先来先服务-FCFS-调度算法"><a href="#先来先服务-FCFS-调度算法" class="headerlink" title="先来先服务(FCFS)调度算法"></a>先来先服务(FCFS)调度算法</h5><p>FCFS 调度算法是一种最简单的调度算法，它既可用于作业调度，又可用于进程调度。在作业调度中，算法每次从后备作业队列中选择最先进入该队列的一个或几个作业，将它们调入内存，分配必要的资源，创建进程并放入就绪队列。</p>
<p>在进程调度中，FCFS 调度算法每次从就绪队列中选择最先进入该队列的进程，将处理机分配给它，使之投入运行，直到完成或因某种原因而阻塞时才释放处理机。</p>
<p>下面通过一个实例来说明FCFS调度算法的性能。假设系统中有4个作业，它们的提交时间分别是8,8.4,8.8,9，运行时间依次是2,1,0.5,0.2，系统采用FCFS调度算法，这组作业的平均等待时间、平均周转时间和平均带权周转时间见表2.2。</p>
<details><summary>表2.2 FCFS调度算法的性能</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210924234106.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210924234106.png';" /></details>

<p>FCFS 调度算法属于不可剥夺算法。从表面上看，它对所有作业都是公平的，但若一个长作业先到达系统，就会使后面的许多短作业等待很长时间，因此它不能作为分时系统和实时系统的主要调度策略。但它常被结合在其他调度策略中使用。例如，在使用优先级作为调度策略的系统中，往往对多个具有相同优先级的进程按FCFS原则处理。</p>
<p>FCFS 调度算法的特点是算法简单，但效率低;对长作业比较有利，但对短作业不利（相对SJF和高响应比);有利于CPU繁忙型作业，而不利于I/O繁忙型作业。</p>
<h5 id="短作业优先-SJF-调度算法"><a href="#短作业优先-SJF-调度算法" class="headerlink" title="短作业优先(SJF)调度算法"></a>短作业优先(SJF)调度算法</h5><p>短作业（进程）优先调度算法是指对短作业（进程）优先调度的算法。短作业优先（SJF)调度算法从后备队列中选择一个或若干估计运行时间最短的作业，将它们调入内存运行;短进程优先(SPF）调度算法从就绪队列中选择一个估计运行时间最短的进程，将处理机分配给它，使之立即执行，直到完成或发生某事件而阻塞时，才释放处理机。</p>
<p>例如，考虑表2.2中给出的一组作业，若系统采用短作业优先调度算法，其平均等待时间、平均周转时间和平均带权周转时间见表2.3。</p>
<details><summary>表2.3SJF调度算法的性能</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210924234230.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210924234230.png';" /></details>

<p>SJF调度算法也存在不容忽视的缺点:</p>
<p>1）该算法对长作业不利，由表2.2和表2.3可知，SJF调度算法中长作业的周转时间会增加更严重的是，若有一长作业进入系统的后备队列，由于调度程序总是优先调度那些（E使是后进来的）短作业，将导致长作业长期不被调度（“饥饿”现象，注意区分“死锁”后者是系统环形等待，前者是调度策略问题)。</p>
<p>2）该算法完全未考虑作业的紧迫程度，因而不能保证紧迫性作业会被及时处理。</p>
<p>3）由于作业的长短只是根据用户所提供的估计执行时间而定的，而用户又可能会有意或无意地缩短其作业的估计运行时间，致使该算法不一定能真正做到短作业优先调度。</p>
<blockquote>
<p>注意，SJF调度算法的平均等待时间、平均周转时间最少。</p>
</blockquote>
<h5 id="优先级调度算法"><a href="#优先级调度算法" class="headerlink" title="优先级调度算法"></a>优先级调度算法</h5><p>优先级调度算法又称优先权调度算法，它既可用于作业调度，又可用于进程调度。该算法中的优先级用于描述作业运行的紧迫程度。</p>
<p>在作业调度中，优先级调度算法每次从后备作业队列中选择优先级最高的一个或几个作业，将它们调入内存，分配必要的资源，创建进程并放入就绪队列。在进程调度中，优先级调度算法每次从就绪队列中选择优先级最高的进程，将处理机分配给它，使之投入运行。</p>
<p>根据新的更高优先级进程能否抢占正在执行的进程，可将该调度算法分为如下两种:</p>
<p>1） $\color{green}{\text{非剥夺式优先级调度算法}}$ 。当一个进程正在处理机上运行时，即使有某个更为重要或紧迫的进程进入就绪队列，仍然让正在运行的进程继续运行，直到由于其自身的原因而主动让出处理机时（任务完成或等待事件)，才把处理机分配给更为重要或紧迫的进程。</p>
<p>2） $\color{green}{\text{剥夺式优先级调度算法}}$ 。当一个进程正在处理机上运行时，若有某个更为重要或紧迫的进程进入就绪队列，则立即暂停正在运行的进程，将处理机分配给更重要或紧迫的进程。</p>
<p>而根据进程创建后其优先级是否可以改变，可以将进程优先级分为以下两种:</p>
<p>1） $\color{green}{\text{静态优先级}}$ 。优先级是在创建进程时确定的，且在进程的整个运行期间保持不变。确定静态优先级的主要依据有进程类型、进程对资源的要求、用户要求。</p>
<p>2） $\color{green}{\text{动态优先级}}$ 。在进程运行过程中，根据进程情况的变化动态调整优先级。动态调整优先级的主要依据有进程占有CPU时间的长短、就绪进程等待CPU时间的长短。</p>
<p>一般来说，进程优先级的设置可以参照以下原则:</p>
<p>1） $\color{green}{\text{系统进程}}$ &gt; $\color{green}{\text{用户进程}}$ 。系统进程作为系统的管理者，理应拥有更高的优先级。</p>
<p>2） $\color{green}{\text{交互型进程}}$ &gt; $\color{green}{\text{非交互型进程}}$ （或前台进程&gt;后台进程)。大家平时在使用手机时，在前台运行的正在和你交互的进程应该更快速地响应你，因此自然需要被优先处理，即要有更高的优先级。</p>
<p>3） $\color{green}{\text{I/O型进程}}$ &gt; $\color{green}{\text{计算型进程}}$ 。所谓IO型进程，是指那些会频繁使用IO设备的进程，而计算型进程是那些频繁使用CPU的进程（很少使用I/O设备)。我们知道，IO设备（如打印机）的处理速度要比CPU慢得多，因此若将IO型进程的优先级设置得更高，就更有可能让IO设备尽早开始工作，进而提升系统的整体效率。</p>
<h5 id="高响应比优先调度算法"><a href="#高响应比优先调度算法" class="headerlink" title="高响应比优先调度算法"></a>高响应比优先调度算法</h5><p>高响应比优先调度算法主要用于作业调度，是对FCFS调度算法和SJF调度算法的一种综合平衡，同时考虑了每个作业的等待时间和估计的运行时间。在每次进行作业调度时，先计算后备作业队列中每个作业的响应比，从中选出响应比最高的作业投入运行。</p>
<p> $\color{red}{\text{响应比}}$ 的变化规律可描述为</p>
<p style='text-align:center'> $\text{响应比}R_p=\dfrac{\text{等待时间}+\text{要求服务时间}}{\text{要求服务时间}}$ </p>

<p>根据公式可知：<br>1）作业的等待时间相同时，要求服务时间越短，响应比越高，有利于 $\color{green}{\text{短作业}}$ 。</p>
<p>2）要求服务时间相同时，作业的响应比由其等待时间决定，等待时间越长，其响应比越高,因而它实现的是 $\color{green}{\text{先来先服务}}$ 。</p>
<p>3）对于长作业，作业的响应比可以随等待时间的增加而提高，等待时间足够长时，其响应比便可升到很高，从而也可获得处理机。因此，克服了 $\color{green}{\text{饥饿状态}}$ ，兼顾了长作业。</p>
<h5 id="时间片轮转调度算法"><a href="#时间片轮转调度算法" class="headerlink" title="时间片轮转调度算法"></a>时间片轮转调度算法</h5><p>时间片轮转调度算法主要适用于分时系统。在这种算法中，系统将所有就绪进程按到达时间的先后次序排成一个队列，进程调度程序总是选择就绪队列中的第一个进程执行，即先来先服务的原则，但仅能运行一个时间片，如100ms。在使用完一个时间片后，即使进程并未完成其运行，它也必须释放出（被剥夺）处理机给下一个就绪的进程，而被剥夺的进程返回到就绪队列的末尾重新排队，等候再次运行。</p>
<p>在时间片轮转调度算法中，时间片的大小对系统性能的影响很大。若时间片足够大，以至于所有进程都能在一个时间片内执行完毕，则时间片轮转调度算法就退化为先来先服务调度算法。若时间片很小，则处理机将在进程间过于频繁地切换，使处理机的开销增大，而真正用于运行用户进程的时间将减少。因此，时间片的大小应选择适当。</p>
<p>时间片的长短通常由以下因素确定:系统的响应时间、就绪队列中的进程数目和系统的处理能力。</p>
<h5 id="多级反馈队列调度算法（融合了前几种算法的优点）"><a href="#多级反馈队列调度算法（融合了前几种算法的优点）" class="headerlink" title="多级反馈队列调度算法（融合了前几种算法的优点）"></a>多级反馈队列调度算法（融合了前几种算法的优点）</h5><p>多级反馈队列调度算法是时间片轮转调度算法和优先级调度算法的综合与发展，如图2.7所示。通过动态调整进程优先级和时间片大小,多级反馈队列调度算法可以兼顾多方面的系统目标。例如，为提高系统吞吐量和缩短平均周转时间而照顾短进程;为获得较好的IO设备利用率和缩短响应时间而照顾IO型进程;同时，也不必事先估计进程的执行时间。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210924234833.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210924234833.png';" /></details>

<p>多级反馈队列调度算法的实现思想如下:</p>
<p>1）设置多个就绪队列，并为各个队列赋予不同的优先级，第1级队列的优先级最高，第2级队列次之，其余队列的优先级逐次降低。</p>
<p>2）赋予各个队列中进程执行时间片的大小各不相同。在优先级越高的队列中，每个进程的运行时间片越小。例如，第2级队列的时间片要比第1级队列的时间片长1倍……第 $i$ +1级队列的时间片要比第 $i$ 级队列的时间片长1倍。</p>
<p>3）一个新进程进入内存后，首先将它放入第1级队列的末尾，按FCFS原则排队等待调度当轮到该进程执行时，如它能在该时间片内完成，便可准备撤离系统;若它在一个时间片结束时尚未完成，调度程序便将该进程转入第2级队列的末尾，再同样按FCFS原则等待调度执行;若它在第2级队列中运行一个时间片后仍未完成，再以同样的方法放入第3级队列……如此下去，当一个长进程从第1级队列依次降到第 $n$ 级队列后，在第 $n$ 级队列中便采用时间片轮转的方式运行。</p>
<p>4）仅当第1级队列为空时，调度程序才调度2级队列中的进程运行;仅当第1~( $i$ -1)级队列均为空时，才会调度第 $i$ 级队列中的进程运行。若处理机正在执行第 $i$ 级队列中的某进程，这时又有新进程进入优先级较高的队列〔第1～( $i$ -1)中的任何一个队列]，则此时新进程将抢占正在运行进程的处理机,即由调度程序把正在运行的进程放回第i级队列的末尾，把处理机分配给新到的更高优先级的进程。</p>
<p>多级反馈队列的优势有以下几点:</p>
<p>1）终端型作业用户:短作业优先。</p>
<p>2）短批处理作业用户:周转时间较短。</p>
<p>3）长批处理作业用户:经过前面几个队列得到部分执行，不会长期得不到处理。</p>
<h4 id="本章小结"><a href="#本章小结" class="headerlink" title="本章小结"></a>本章小结</h4><p>本节开头提出的问题的参考答案如下。</p>
<h5 id="为什么要进行处理机调度"><a href="#为什么要进行处理机调度" class="headerlink" title="为什么要进行处理机调度?"></a>为什么要进行处理机调度?</h5><p>若没有处理机调度，同意味着要等到当前运行的进程执行完毕后，下一个进程才能执行，而实际情况中，进程时常需要等待一些外部设备的输入，而外部设备的速度与处理机相比是非常缓慢的，若让处理机总是等待外部设备，则对处理机的资源是极大的浪费。而引进处理机调度后，可在运行进程等待外部设备时，把处理机调度给其他进程，从而提高处理机的利用率。用一句简单的话说，就是为了合理地处理计算机的软/硬件资源。</p>
<h5 id="调度算法有哪几种-结合第1章学习的分时操作系统和实时操作系统，思考有没有哪种调度算法比较适合这两种操作系统。"><a href="#调度算法有哪几种-结合第1章学习的分时操作系统和实时操作系统，思考有没有哪种调度算法比较适合这两种操作系统。" class="headerlink" title="调度算法有哪几种?结合第1章学习的分时操作系统和实时操作系统，思考有没有哪种调度算法比较适合这两种操作系统。"></a>调度算法有哪几种?结合第1章学习的分时操作系统和实时操作系统，思考有没有哪种调度算法比较适合这两种操作系统。</h5><p>本节介绍的调度算法有先来先服务调度算法、短作业优先调度算法、优先级调度算法、高响应比优先调度算法、时间片轮转调度算法、多级反馈队列调度算法6种。</p>
<p>先来先服务算法和短作业优先算法无法保证及时地接收和处理问题，因此尤法休证仕规疋时时间间隔内响应每个用户的需求，也同样无法达到实时操作系统的及的性而水。优兀级A度开仫按照任务的优先级进行调度，对于更紧急的任务给予更高的优先级，适合实时操作系统。</p>
<p>高响应比优先调度算法、时间片轮转调度算法、多级反馈队列调度算法都能保证每个任务在一定时间内分配到时间片，并轮流占用CPU，适合分时操作系统。</p>
<p>本节主要介绍了处理机调度的概念。操作系统主要管理处理机、内存、文件、设备几种资源，只要对资源的请求大于资源本身的数量，就会涉及调度。例如，在单处理机系统中，处理机只有一个，而请求服务的进程却有多个，所以就有处理机调度的概念出现。而出现调度的概念后，又有了一个问题，即如何调度、应该满足谁、应该让谁等待，这是调度算法所回答的问题;而应该满足谁、应该让谁等待，要遵循一定的准则，即调度的准则。调度这一概念贯穿于操作系统的始终，读者在接下来的学习中，将接触到几种资源的调度问题和相应的调度算法。将它们与处理机调度的内容相对比，将会发现它们有异曲同工之妙。</p>
<h3 id="进程同步"><a href="#进程同步" class="headerlink" title="进程同步"></a>进程同步</h3><p>在学习本节时，请读者思考以下问题:</p>
<p>1）为什么要引入进程同步的概念?</p>
<p>2）不同的进程之间会存在什么关系?</p>
<p>3）当单纯用本节介绍的方法解决这些问题时会遇到什么新的问题吗?</p>
<p>用PV操作解决进程之间的同步互斥问题是这一节的重点，考试已经多次考查过这一内容，读者务必多加练习，掌握好求解问题的方法。</p>
<ul>
<li>大题中遇到相关的题目，信号量的类型直接定义为semaphore</li>
</ul>
<h4 id="进程同步的基本概念"><a href="#进程同步的基本概念" class="headerlink" title="进程同步的基本概念"></a>进程同步的基本概念</h4><p>在多道程序环境下，进程是并发执行的，不同进程之间存在着不同的相互制约关系。为了协调进程之间的相互制约关系，引入了进程同步的概念。下面举一个简单的例于米帮大豕理解赵个概念。例如，让系统计算1+2x3，假设系统产生两个进程:一个是加法进程，一个是乘法进程。要让计算结果是正确的，一定要让加法进程发生在乘法进程之后，但实际上操作系统具有异步性，若不加以制约，加法进程发生在乘法进程之前是绝对有可能的，因此要制定一定的机制去约束加法进程，让它在乘法进程完成之后才发生，而这种机制就是本节要讨论的内容。</p>
<h5 id="临界资源"><a href="#临界资源" class="headerlink" title="临界资源"></a>临界资源</h5><p>虽然多个进程可以共享系统中的各种资源，但其中许多资源一次只能为一个进程所用，我们将一次仅允许一个进程使用的资源称为临界资源。许多物理设备都属于临界资源，如打印机等。此外，还有许多变量、数据等都可以被若干进程共享，也属于临界资源。</p>
<p>对临界资源的访问，必须互斥地进行，在每个进程中，访问临界资源的那段代码称为临界区。为了保证临界资源的正确使用，可把临界资源的访问过程分成4个部分:</p>
<p>1） $\color{green}{\text{进入区}}$ 。为了进入临界区使用临界资源，在进入区要检查可否进入临界区，若能进入临界区，则应设置正在访问临界区的标志，以阻止其他进程同时进入临界区。</p>
<p>2） $\color{green}{\text{临界区}}$ 。进程中访问临界资源的那段代码，又称临界段。</p>
<p>3） $\color{green}{\text{退出区}}$ 。将正在访问临界区的标志清除。</p>
<p>4） $\color{green}{\text{剩余区}}$ 。代码中的其余部分。</p>
<details><summary>代码详情</summary>

<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">do&#123;</span><br><span class="line">  entry section; &#x2F;&#x2F;进入区</span><br><span class="line">  critical section; &#x2F;&#x2F;临界区</span><br><span class="line">  exit section;&#x2F;&#x2F;退出区</span><br><span class="line">  remainder section;&#x2F;&#x2F;剩余区</span><br><span class="line">&#125; while (true)</span><br></pre></td></tr></table></figure>
</details>

<h5 id="同步"><a href="#同步" class="headerlink" title="同步"></a>同步</h5><p>同步亦称直接制约关系，是指为完成某种任务而建立的两个或多个进程，这些进程因为需要在某些位置上协调它们的工作次序而等待、传递信息所产生的制约关系。进程间的直接制约关系源于它们之间的相互合作。</p>
<p>例如，输入进程A通过单缓冲向进程B提供数据。当该缓冲区空时，进程B不能获得所需数据而阻塞，一旦进程A将数据送入缓冲区，进程B就被唤醒。反之，当缓冲区满时，进程A被阻塞，仅当进程B取走缓冲数据时，才唤醒进程A。</p>
<h5 id="互斥"><a href="#互斥" class="headerlink" title="互斥"></a>互斥</h5><p>互斥也称间接制约关系。当一个进程进入临界区使用临界资源时，另一个进程必须等待，当占用临界资源的进程退出临界区后，另一进程才允许去访问此临界资源。</p>
<p>例如，在仅有一台打印机的系统中，有两个进程A和进程B，若进程A需要打印时，系统已将打印机分配给进程B，则进程A必须阻塞。一旦进程B将打印机释放，系统便将进程唤醒，并将其由阻塞态变为就绪态。</p>
<p>为禁止两个进程同时进入临界区，同步机制应遵循以下准则:</p>
<p>1） $\color{green}{\text{空闲让进}}$ 。临界区空闲时，可以允许一个请求进入临界区的进程立即进入临界区。</p>
<p>2） $\color{green}{\text{忙则等待}}$ 。当已有进程进入临界区时，其他试图进入临界区的进程必须等待。</p>
<p>3） $\color{green}{\text{有限等待}}$ 。对请求访问的进程，应保证能在有限时间内进入临界区。</p>
<p>4） $\color{green}{\text{让权等待}}$ 。当进程不能进入临界区时，应立即释放处理器，防止进程忙等待。</p>
<h4 id="实现临界区互斥的基本方法"><a href="#实现临界区互斥的基本方法" class="headerlink" title="实现临界区互斥的基本方法"></a>实现临界区互斥的基本方法</h4><h5 id="软件实现方法"><a href="#软件实现方法" class="headerlink" title="软件实现方法"></a>软件实现方法</h5><p>在进入区设置并检查一些标志来标明是否有进程在临界区中，若已有进程在临界区，则在进入区通过循环检查进行等待，进程离开临界区后则在退出区修改标志。</p>
<p>1）算法一:单标志法。该算法设置一个公用整型变量turn，用于指示被允许进入临界区的进程编号，即若turn=0，则允许 $P_0$ 进程进入临界区。该算法可确保每次只允许一个进程进入临界区。但两个进程必须交替进入临界区，若某个进程不再进入临界区，则另一个进程也将无法进入临界区（违背“空闲让进”)。这样很容易造成资源利用不充分。若 $P_0$ 顺利进入临界区并从临界区离开，则此时临界区是空闲的，但 $P_1$ 并没有进入临界区的打算，turn=1一直成立， $P_0$ 就无法再次进入临界区（一直被while死循环困住)。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925000015.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925000015.png';" /></details>

<p>2）算法二:双标志法先检查。该算法的基本思想是在每个进程访问临界区资源之前，先查看临界资源是否正被访问，若正被访问，该进程需等待;否则，进程才进入自己的临界区。为此，设置一个数据flag[ $i$ ]，如第i个元素值为FALSE，表示 $P_i$ 进程未进入临界区，值为TRUE，表示P进程进入临界区。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925000115.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925000115.png';" /></details>

<p>优点:不用交替进入，可连续使用;缺点: $P_i$ 和 $P_j$ ,可能同时进入临界区。按序列①② ${\textstyle\unicode{x2462}}$ ${\textstyle\unicode{x2463}}$  执行时，会同时进入临界区（违背“忙则等待”)。即在检查对方的 flag 后和切换自己的flag前有一段时间，结果都检查通过。这里的问题出在检查和修改操作不能一次进行。</p>
<p>3）算法三:双标志法后检查。算法二先检测对方的进程状态标志，再置自己的标志，由在检测和放置中可插入另一个进程到达时的检测操作，会造成两个进程在分别检测后同时进入临界区。为此，算法三先将自己的标志设置为 TRUE，再检测对方的状态标志，若对方标志为TRUE，则进程等待;否则进入临界区。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925000242.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925000242.png';" /></details>

<p>两个进程几乎同时都想进入临界区时，它们分别将自己的标志值 flag 设置为 TRUE，并且同时检测对方的状态（执行while语句)，发现对方也要进入临界区时，双方互相谦让，结果谁也进不了临界区，从而导致“饥饿”现象。</p>
<p>4）算法四:Peterson’s Algorithm。为了防止两个进程为进入临界区而无限期等待，又设置变量 turn，每个进程在先设置自己的标志后再设置turn标志。这时，再同时检测另一个进程状态标志和不允许进入标志，以便保证两个进程同时要求进入临界区时，只允许个进程进入临界区。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925000331.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925000331.png';" /></details>

<p>具体如下:考虑进程 $P_i$ ，一旦设置flag[ $i$ ] = true，就表示它想要进入临界区，同时turn =j,此时若进程 $P_j$ 已在临界区中，符合进程 $P_i$ 中的while循环条件，则 $P_i$ 不能进入临界区。若 $P_j$ 不想要进入临界区，即 flag[ $j$ ] = false，循环条件不符合，则 $P_i$ 可以顺利进入，反之亦然。本算法的基本思想是算法一和算法三的结合。利用flag解决临界资源的互斥访问，而利用turn解决“饥饿”现象。</p>
<p>理解Peterson’s Algorithm的最好方法就是手动模拟。</p>
<ul>
<li>$\color{red}{\text{Q：}}$ 如果一个进程执行一行语句，就转到另一个进程里面，不就会执行出错吗（<a target="_blank" rel="noopener" href="https://stackoverflow.com/questions/37428435/difficulty-in-understanding-petersons-algorithm">一些讨论，虽然和我的问题不太一样</a>）</li>
</ul>
<h5 id="硬件实现方法"><a href="#硬件实现方法" class="headerlink" title="硬件实现方法"></a>硬件实现方法</h5><p>理解本节介绍的硬件实现，对学习后面的信号量很有帮助。计算机提供了特殊的硬件指令，允许对一个字中的内容进行检测和修正，或对两个字的内容进行交换等。通过硬件支持实现临界段问题的方法称为低级方法，或称元方法。</p>
<p>(1）中断屏蔽方法</p>
<p>当一个进程正在使用处理机执行它的临界区代码时，防止其他进程进入其临界区进行访问的最简方法是，禁止一切中断发生，或称之为屏蔽中断、关中断。因为CPU只在发生中断时引起进程切换，因此屏蔽中断能够保证当前运行的进程让临界区代码顺利地执行完，进而保证互斥的正确实现，然后执行开中断。其典型模式为</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925000605.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925000605.png';" /></details>

<p>这种方法限制了处理机交替执行程序的能力，因此执行的效率会明显降低。对内核来说，在它执行更新变量或列表的几条指令期间，关中断是很方便的，但将关中断的权力交给用户则很不明智，若一个进程关中断后不再开中断，则系统可能会因此终止。</p>
<p>(2）硬件指令方法</p>
<p>TestAndSet 指令:这条指令是原子操作，即执行该代码时不允许被中断。其功能是读出指定标志后把该标志设置为真。指令的功能描述如下:</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925000736.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925000736.png';" /></details>

<p>可以为每个临界资源设置一个共享布尔变量lock，表示资源的两种状态: true表示正被占用，初值为false。在进程访问临界资源之前，利用TestAndSet检查和修改标志lock;若有进程在临界区，则重复检查，直到进程退出。利用该指令实现进程互斥的算法描述如下:</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925000758.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925000758.png';" /></details>

<p>Swap指令:该指令的功能是交换两个字（字节）的内容。其功能描述如下:</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925000817.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925000817.png';" /></details>

<blockquote>
<p>注意:以上对TestAndSet和Swap指令的描述仅是功能实现，而并非软件实现的定义。事实上，它们是由硬件逻辑直接实现的，不会被中断。</p>
</blockquote>
<p>应为每个临界资源设置一个共享布尔变量lock，初值为false;在每个进程中再设置一个局部布尔变量 key，用于与lock交换信息。在进入临界区前，先利用Swap 指令交换lock 与 key的内容，然后检查key 的状态;有进程在临界区时，重复交换和检查过程，直到进程退出。利用Swap指令实现进程互斥的算法描述如下:</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925001048.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925001048.png';" /></details>

<p>硬件方法的优点:适用于任意数目的进程，而不管是单处理机还是多处理机;简单、容易验证其正确性。可以支持进程内有多个临界区，只需为每个临界区设立一个布尔变量。</p>
<p>硬件方法的缺点:进程等待进入临界区时要耗费处理机时间，不能实现让权等待。从等待进程中随机选择一个进入临界区，有的进程可能一直选不上，从而导致“饥饿”现象。</p>
<p>无论是软件实现方法还是硬件实现方法，读者只需理解它的执行过程即可，关键是软件实现方法。实际练习和考试中很少让读者写出某种软件和硬件实现方法，因此读者并不需要默写或记忆。以上的代码实现与我们平时在编译器上写的代码意义不同，以上的代码实现是为了表述进程实现同步和互斥的过程，并不是说计算机内部实现同步互斥的就是这些代码。</p>
<h4 id="信号量"><a href="#信号量" class="headerlink" title="信号量"></a>信号量</h4><p>信号量机制是一种功能较强的机制，可用来解决互斥与同步问题，它只能被两个标准的 $\color{green}{\text{原语}}$ wait(S)和 signal(S)访问，也可记为“P操作”和“V操作”。</p>
<p>原语是指完成某种功能且不被分割、不被中断执行的操作序列，通常可由硬件来实现。例如,前述的Test-and-Set和Swap指令就是由硬件实现的原子操作。原语功能的不被中断执行特性在单处理机上可由软件通过屏蔽中断方法实现。</p>
<p>原语之所以不能被中断执行，是因为原语对变量的操作过程若被打断，可能会去运行另一个对同一变量的操作过程，从而出现临界段问题。若能够找到一种解决临界段问题的元方法，就可以实现对共享变量操作的原子性。</p>
<h5 id="整型信号量"><a href="#整型信号量" class="headerlink" title="整型信号量"></a>整型信号量</h5><p>整型信号量被定义为一个用于表示资源数目的整型量S，wait和 signal操作可描述为</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925001244.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925001244.png';" /></details>

<p>wait 操作中，只要信号量S≤0，就会不断地测试。因此，该机制并未遵循“让权等待”的准则，而是使进程处于“忙等”的状态。</p>
<h5 id="记录型信号量"><a href="#记录型信号量" class="headerlink" title="记录型信号量"></a>记录型信号量</h5><p>记录型信号量是不存在“忙等”现象的进程同步机制。除需要一个用于代表资源数目的整型变量value 外，再增加一个进程链表L，用于链接所有等待该资源的进程。记录型信号量得名于采用了记录型的数据结构。记录型信号量可描述为</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925001447.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925001447.png';" /></details>

<p>相应的 wait(S)和 signal(S)的操作如下:</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925001545.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925001545.png';" /></details>

<p>wait操作，S.value–表示进程请求一个该类资源，当S.value &lt;0时，表示该类资源已分配完毕，因此进程应调用block 原语，进行自我阻塞，放弃处理机，并插入该类资源的等待队列S.L，可见该机制遵循了“让权等待”的准则。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925095037.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925095037.png';" /></details>

<p>signal操作,表示进程释放一个资源,使系统中可供分配的该类资源数增1,因此有S.value ++。若加1后仍是S.value≤0，则表示在S.L中仍有等待该资源的进程被阻塞，因此还应调用wakeup原语，将S.L中的第一个等待进程唤醒。</p>
<h5 id="利用信号量实现同步"><a href="#利用信号量实现同步" class="headerlink" title="利用信号量实现同步"></a>利用信号量实现同步</h5><p>信号量机制能用于解决进程间的各种同步问题。设S为实现进程 $P_1$ , $P_2$ 同步的公共信号量，初值为0。进程 $P_2$ 中的语句y要使用进程 $P_1$ 中语句x的运行结果，所以只有当语句x执行完成之后语句y 才可以执行。其实现进程同步的算法如下:</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925095158.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925095158.png';" /></details>

<p>若 $P_2$ 先执行到P(S)时，S为0，执行Р操作会把进程 $P_2$ 阻塞，并放入阻塞队列;当进程 $P_1$ 中的x执行完后，执行V操作，把 $P_2$ 从阻塞队列中放回就绪队列，当 $P_2$ 得到处理机时，就得以继续执行。</p>
<h5 id="利用信号量实现进程互斥"><a href="#利用信号量实现进程互斥" class="headerlink" title="利用信号量实现进程互斥"></a>利用信号量实现进程互斥</h5><p>信号量机制也能很方便地解决进程互斥问题。设S为实现进程 $P_1$ , $P_2$ 互斥的信号量，由于每次只允许一个进程进入临界区﹐所以S的初值应为1(即可用资源数为1)。只需把临界区置于P(S)和V(S)之间，即可实现两个进程对临界资源的互斥访问。其算法如下:</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925095407.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925095407.png';" /></details>

<p>当没有进程在临界区时，任意一个进程要进入临界区，就要执行Р操作，把S的值减为0，然后进入临界区;当有进程存在于临界区时，S的值为0，再有进程要进入临界区，执行Р操作时将会被阻塞，直至在临界区中的进程退出，这样便实现了临界区的互斥。</p>
<p>互斥是不同进程对同一信号量进行P,V操作实现的，一个进程成功对信号量执行了Р操作后进入临界区，并在退出临界区后，由该进程本身对该信号量执行V操作，表示当前没有进程进入临界区，可以让其他进程进入。</p>
<p>下面简单总结一下PV操作在同步互斥中的应用:在同步问题中，若某个行为要用到某种资源，则在这个行为前面Р这种资源一下;若某个行为会提供某种资源，则在这个行为后面V这种资源一下。在互斥问题中，P, V操作要紧夹使用互斥资源的那个行为，中间不能有其他冗余代码。</p>
<h5 id="利用信号量实现前驱关系"><a href="#利用信号量实现前驱关系" class="headerlink" title="利用信号量实现前驱关系"></a>利用信号量实现前驱关系</h5><details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925100154.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925100154.png';" /></details>

<p>信号量也可用来描述程序之间或语句之间的前驱关系。图2.8给出了一个前驱图，其中 $S_1$ , $S_2$ , $S_3$ ,… , $S_6$ 是最简单的程序段(只有一条语句)。为使各程序段能正确执行，应设置若干初始值为“0”的信号量。例如，为保证 $S_1$ → $S_2$ , $S_1$ → $S_3$ 的前驱关系，应分别设置信号量al, a2。同样，为保证 $S_2$ → $S_4$ , $S_2$ → $S_5$ ,  $S_5$ → $S_6$ ,  $S_4$ → $S_6$ ,  $S_5$ → $S_6$ ，应设置信号量b1, b2,c, d,e。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925100516.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925100516.png';" /></details>

<h5 id="分析进程同步和互斥问题的方法步骤"><a href="#分析进程同步和互斥问题的方法步骤" class="headerlink" title="分析进程同步和互斥问题的方法步骤"></a>分析进程同步和互斥问题的方法步骤</h5><p>1）关系分析。找出问题中的进程数，并分析它们之间的同步和互斥关系。同步、互斥、前驱关系直接按照上面例子中的经典范式改写。</p>
<p>2）整理思路。找出解决问题的关键点，并根据做过的题目找出求解的思路。根据进程的操作流程确定Р操作、V操作的大致顺序。</p>
<p>3）设置信号量。根据上面的两步，设置需要的信号量，确定初值，完善整理。</p>
<p>这是一个比较直观的同步问题，以 $S_2$ 为例，它是 $S_1$ 的后继，所以要用到 $S_1$ 的资源，在前面的简单总结中我们说过，在同步问题中，要用到某种资源，就要在行为（题中统一抽象成L)前面Р这种资源一下。 $S_2$ 是 $S_4$ ， $S_5$ 的前驱，给 $S_4$ ， $S_5$ ,提供资源，所以要在L行为后面V由 $S_4$ 和 $S_5$ 代表的资源一下。</p>
<h4 id="管程"><a href="#管程" class="headerlink" title="管程"></a>管程</h4><p>在信号量机制中，每个要访问临界资源的进程都必须自备同步的PV操作，大量分散的同步操作给系统管理带来了麻烦，且容易因同步操作不当而导致系统死锁。于是，便产生了一种新的进程同步工具-管程。管程的特性保证了进程互斥，无须程序员自己实现互斥，从而降低了死锁发生的可能性。同时管程提供了条件变量，可以让程序员灵活地实现进程同步。</p>
<h5 id="管程的定义"><a href="#管程的定义" class="headerlink" title="管程的定义"></a>管程的定义</h5><p>系统中的各种硬件资源和软件资源，均可用数据结构抽象地描述其资源特性，即用少量信息和对资源所执行的操作来表征该资源，而忽略它们的内部结构和实现细节。</p>
<p>利用共享数据结构抽象地表示系统中的共享资源，而把对该数据结构实施的操作定义为一组过程。进程对共享资源的申请、释放等操作，都通过这组过程来实现，这组过程还可以根据资源情况，或接受或阻塞进程的访问，确保每次仅有一个进程使用共享资源，这样就可以统一管理对共享资源的所有访问，实现进程互斥。这个代表共享资源的数据结构，以及由对该共享数据结构实施操作的一组过程所组成的资源管理程序，称为管程( monitor )。管程定义了一个数据结构和能为并发进程所执行(在该数据结构上)的一组操作，这组操作能同步进程和改变管程中的数据。</p>
<p>由上述定义可知，管程由4部分组成:</p>
<p>①管程的名称;<br>${\textstyle\unicode{x2461}}$  局部于管程内部的共享结构数据说明;</p>
<p>${\textstyle\unicode{x2462}}$对该数据结构进行操作的一组过程(或函数);</p>
<p>${\textstyle\unicode{x2463}}$ 对局部于管程内部的共享数据设置初始值的语句。</p>
<p>管程的定义描述举例如下:</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925101003.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925101003.png';" /></details>

<p>熟悉面向对象程序设计的读者看到管程的组成后，会立即联想到管程很像一个类(class)。</p>
<p>1）管程把对共享资源的操作封装起来，管程内的共享数据结构只能被管程内的过程所访问。一个进程只有通过调用管程内的过程才能进入管程访问共享资源。对于上例，外部进程只能通过调用take_away()过程来申请一个资源;归还资源也一样。</p>
<p>2）每次仅允许一个进程进入管程，从而实现进程互斥。若多个进程同时调用take_away(),give_back()，则只有某个进程运行完它调用的过程后，下个进程才能开始运行它调用的过程。也就是说，各个进程只能串行执行管程内的过程，这一特性保证了进程“互斥”访问共享数据结构S。</p>
<h5 id="条件变量"><a href="#条件变量" class="headerlink" title="条件变量"></a>条件变量</h5><p>当一个进程进入管程后被阻塞，直到阻塞的原因解除时，在此期间，如果该进程不释放管程，那么其他进程无法进入管程。为此，将阻塞原因定义为条件变量condition。通常，一个进程被阻塞的原因可以有多个，因此在管程中设置了多个条件变量。每个条件变量保存了一个等待队列，用于记录因该条件变量而阻塞的所有进程，对条件变量只能进行两种操作，即 wait和 signal。</p>
<p>x.wait:当x对应的条件不满足时，正在调用管程的进程调用x.wait将自己插入x条件的等待队列，并释放管程。此时其他进程可以使用该管程。</p>
<p>x.signal: x对应的条件发生了变化，则调用x.signal，唤醒一个因x条件而阻塞的进程。下面给出条件变量的定义和使用:</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925101118.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925101118.png';" /></details>

<p>条件变量和信号量的比较:</p>
<p>相似点:条件变量的wait/signal操作类似于信号量的P/V操作，可以实现进程的阻塞/唤醒。</p>
<p>不同点:条件变量是“没有值”的，仅实现了“排队等待”功能;而信号量是“有值”的，信号量的值反映了剩余资源数，而在管程中，剩余资源数用共享数据结构记录。</p>
<h4 id="经典同步问题"><a href="#经典同步问题" class="headerlink" title="经典同步问题"></a>经典同步问题</h4><h5 id="生产者-消费者问题"><a href="#生产者-消费者问题" class="headerlink" title="生产者-消费者问题"></a>生产者-消费者问题</h5><p><strong>问题描述</strong>:一组生产者进程和一组消费者进程共享一个初始为空、大小为n的缓冲区，只有缓冲区没满时，生产者才能把消息放入缓冲区，否则必须等待;只有缓冲区不空时，消费者才能从中取出消息，否则必须等待。由于缓冲区是临界资源，它只允许一个生产者放入消息，或一个消费者从中取出消息。</p>
<p><strong>问题分析</strong>:</p>
<p>1）关系分析。生产者和消费者对缓冲区互斥访问是互斥关系，同时生产者和消费者又是个相互协作的关系，只有生产者生产之后，消费者才能消费，它们也是同步关系。</p>
<p>2）整理思路。这里比较简单，只有生产者和消费者两个进程，正好是这两个进程存在着互斥关系和同步关系。那么需要解决的是互斥和同步PV操作的位置。</p>
<p>3）信号量设置。信号量mutex作为互斥信号量，用于控制互斥访问缓冲池，互斥信号量初值为1;信号量full用于记录当前缓冲池中的“满”缓冲区数，初值为0。信号量empty用于记录当前缓冲池中的“空”缓冲区数，初值为 $n$ 。</p>
<p>我们对同步互斥问题的介绍是一个循序渐进的过程。上面介绍了一个同步问题的例子和一个互斥问题的例子，下面来看生产者-消费者问题的例子是什么样的。</p>
<p>生产者-消费者进程的描述如下:</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925101330.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925101330.png';" /></details>

<p>该类问题要注意对缓冲区大小为n的处理，当缓冲区中有空时，便可对empty变量执行Р操作，一旦取走一个产品便要执行V操作以释放空闲区。对empty和 full变量的Р操作必须放在对mutex 的P操作之前。若生产者进程先执行P(mutex)，然后执行P(empty)，消费者执行P(mutex),然后执行 P(full)，这样可不可以﹖答案是否定的。设想生产者进程已将缓冲区放满，消费者进程并没有取产品，即 empty =0，当下次仍然是生产者进程运行时，它先执行P(mutex)封锁信号量,再执行P(empty)时将被阻塞，希望消费者取出产品后将其唤醒。轮到消费者进程运行时，它先执行P(mutex)，然而由于生产者进程已经封锁mutex信号量，消费者进程也会被阻塞，这样一来生产者、消费者进程都将阻塞，都指望对方唤醒自己，因此陷入了无休止的等待。同理，若消费者进程已将缓冲区取空，即 full = 0，下次若还是消费者先运行，也会出现类似的死锁。不过生产者释放信号量时，mutex, full 先释放哪一个无所谓，消费者先释放mutex 或empty都可以。</p>
<p>根据对同步互斥问题的简单总结，我们发现，其实生产者-消费者问题只是一个同步互斥问题的综合而已。</p>
<p>下面再看一个较为复杂的生产者-消费者问题。</p>
<p><strong>问题描述</strong>:桌子上有一个盘子，每次只能向其中放入一个水果。爸爸专向盘子中放苹果，妈妈专向盘子中放橘子，儿子专等吃盘子中的橘子，女儿专等吃盘子中的苹果。只有盘子为空时，爸爸或妈妈才可向盘子中放一个水果;仅当盘子中有自己需要的水果时，儿子或女儿可以从盘子中取出。</p>
<p><strong>问题分析</strong>:</p>
<p>1）关系分析。这里的关系要稍复杂一些。由每次只能向其中放入一只水果可知，爸爸和妈妈是互斥关系。爸爸和女儿、妈妈和儿子是同步关系，而且这两对进程必须连起来，儿子和女儿之间没有互斥和同步关系，因为他们是选择条件执行，不可能并发，如图2.9所示。</p>
<p>2）整理思路。这里有4个进程，实际上可抽象为两个生产者和两个消费者被连接到大小为1的缓冲区上。</p>
<p>3）信号量设置。首先将信号量plate设置互斥信号量，表示是否允许向盘子放入水果，初值为1表示允许放入，且只允许放入一个。信号量apple表示盘子中是否有苹果，初值为0表示盘子为空，不许取，apple = 1表示可以取。信号量orange表示盘子中是否有橘子，初值为0表示盘子为空，不许取，orange =1表示可以取。</p>
<details><summary>图2.9进程之间的关系</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925101528.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925101528.png';" /></details>

<p>解决该问题的代码如下:</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925101624.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925101624.png';" /></details>

<p>进程间的关系如图2.9所示。dad()和 daughter()、mom()和 son()必须连续执行，正因为如此，也只能在女儿拿走苹果后或儿子拿走橘子后才能释放盘子，即V(plate)操作。</p>
<h5 id="读者-写者问题"><a href="#读者-写者问题" class="headerlink" title="读者-写者问题"></a>读者-写者问题</h5><p><strong>问题描述</strong>:有读者和写者两组并发进程，共享一个文件，当两个或以上的读进程同时访问共享数据时不会产生副作用，但若某个写进程和其他进程（读进程或写进程）同时访问共享数据时则可能导致数据不一致的错误。因此要求:①允许多个读者可以同时对文件执行读操作;②只允许一个写者往文件中写信息;③任一写者在完成写操作之前不允许其他读者或写者工作;④写者执行写操作前，应让已有的读者和写者全部退出。</p>
<p><strong>问题分析</strong>:</p>
<p>1）关系分析。由题目分析读者和写者是互斥的，写者和写者也是互斥的，而读者和读者不存在互斥问题。</p>
<p>2）整理思路。两个进程，即读者和写者。写者是比较简单的，它和任何进程互斥，用互斥信号量的Р操作、V操作即可解决。读者的问题比较复杂，它必须在实现与写者互斥的同时，实现与其他读者的同步，因此简单的一对Р操作、V操作是无法解决问题的。这里用到了一个计数器，用它来判断当前是否有读者读文件。当有读者时，写者是无法写文件的，此时读者会一直占用文件，当没有读者时，写者才可以写文件。同时，这里不同读者对计数器的访问也应该是互斥的。</p>
<p>3）信号量设置。首先设置信号量count为计数器，用于记录当前读者的数量，初值为0;设置mutex为互斥信号量，用于保护更新count变量时的互斥;设置互斥信号量rw，用于保证读者和写者的互斥访问。</p>
<p>代码如下:</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925101839.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925101839.png';" /></details>

<p>在上面的算法中，读进程是优先的，即当存在读进程时，写操作将被延迟，且只要有一个读进程活跃，随后而来的读进程都将被允许访问文件。这样的方式会导致写进程可能长时间等待，且存在写进程“饿死”的情况。</p>
<p>若希望写进程优先，即当有读进程正在读共享文件时，有写进程请求访问，这时应禁止后续读进程的请求，等到已在共享文件的读进程执行完毕，立即让写进程执行，只有在无写进程执行的情况下才允许读进程再次运行。为此，增加一个信号量并在上面程序的 writer()和reader()函数中各增加一对PV操作，就可以得到写进程优先的解决程序。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925101951.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925101951.png';" /></details>

<p>这里的写进程优先是相对而言的，有些书上把这个算法称为读写公平法，即读写进程具有-一样的优先级。当一个写进程访问文件时，若先有一些读进程要求访问文件，后有另一个写进程要求访问文件，则当前访问文件的进程结束对文件的写操作时，会是一个读进程而不是一个写进程占用文件(在信号量w的阻塞队列上，因为读进程先来，因此排在阻塞队列队首，而V操作唤醒进程时唤醒的是队首进程)，所以说这里的写优先是相对的，想要了解如何做到真正写者优先，</p>
<p>可参考其他相关资料。</p>
<p>读者-写者问题有一个关键的特征，即有一个互斥访问的计数器count，因此遇到一个不太好解决的同步互斥问题时，要想一想用互斥访问的计数器count能否解决问题。</p>
<h5 id="哲学家进餐问题"><a href="#哲学家进餐问题" class="headerlink" title="哲学家进餐问题"></a>哲学家进餐问题</h5><details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925102039.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925102039.png';" /></details>

<p><strong>问题描述</strong>:一张圆桌边上坐着5名哲学家，每两名哲学家之间的桌上摆一根筷子，两根筷子中间是一碗米饭，如图2.10所示。哲学家们倾注毕生精力用于思考和进餐，哲学家在思考时，并不影响他人。只有当哲学家饥饿时，才试图拿起左、右两根筷子(一根一根地拿起)。若筷子已在他人手上，则需要等待。饥饿的哲学家只有同时拿到了两根筷子才可以开始进餐，进餐完毕后，放下筷子继续思考。</p>
<p><strong>问题分析</strong>:</p>
<p>1）关系分析。5名哲学家与左右邻居对其中间筷子的访问是互斥关系。</p>
<p>2）整理思路。显然，这里有5个进程。本题的关键是如何让一名哲学家拿到左右两根筷子而不造成死锁或饥饿现象。解决方法有两个:一是让他们同时拿两根筷子;二是对每名哲学家的动作制定规则，避免饥饿或死锁现象的发生。</p>
<p>3）信号量设置。定义互斥信号量数组chopstick[ 5 ]={1,1,1,1,1}，用于对5个筷子的互斥访问。哲学家按顺序编号为0～4，哲学家i左边筷子的编号为i，哲学家右边筷子的编号为(i +1)%5。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925102239.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925102239.png';" /></details>

<p>该算法存在以下问题:当5名哲学家都想要进餐并分别拿起左边的筷子时（都恰好执行完wait(chopstick[ i]);）筷子已被拿光,等到他们再想拿右边的筷子时（执行wait(chopstick[(i + 1)%5]);）就全被阻塞，因此出现了死锁。<br>为防止死锁发生，可对哲学家进程施加一些限制条件，比如至多允许4名哲学家同时进餐;仅当一名哲学家左右两边的筷子都可用时，才允许他抓起筷子;对哲学家顺序编号，要求奇数号哲学家先拿左边的筷子，然后拿右边的筷子，而偶数号哲学家刚好相反。</p>
<p>制定的正确规则如下:假设采用第二种方法，当一名哲学家左右两边的筷子都可用时，才允许他抓起筷子。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925102409.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925102409.png';" /></details>

<p>此外，还可采用AND型信号量机制来解决哲学家进餐问题，有兴趣的读者可以查阅相关资料，自行思考。</p>
<p>熟悉ACM或有过相关训练的读者都应知道贪心算法，哲学家进餐问题的思想其实与贪心算法的思想截然相反，贪心算法强调争取眼前认为最好的，而不考虑后续会有什么后果。若哲学家进餐问题用贪心算法来解决，即只要眼前有筷子能拿起就拿起的话，就会出现死锁。然而,若不仅考虑眼前的一步，而且考虑下一步，即不因为有筷子能拿起就拿起，而考虑能不能一次拿起两根筷子才做决定的话，就会避免死锁问题，这就是哲学家进餐问题的思维精髓。</p>
<p>大部分练习题和真题用消费者-生产者模型或读者-写者问题就能解决，但对于哲学家进餐问题和吸烟者问题仍然要熟悉。考研复习的关键在于反复多次和全面，“偷工减料”是要吃亏的。</p>
<h5 id="吸烟者问题"><a href="#吸烟者问题" class="headerlink" title="吸烟者问题"></a>吸烟者问题</h5><p><strong>问题描述</strong>:假设一个系统有三个抽烟者进程和一个供应者进程。每个抽烟者不停地卷烟并抽掉它，但要卷起并抽掉一支烟，抽烟者需要有三种材料:烟草、纸和胶水。三个抽烟者中,第一个拥有烟草，第二个拥有纸，第三个拥有胶水。供应者进程无限地提供三种材料，供应者每次将两种材料放到桌子上，拥有剩下那种材料的抽烟者卷一根烟并抽掉它，并给供应者一个信号告诉已完成，此时供应者就会将另外两种材料放到桌上，如此重复（让三个抽烟者轮流地抽烟）。</p>
<p><strong>问题分析</strong>:<br>1）关系分析。供应者与三个抽烟者分别是同步关系。由于供应者无法同时满足两个或以上的抽烟者，三个抽烟者对抽烟这个动作互斥（或由三个抽烟者轮流抽烟得知)。</p>
<p>2）整理思路。显然这里有4个进程。供应者作为生产者向三个抽烟者提供材料。</p>
<p>3）信号量设置。信号量offer1, offer2, offer3分别表示烟草和纸组合的资源、烟草和胶水组合的资源、纸和胶水组合的资源。信号量finish用于互斥进行抽烟动作。</p>
<p>代码如下:</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925102637.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925102637.png';" /></details>

<h4 id="本节小结-1"><a href="#本节小结-1" class="headerlink" title="本节小结"></a>本节小结</h4><p>本节开头提出的问题的参考答案如下。</p>
<h5 id="为什么要引入进程同步的概念"><a href="#为什么要引入进程同步的概念" class="headerlink" title="为什么要引入进程同步的概念?"></a>为什么要引入进程同步的概念?</h5><p>在多道程序共同执行的条件下，进程与进程是并发执行的，不同进程之间存在不同的相互制约关系。为了协调进程之间的相互制约关系，引入了进程同步的概念。</p>
<h5 id="不同的进程之间会存在什么关系"><a href="#不同的进程之间会存在什么关系" class="headerlink" title="不同的进程之间会存在什么关系?"></a>不同的进程之间会存在什么关系?</h5><p>进程之间存在同步与互斥的制约关系。</p>
<p>同步是指为完成某种任务而建立的两个或多个进程，这些进程因为需要在某些位置上协调它们的工作次序而等待、传递信息所产生的制约关系。</p>
<p>互斥是指当一个进程进入临界区使用临界资源时，另一个进程必须等待，当占用临界资源的进程退出临界区后，另一进程才允许去访问此临界资源。</p>
<h5 id="当单纯用本节介绍的方法解决这些问题时会遇到什么新的问题吗"><a href="#当单纯用本节介绍的方法解决这些问题时会遇到什么新的问题吗" class="headerlink" title="当单纯用本节介绍的方法解决这些问题时会遇到什么新的问题吗?"></a>当单纯用本节介绍的方法解决这些问题时会遇到什么新的问题吗?</h5><p>当两个或两个以上的进程在执行过程中，因占有一些资源而又需要对方的资源时，会因为争夺资源而造成一种互相等待的现象，若无外力作用，它们都将无法推进下去。这种现象称为死锁，具体介绍和解决方案请参考下一节。</p>
<h3 id="死锁"><a href="#死锁" class="headerlink" title="死锁"></a>死锁</h3><p>在学习本节时，请读者思考以下问题:</p>
<p>1）为什么会产生死锁?产生死锁有什么条件?</p>
<p>2）有什么办法可以解决死锁问题?</p>
<p>学完本节，读者应了解死锁的由来、产生条件及基本解决方法，区分死锁的避免和死锁的预防。</p>
<h4 id="死锁的概念"><a href="#死锁的概念" class="headerlink" title="死锁的概念"></a>死锁的概念</h4><h5 id="死锁的定义"><a href="#死锁的定义" class="headerlink" title="死锁的定义"></a>死锁的定义</h5><p>在多道程序系统中，由于多个进程的并发执行，改善了系统资源的利用率并提高了系统的处理能力。然而，多个进程的并发执行也带来了新的问题–死锁。所谓死锁，是指多个进程因竞争资源而造成的一种僵局（互相等待)，若无外力作用，这些进程都将无法向前推进。</p>
<p>下面通过一些实例来说明死锁现象。</p>
<p>先看生活中的一个实例。在一条河上有一座桥，桥面很窄，只能容纳一辆汽车通行。若有两辆汽车分别从桥的左右两端驶上该桥，则会出现下述冲突情况:此时，左边的汽车占有桥面左边的一段，要想过桥还需等待右边的汽车让出桥面右边的一段;右边的汽车占有桥面右边的一段，要想过桥还需等待左边的汽车让出桥面左边的一段。此时，若左右两边的汽车都只能向前行驶，则两辆汽车都无法过桥。</p>
<p>在计算机系统中也存在类似的情况。例如，某计算机系统中只有一台打印机和一台输入设备，进程 $P_1$ 正占用输入设备，同时又提出使用打印机的请求，但此时打印机正被进程 $P_2$ 所占用，而 $P_2$ 在未释放打印机之前，又提出请求使用正被 $P_1$ 占用的输入设备。这样，两个进程相互无休止地等待下去，均无法继续执行，此时两个进程陷入死锁状态。</p>
<h5 id="死锁产生的原因"><a href="#死锁产生的原因" class="headerlink" title="死锁产生的原因"></a>死锁产生的原因</h5><h6 id="系统资源的竞争"><a href="#系统资源的竞争" class="headerlink" title="系统资源的竞争"></a>系统资源的竞争</h6><p>通常系统中拥有的不可剥夺资源，其数量不足以满足多个进程运行的需要，使得进程在运行过程中，会因争夺资源而陷入僵局，如磁带机、打印机等。只有对不可剥夺资源的竞争才可能产生死锁，对可剥夺资源的竞争是不会引起死锁的。</p>
<h6 id="进程推进顺序非法"><a href="#进程推进顺序非法" class="headerlink" title="进程推进顺序非法"></a>进程推进顺序非法</h6><p>进程在运行过程中，请求和释放资源的顺序不当，也同样会导致死锁。例如，并发进程 $P_1$ , $P_2$ 分别保持了资源 $R_1$ ,  $R_2$ ，而进程 $P_1$ 申请资源 $R_2$ 、进程 $P_2$ 申请资源 $R_1$ 时，两者都会因为所需资源被占用而阻塞。</p>
<p>信号量使用不当也会造成死锁。进程间彼此相互等待对方发来的消息，也会使得这些进程间无法继续向前推进。例如，进程A等待进程B发的消息，进程B又在等待进程A发的消息，可以看出进程A和B不是因为竞争同一资源，而是在等待对方的资源导致死锁。</p>
<h6 id="死锁产生的必要条件"><a href="#死锁产生的必要条件" class="headerlink" title="死锁产生的必要条件"></a>死锁产生的必要条件</h6><p>产生死锁必须同时满足以下4个条件，只要其中任意一个条件不成立，死锁就不会发生。</p>
<p><strong>互斥条件</strong>:进程要求对所分配的资源（如打印机）进行排他性控制，即在一段时间内某资源仅为一个进程所占有。此时若有其他进程请求该资源，则请求进程只能等待。</p>
<p><strong>不剥夺条件</strong>:进程所获得的资源在未使用完之前，不能被其他进程强行夺走，即只能由获得该资源的进程自己来释放（只能是主动释放)。</p>
<p><strong>请求并保持条件</strong>:进程已经保持了至少一个资源，但又提出了新的资源请求，而该资源已被其他进程占有，此时请求进程被阻塞，但对自己已获得的资源保持不放。</p>
<p><strong>循环等待条件</strong>:存在一种进程资源的循环等待链，链中每个进程已获得的资源同时被链中下一个进程所请求。即存在一个处于等待态的进程集合{ $P_1$ , $P_2$ ,…, $P_n$ }，其中 $P_i$ 等待的资源被 $P_{i+1}$  ( $i$ =0,1,… ,  $n$ -1）占有， $P_n$ 等待的资源被 $P_0$ 占有，如图2.11所示。</p>
<p>直观上看，循环等待条件似乎和死锁的定义一样，其实不然。按死锁定义构成等待环所要求的条件更严，它要求 $P_i$ 等待的资源必须由 $P_{i+1}$ 来满足，而循环等待条件则无此限制。例如，系统中有两台输出设备， $P_0$ 占有一台， $P_K$ 占有另一台，且K不属于集合{0,1，…, n}。 $P_n$ 等待一台输出设备，它可从 $P_0$ 获得，也可能从 $P_K$ 获得。因此，虽然 $P_n$ , $P_0$ 和其他一些进程形成了循环等待圈,但 $P_K$ 不在圈内，若 $P_K$ 释放了输出设备，则可打破循环等待，如图2.12所示。因此循环等待只是死锁的必要条件。</p>
<details><summary>图2.11和图2.12</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925224155.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925224155.png';" /></details>

<p>资源分配图含圈而系统又不一定有死锁的原因是，同类资源数大于1。但若系统中每类资源都只有一个资源，则资源分配图含圈就变成了系统出现死锁的充分必要条件。</p>
<p>要注意区分不剥夺条件与请求并保持条件。下面用一个简单的例子进行说明:若你手上拿着一个苹果（即便你不打算吃)，别人不能把你手上的苹果拿走，则这就是不剥夺条件;若你左手拿着一个苹果，允许你右手再去拿一个苹果，则这就是请求并保持条件。</p>
<h4 id="死锁的处理策略"><a href="#死锁的处理策略" class="headerlink" title="死锁的处理策略"></a>死锁的处理策略</h4><p>为使系统不发生死锁，必须设法破坏产生死锁的4个必要条件之一，或允许死锁产生，但当死锁发生时能检测出死锁，并有能力实现恢复。</p>
<ul>
<li>预防和避免的区别：<a target="_blank" rel="noopener" href="https://blog.csdn.net/Aloneingchild/article/details/111339390">参考文献</a></li>
</ul>
<h5 id="死锁预防"><a href="#死锁预防" class="headerlink" title="死锁预防"></a>死锁预防</h5><p>设置某些限制条件，破坏产生死锁的4个必要条件中的一个或几个，以防止发生死锁。</p>
<h5 id="避免死锁"><a href="#避免死锁" class="headerlink" title="避免死锁"></a>避免死锁</h5><p>在资源的动态分配过程中，用某种方法防止系统进入不安全状态，从而避免死锁。</p>
<h5 id="死锁的检测及解除"><a href="#死锁的检测及解除" class="headerlink" title="死锁的检测及解除"></a>死锁的检测及解除</h5><p>无须采取任何限制性措施，允许进程在运行过程中发生死锁。通过系统的检测机构及时地检测出死锁的发生，然后采取某种措施解除死锁。</p>
<p>预防死锁和避免死锁都属于事先预防策略，预防死锁的限制条件比较严格，实现起来较为简单，但往往导致系统的效率低，资源利用率低;避免死锁的限制条件相对宽松，资源分配后需要通过算法来判断是否进入不安全状态，实现起来较为复杂。</p>
<p>死锁的几种处理策略的比较见表2.4。</p>
<details><summary>表2.4 死锁处理策略的比较</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925225720.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925225720.png';" /></details>

<h4 id="死锁预防-1"><a href="#死锁预防-1" class="headerlink" title="死锁预防"></a>死锁预防</h4><p>防止死锁的发生只需破坏死锁产生的4个必要条件之一即可。</p>
<h5 id="破坏互斥条件"><a href="#破坏互斥条件" class="headerlink" title="破坏互斥条件"></a>破坏互斥条件</h5><p>若允许系统资源都能共享使用，则系统不会进入死锁状态。但有些资源根本不能同时访问，如打印机等临界资源只能互斥使用。所以，破坏互斥条件而预防死锁的方法不太可行，而且在有的场合应该保护这种互斥性。</p>
<h5 id="破坏不剥夺条件"><a href="#破坏不剥夺条件" class="headerlink" title="破坏不剥夺条件"></a>破坏不剥夺条件</h5><p>当一个已保持了某些不可剥夺资源的进程请求新的资源而得不到满足时，它必须释放已经保持的所有资源，待以后需要时再重新申请。这意味着，一个进程已占有的资源会被暂时释放，或者说是被剥夺，或从而破坏了不剥夺条件。</p>
<p>该策略实现起来比较复杂，释放已获得的资源可能造成前一阶段工作的失效，反复地申请和释放资源会增加系统开销,降低系统吞吐量。这种方法常用于状态易于保存和恢复的资源,如 CPU的寄存器及内存资源，一般不能用于打印机之类的资源。</p>
<h5 id="破坏请求并保持条件"><a href="#破坏请求并保持条件" class="headerlink" title="破坏请求并保持条件"></a>破坏请求并保持条件</h5><p>采用预先静态分配方法，即进程在运行前一次申请完它所需要的全部资源，在它的资源未满足前，不把它投入运行。一旦投入运行，这些资源就一直归它所有，不再提出其他资源请求，这样就可以保证系统不会发生死锁。</p>
<p>这种方式实现简单，但缺点也显而易见，系统资源被严重浪费，其中有些资源可能仅在运行初期或运行快结束时才使用，甚至根本不使用。而且还会导致“饥饿”现象，由于个别资源长期被其他进程占用时，将致使等待该资源的进程迟迟不能开始运行。</p>
<h5 id="破坏循环等待条件"><a href="#破坏循环等待条件" class="headerlink" title="破坏循环等待条件"></a>破坏循环等待条件</h5><p>为了破坏循环等待条件，可采用顺序资源分配法。首先给系统中的资源编号，规定每个进程必须按编号递增的顺序请求资源，同类资源一次申请完。也就是说，只要进程提出申请分配资源 $R_i$ ，则该进程在以后的资源申请中就只能申请编号大于 $R_i$ 的资源。</p>
<p>这种方法存在的问题是，编号必须相对稳定，这就限制了新类型设备的增加;尽管在为资源编号时已考虑到大多数作业实际使用这些资源的顺序，但也经常会发生作业使用资源的顺序与系统规定顺序不同的情况，造成资源的浪费;此外，这种按规定次序申请资源的方法，也必然会给用户的编程带来麻烦。</p>
<h4 id="死锁避免"><a href="#死锁避免" class="headerlink" title="死锁避免"></a>死锁避免</h4><p>避免死锁同样属于事先预防策略，但并不是事先采取某种限制措施破坏死锁的必要条件，而是在资源动态分配过程中，防止系统进入不安全状态，以避免发生死锁。这种方法所施加的限制条件较弱，可以获得较好的系统性能。</p>
<h5 id="系统安全状态"><a href="#系统安全状态" class="headerlink" title="系统安全状态"></a>系统安全状态</h5><p>避免死锁的方法中，允许进程动态地申请资源，但系统在进行资源分配之前，应先计算此次分配的安全性。若此次分配不会导致系统进入不安全状态，则允许分配;否则让进程等待。</p>
<p>所谓安全状态，是指系统能按某种进程推进顺序( $P_1$ , $P_2$ ,…, $P_n$ )为每个进程 $P_i$ 分配其所需的资源，直至满足每个进程对资源的最大需求，使每个进程都可顺序完成。此时称 $P_i$ ， $P_2$ .…， $P_n$ 为安全序列。若系统无法找到一个安全序列，则称系统处于不安全状态。</p>
<p>假设系统中有三个进程 $P_1$ , $P_2$ ,和 $P_3$ ，共有12台磁带机。进程 $P_1$ 共需要10台磁带机， $P_2$ 和  $P_3$ 分别需要4台和9台。假设在 $T_0$ 时刻，进程 $P_1$ ， $P_2$ 和 $P_3$ 已分别获得5台、2台和2台，尚有3台未分配，见表2.5。</p>
<details><summary>表2.5 资源分配</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925232929.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925232929.png';" /></details>

<p>在 $T_0$ 时刻是安全的，因为存在一个安全序列 $P_2$ ,  $P_1$ , $P_3$ ，只要系统按此进程序列分配资源，那么每个进程都能顺利完成。也就是说，当前可用磁带机为3台，先把3台磁带机分配给 $Р_2$ 以满足其最大需求， $P_2$ 结束并归还资源后，系统有5台磁带机可用;接下来给 $P_1$ 分配5台磁带机以满足其最大需求， $P_1$ 结束并归还资源后，剩余10台磁带机可用;最后分配7台磁带机给 $P_3$ ，这样$P_3$也能顺利完成。</p>
<p>若在 $T_0$ 时刻后，系统分配1台磁带机给 $P_3$ ，系统剩余可用资源数为2，此时系统进入不安全状态，因为此时已无法再找到一个安全序列。当系统进入不安全状态后，便可能导致死锁。例如，把剩下的2台磁带机分配给 $P_2$ 这样， $P_2$ 完成后只能释放4台磁带机，既不能满足 $P_1$ 又不能满足 $P_3$ ，致使它们都无法推进到完成，彼此都在等待对方释放资源，陷入僵局，即导致死锁。</p>
<p>并非所有的不安全状态都是死锁状态，但当系统进入不安全状态后，便可能进入死锁状态;反之，只要系统处于安全状态，系统便可避免进入死锁状态。</p>
<h5 id="银行家算法"><a href="#银行家算法" class="headerlink" title="银行家算法"></a>银行家算法</h5><p>银行家算法是最著名的死锁避免算法，其思想是:把操作系统视为银行家，操作系统管理的资源相当于银行家管理的资金，进程向操作系统请求分配资源相当于用户向银行家贷款。操作系统按照银行家制定的规则为进程分配资源。进程运行之前先声明对各种资源的最大需求量，当进程在执行中继续申请资源时，先测试该进程已占用的资源数与本次申请的资源数之和是否超过该进程声明的最大需求量。若超过则拒绝分配资源，若未超过则再测试系统现存的资源能否满足该进程尚需的最大资源量，若能满足则按当前的申请量分配资源，否则也要推迟分配。</p>
<h6 id="数据结构描述"><a href="#数据结构描述" class="headerlink" title="数据结构描述"></a>数据结构描述</h6><p>可利用资源向量Available:含有m个元素的数组，其中每个元素代表一类可用的资源数目。Available[ j ]=K表示系统中现有 $R_j$ 类资源K个。</p>
<p>最大需求矩阵 Max: n×m 矩阵，定义系统中 $n$ 个进程中的每个进程对 $m$ 类资源的最大需求。简单来说，一行代表一个进程，一列代表一类资源。Max[ i,j ]=K表示进程i需要 $R_j$ 类资源的最大数目为K。</p>
<p>分配矩阵Allocation:  $n \times m$ 矩阵，定义系统中每类资源当前已分配给每个进程的资源数。Allocation[ i,j ]=K表示进程 $i$ 当前已分得 $R_j$ 类资源的数目为K。初学者容易混淆Available向量和Allocation矩阵，在此特别提醒。</p>
<p>需求矩阵Need:  $n \times m$ 矩阵，表示每个进程接下来最多还需要多少资源。Need[ i,j ]=K表示进程i还需要 $R_j$类资源的数目为K。</p>
<p>上述三个矩阵间存在下述关系:</p>
<p style='text-align:center'>Need= Max- Allocation</p>

<p>一般情况下，在银行家算法的题目中，Max矩阵和Allocation矩阵是已知条件，而求出 Need矩阵是解题的第一步。</p>
<h6 id="银行家算法描述"><a href="#银行家算法描述" class="headerlink" title="银行家算法描述"></a>银行家算法描述</h6><details><summary>银行家算法描述</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925233723.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925233723.png';" /></details>


<h5 id="安全性算法举例"><a href="#安全性算法举例" class="headerlink" title="安全性算法举例"></a>安全性算法举例</h5><details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925235124.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925235124.png';" /></details>

<h5 id="银行家算法举例"><a href="#银行家算法举例" class="headerlink" title="银行家算法举例"></a>银行家算法举例</h5><details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925235226.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925235226.png';" /></details>


<h4 id="死锁检测和解除"><a href="#死锁检测和解除" class="headerlink" title="死锁检测和解除"></a>死锁检测和解除</h4><p>前面介绍的死锁预防和避免算法，都是在为进程分配资源时施加限制条件或进行检测，若系统为进程分配资源时不采取任何措施，则应该提供死锁检测和解除的手段。</p>
<h5 id="资源分配图"><a href="#资源分配图" class="headerlink" title="资源分配图"></a>资源分配图</h5><p>系统死锁可利用资源分配图来描述。如图2.13所示，用圆圈代表一个进程，用框代表一类资源。由于一种类型的资源可能有多个，因此用框中的一个圆代表一类资源中的一个资源。从进程到资源的有向边称为 $\color{green}{\text{请求边}}$ ，表示该进程申请一个单位的该类资源;从资源到进程的边称为 $\color{green}{\text{分配边}}$ ，表示该类资源已有一个资源分配给了该进程。</p>
<p>在图2.13所示的资源分配图中，进程 $P_1$ 已经分得了两个 $R_1$ 资源，并又请求一个 $R_2$ 资源;进程 $P_2$ ,分得了一个 $R_1$ 资源和一个 $R_2$ 资源，并又请求一个 $R_1$ 资源。</p>
<details><summary>图2.13 资源分配示例</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925235430.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925235430.png';" /></details>


<h5 id="死锁定理"><a href="#死锁定理" class="headerlink" title="死锁定理"></a>死锁定理</h5><p>简化资源分配图可检测系统状态S是否为死锁状态。简化方法如下:</p>
<p>1）在资源分配图中，找出既不阻塞又不孤点的进程 $P_i$ (即找出一条有向边与它相连，且该有向边对应资源的申请数量小于等于系统中己有的空闲资源数量，如在图2.13中， $R_1$ 没有空闲资源， $R_2$ 有一个空闲资源。若所有连接该进程的边均满足上述条件，则这个进程能继续运行直至完成，然后释放它所占有的所有资源)。消去它所有的请求边和分配边，使之成为孤立的结点。在图2.14(a)中， $P_1$ 是满足这一条件的进程结点，将P的所有边消去，便得到图2.14(b)所示的情况。</p>
<p>这里要注意一个问题，判断某种资源是否有空间，应用它的资源数量减去它在资源分配图中的出度，例如在图2.13中， $R_1$ 的资源数为3，而出度也为3，所以 $R_1$ 没有空闲资源, $R_2$ 的资源数为2，出度为1，所以 $R_2$ 有一个空闲资源。</p>
<p>2）进程 $P_i$ 所释放的资源，可以唤醒某些因等待这些资源而阻塞的进程，原来的阻塞进程可能变为非阻塞进程。在图2.13中，进程 $P_2$ 就满足这样的条件。根据1)中的方法进行一系列简化后，若能消去图中所有的边，则称该图是可完全简化的，如图2.14(c)所示。</p>
<details><summary>图2.14 资源分配图的化简</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210925235734.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210925235734.png';" /></details>

<p>S为死锁的条件是当且仅当S状态的资源分配图是不可完全简化的，该条件为 $\color{green}{\text{死锁定理}}$ 。</p>
<h5 id="死锁解除"><a href="#死锁解除" class="headerlink" title="死锁解除"></a>死锁解除</h5><p>一旦检测出死锁，就应立即采取相应的措施来解除死锁。死锁解除的主要方法有:</p>
<p>1） $\color{green}{\text{资源剥夺法}}$ 。挂起某些死锁进程，并抢占它的资源，将这些资源分配给其他的死锁进程。但应防止被挂起的进程长时间得不到资源而处于资源匮乏的状态。</p>
<p>2） $\color{green}{\text{撤销进程法}}$ 。强制撤销部分甚至全部死锁进程并剥夺这些进程的资源。撤销的原则可以按进程优先级和撤销进程代价的高低进行。</p>
<p>3） $\color{green}{\text{进程回退法}}$ 。让一（或多〉个进程回退到足以回避死锁的地步，进程回退时自愿释放资源而非被剥夺。要求系统保持进程的历史信息，设置还原点。</p>
<h4 id="本节小结-2"><a href="#本节小结-2" class="headerlink" title="本节小结"></a>本节小结</h4><p>本节开头提出的问题的参考答案如下。</p>
<p>1）为什么会产生死锁?产生死锁有什么条件?</p>
<p>由于系统中存在一些不可剥夺资源，当两个或两个以上的进程占有自身的资源并请求对方的资源时，会导致每个进程都无法向前推进，这就是死锁。死锁产生的必要条件有4个，分别是互斥条件、不剥夺条件、请求并保持条件和循环等待条件。</p>
<p>互斥条件是指进程要求分配的资源是排他性的，即最多只能同时供一个进程使用。</p>
<p>不剥夺条件是指进程在使用完资源之前，资源不能被强制夺走。</p>
<p>请求并保持条件是指进程占有自身本来拥有的资源并要求其他资源。</p>
<p>循环等待条件是指存在一种进程资源的循环等待链。</p>
<p>2）有什么办法可以解决死锁问题?</p>
<p>死锁的处理策略可以分为预防死锁、避免死锁及死锁的检测与解除。</p>
<p>死锁的预防是指通过设立一些限制条件，破坏死锁的一些必要条件，让死锁无法发生。</p>
<p>死锁的避免是指在动态分配资源的过程中，用一些算法防止系统进入不安全状态，从而避免死锁。</p>
<p>死锁的检测和解除是指在死锁产生前不采取任何措施，只检测当前系统有没有发生死锁，若有，则采取一些措施解除死锁。</p>
<h3 id="本章疑难点"><a href="#本章疑难点" class="headerlink" title="本章疑难点"></a>本章疑难点</h3><h4 id=""><a href="#" class="headerlink" title=""></a></h4><h5 id="进程与程序的区别与联系"><a href="#进程与程序的区别与联系" class="headerlink" title="进程与程序的区别与联系"></a>进程与程序的区别与联系</h5><p>1）进程是程序及其数据在计算机上的一次运行活动，是一个动态的概念。进程的运行实体是程序，离开程序的进程没有存在的意义。从静态角度看，进程是由程序、数据和进程控制块（PCB）三部分组成的。而程序是一组有序的指令集合，是一种静态的概念。</p>
<p>2）进程是程序的一次执行过程，它是动态地创建和消亡的，具有一定的生命周期，是暂时存在的;而程序则是一组代码的集合，是永久存在的，可长期保存。</p>
<p>3）一个进程可以执行一个或几个程序，一个程序也可构成多个进程。进程可创建进程，而程序不可能形成新的程序。</p>
<p>4）进程与程序的组成不同。进程的组成包括程序、数据和PCB。</p>
<h5 id="死锁与饥饿"><a href="#死锁与饥饿" class="headerlink" title="死锁与饥饿"></a>死锁与饥饿</h5><p>具有等待队列的信号量的实现可能导致这样的情况:两个或多个进程无限地等待一个事件，而该事件只能由这些等待进程之一来产生。这里的事件是V操作的执行（即释放资源)。出现这样的状态时，这些进程称为死锁(Deadlocked)。</p>
<p>为加以说明，考虑一个由两个进程 $P_0$ 和 $P_1$ 组成的系统，每个进程都访问两个信号量S和Q,这两个信号量的初值均为1。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210926000155.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210926000155.png';" /></details>

<p>假设进程 $P_0$ 执行P(S)，接着进程 $P_1$ 执行P(Q)。当进程 $P_0$ 执行P(Q)时，它必须等待，直到进程 $P_1$ 执行V(Q)。类似地，当进程 $P_1$ 执行P(S)时，它必须等待，直到进程 $P_0$ 执行V(S)。由于这两个V操作都不能执行，因此进程 $P_0$ 和进程 $P_1$ 就死锁了。</p>
<p>一组进程处于死锁状态是指组内的每个进程都在等待一个事件，而该事件只可能由组内的另一个进程产生。这里所关心的主要是事件是资源的获取和释放。</p>
<p>与死锁相关的另一个问题是无限期阻塞（Indefinite Blocking）或饥饿(Starvation)，即进程在信号量内无穷等待的情况。</p>
<p>产生饥饿的主要原因是:在一个动态系统中，对于每类系统资源，操作系统需要确定一个分配策略，当多个进程同时申请某类资源时，由分配策略确定资源分配给进程的次序。有时资源分配策略可能是不公平的，即不能保证等待时间上界的存在。在这种情况下，即使系统没有发生死锁，某些进程也可能会长时间等待。当等待时间给进程推进和响应带来明显影响时，称发生了进程“饥饿”，当“饥饿”到一定程度的进程所赋予的任务即使完成也不再具有实际意义时，称该进程被“饿死”。</p>
<p>例如，当有多个进程需要打印文件时，若系统分配打印机的策略是最短文件优先，则长文件的打印任务将由于短文件的源源不断到来而被无限期推迟，导致最终“饥饿”甚至“饿死”。</p>
<p>“饥饿”并不表示系统一定会死锁，但至少有一个进程的执行被无限期推迟。“饥饿”与死锁的主要差别如下:</p>
<p>1）进入“饥饿”状态的进程可以只有一个，而因循环等待条件而进入死锁状态的进程却必须大于等于两个。</p>
<p>2）处于“饥饿”状态的进程可以是一个就绪进程，如静态优先权调度算法时的低优先权进程，而处于死锁状态的进程则必定是阻塞进程。</p>
<h5 id="银行家算法的工作原理"><a href="#银行家算法的工作原理" class="headerlink" title="银行家算法的工作原理"></a>银行家算法的工作原理</h5><p>银行家算法的主要思想是避免系统进入不安全状态。在每次进行资源分配时，它首先检查系统是否有足够的资源满足要求，若有则先进行分配，并对分配后的新状态进行安全性检查。若新状态安全，则正式分配上述资源，否则拒绝分配上述资源。这样，它保证系统始终处于安全状态，从而避免了死锁现象的发生。</p>
<h5 id="进程同步、互斥的区别和联系"><a href="#进程同步、互斥的区别和联系" class="headerlink" title="进程同步、互斥的区别和联系"></a>进程同步、互斥的区别和联系</h5><p>并发进程的执行会产生相互制约的关系:一种是进程之间竞争使用临界资源，只能让它们逐个使用，这种现象称为互斥，是一种竞争关系;另一种是进程之间协同完成任务，在关键点上等待另一个进程发来的消息，以便协同一致，是一种协作关系。</p>
<h5 id="作业和进程的关系"><a href="#作业和进程的关系" class="headerlink" title="作业和进程的关系"></a>作业和进程的关系</h5><p>进程是系统资源的使用者，系统的资源大部分都是以进程为单位分配的。而用户使用计算机是为了实现一串相关的任务，通常把用户要求计算机完成的这一串任务称为作业。</p>
<h6 id="批处理系统中作业与进程的关系（进程组织"><a href="#批处理系统中作业与进程的关系（进程组织" class="headerlink" title="批处理系统中作业与进程的关系（进程组织)"></a>批处理系统中作业与进程的关系（进程组织)</h6><p>批处理系统可以通过磁记录设备或卡片机向系统提交批作业，由系统的SPOOLing输入进程将作业放入磁盘的输入井，作为后备作业。作业调度程序（一般也作为独立的进程运行）每当选择一道后备作业运行时，首先为该作业创建一个进程（称为该作业的根进程)。该进程将执行作业控制语言解释程序，解释该作业的作业说明书。父进程在运行过程中可以动态地创建一个或多个子进程，执行说明书中的语句。例如，对一条编译的语句，该进程可以创建一个子进程执行编译程序对用户源程序进行编译。类似地，子进程也可继续创建子进程去完成指定的功能。因此,一个作业就动态地转换成了一组运行实体—进程族。当父进程遇到作业说明书中的“撤出作业”语句时，将该作业从运行态改变为完成态，将作业及相关结果送入磁盘上的输出井。作业终止进程负责将输出井中的作业利用打印机输出，回收作业所占用的资源，删除作业有关的数据结构,删除作业在磁盘输出井中的信息等。作业终止进程撤除一道作业后，可向作业调度进程请求进行新的作业调度。至此，一道进入系统运行的作业全部结束。</p>
<h6 id="分时系统中作业与进程的关系"><a href="#分时系统中作业与进程的关系" class="headerlink" title="分时系统中作业与进程的关系"></a>分时系统中作业与进程的关系</h6><p>在分时系统中，作业的提交方法、组织形式均与批处理作业有很大差异。分时系统的用户通过命令语言逐条与系统应合八把武大系体自动时，系统为每个终端设备建立一个进程（称为终端统内部对应一个(以右T经程序，命令解释程序从终端设备读入俞令，解藉疯令是一茶后台命进程)，该进程执仃类令n可以创建一个子进程去具体执行。若当HPN根据需要创建子孙进程。条命令。对于每条终端命令，可以创建一个子进程去具体执行。若当前的终端命令是一条后台命令，则可以和下一条终端命令并行处理。各子进程在运行过程中完全可以根据需要创建子孙进程。终端命令所对应的进程结束后，命令的功能也相应处理完毕。用户本次上机完毕，用户通过一条登出命令即结束上机过程。</p>
<p>分时系统的作业就是用户的一次上机交互过程，可以认为终端进程的创建是一个交互作业的开始，登出命令运行结束代表用户交互作业的终止。</p>
<p>命令解释程序流程扮演着批处理系统中作业控制语言解释程序的角色，只不过命令解释程序是从用户终端接收命令。</p>
<h6 id="交互地提交批作业"><a href="#交互地提交批作业" class="headerlink" title="交互地提交批作业"></a>交互地提交批作业</h6><p>在同时支持交互和批处理的操作系统中，人们可以用交互的方式准备好批作业的有关程序、数据及作业控制说明书。比如，可用交互式系统提供的全屏幕编辑命令编辑好自编的一个天气预报程序，用编译及装配命令将程序变成可执行文件，用调试命令进行程序调试。调试成功后，用户每天都要做如下工作:准备原始天气数据，运行天气预报执行文件处理原始数据，把结果打印出来等。这时，用交互系统提供的全屏幕编辑命令编辑好将要提交的作业控制说明书文件，如Windows系统的 bat 文件和 Linux系统的sh文件。然后用一条作业提交命令将作业提交到系统作业队列中。系统有专门的作业调度进程负责从作业队列中选择作业，为被选取的作业创建一个父进程运行命令解释程序，解释执行作业控制说明书文件中的命令。</p>
<h5 id="各种调度算法的特点"><a href="#各种调度算法的特点" class="headerlink" title="各种调度算法的特点"></a>各种调度算法的特点</h5><details><summary>各种调度算法的特点</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211011170046.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211011170046.png';" /></details>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://ednow.github.io/2021/07/26/%E7%8E%8B%E9%81%93-%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-ch1-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%AE%BA/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ednow">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ednow">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/26/%E7%8E%8B%E9%81%93-%E6%93%8D%E4%BD%9C%E7%B3%BB%E7%BB%9F-ch1-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%B3%BB%E7%BB%9F%E6%A6%82%E8%AE%BA/" class="post-title-link" itemprop="url">王道-操作系统-ch1-计算机系统概论</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-07-26 15:16:25" itemprop="dateCreated datePublished" datetime="2021-07-26T15:16:25+08:00">2021-07-26</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-10-21 18:04:23" itemprop="dateModified" datetime="2021-10-21T18:04:23+08:00">2021-10-21</time>
              </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="王道"><a href="#王道" class="headerlink" title="王道"></a>王道</h1><h2 id="计算机系统概论"><a href="#计算机系统概论" class="headerlink" title="计算机系统概论"></a>计算机系统概论</h2><details><summary>计算机操作系统总考纲</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210917185554.jpg" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210917185554.png';" /></details>

<p>【考纲内容】</p>
<p>(一）操作系统的概念、特征、功能和提供的服务</p>
<p>(二）操作系统的发展与分类</p>
<p>(三）操作系统的运行环境</p>
<p>内核态与用户态;中断、异常;系统调用</p>
<p>(四）操作系统体系结构</p>
<p>【知识框架】</p>
<p>概论</p>
<ul>
<li>特征<ul>
<li>并发(最基本)</li>
<li>共享(最基本)</li>
<li>虚拟</li>
<li>同步</li>
</ul>
</li>
<li>目标和功能<ul>
<li>计算机系统资源的管理者</li>
<li>用户与计算机系统之间的接口<ul>
<li>命令接口</li>
<li>程序接口</li>
<li>GUI</li>
</ul>
</li>
<li>扩充机器</li>
</ul>
</li>
<li>发展——批处理操作系统——分时操作系统——实时操作系统——网络和分布式操作系统</li>
<li>运行机制<ul>
<li>中断和异常</li>
<li>系统调用</li>
</ul>
</li>
<li>体系结构<ul>
<li>大内核</li>
<li>微内核</li>
</ul>
</li>
</ul>
<p>【复习提示】</p>
<p>本章内容通常以选择题的形式考查，重点考查操作系统的功能、运行环境和提供的服务。要求读者能在宏观上把握操作系统各个部分的功能，微观上掌握细微的知识点。因此，在复习操作系统时，首先要在形成大体框架后，通过反复做题巩固、完善知识体系，然后把操作系统的所有内容串成一个整体。本章的内容有助于读者整体上初步认识操作系统，为后面展开各章节的知识点奠定基础，进而整体把握课程。不要因为本章内容在历年考题中出现的比例不高而忽视。</p>
<h3 id="操作系统的基本概念"><a href="#操作系统的基本概念" class="headerlink" title="操作系统的基本概念"></a>操作系统的基本概念</h3><h4 id="操作系统的概念"><a href="#操作系统的概念" class="headerlink" title="操作系统的概念"></a>操作系统的概念</h4><p>在信息化时代，软件是计算机系统的灵魂，而作为软件核心的操作系统，已与现代计算机系统密不可分、融为一体。计算机系统自下而上可大致分为4部分:$\color{green}{\text{硬件}}$、$\color{green}{\text{操作系统}}$、$\color{green}{\text{应用程序}}$和$\color{green}{\text{用户}}$(这里的划分与计算机组成原理中的分层不同)。操作系统管理各种计算机硬件，为应用程序提供基础，并充当计算机硬件与用户之间的中介。</p>
<p>硬件如中央处理器、内存、输入/输出设备等，提供基本的计算资源。应用程序如字处理程序、电子制表软件、编译器、网络浏览器等，规定按何种方式使用这些资源来解决用户的计算问题。操作系统控制和协调各用户的应用程序对硬件的分配与使用。</p>
<p>在计算机系统的运行过程中，操作系统提供了正确使用这些资源的方法。</p>
<p>综上所述，操作系统(Operating System，OS）是指控制和管理整个计算机系统的硬件与软件资源，合理地组织、调度计算机的工作与资源的分配，进而为用户和其他软件提供方便接口与环境的程序集合。操作系统是计算机系统中最基本的系统软件。</p>
<h4 id="操作系统的特征"><a href="#操作系统的特征" class="headerlink" title="操作系统的特征"></a>操作系统的特征</h4><p>操作系统是一种系统软件，但与其他系统软件和应用软件有很大的不同，它有自己的特殊性即基本特征。操作系统的基本特征包括并发、共享、虚拟和异步。这些概念对理解和掌握操作系统的核心至关重要，将一直贯穿于各个章节中。</p>
<h5 id="并发-Concurrence"><a href="#并发-Concurrence" class="headerlink" title="并发(Concurrence)"></a>并发(Concurrence)</h5><p>并发是指两个或多个事件在同一时间间隔内发生。操作系统的并发性是指计算机系统中同时存在多个运行的程序，因此它具有处理和调度多个程序同时执行的能力。在操作系统中，引入进程的目的是使程序能并发执行。</p>
<p>注意同一时间间隔（$\color{red}{\text{并发}}$）和同一时刻（$\color{red}{\text{并行}}$）的区别。在多道程序环境下，一段时间内，宏观上有多道程序在同时执行，而在每个时刻，单处理机环境下实际仅能有一道程序执行，因此微观上这些程序仍是分时交替执行的。操作系统的并发性是通过分时得以实现的。</p>
<p>注意，并行性是指系统具有同时进行运算或操作的特性，在同一时刻能完成两种或两种以上的工作。并行性需要有相关硬件的支持，如多流水线或多处理机硬件环境。</p>
<p>我们以现实生活中的直观例子来认识并发和并行的区别。例如，如果你在9:00～9:10仅吃面包，在9:10～9:20仅写字，在9:20～9:30仅吃面包，在9:30～10:00仅写字，那么在9:00～10:00吃面包和写字这两种行为就是并发执行的;再如，如果你在9:00～10:00右手写字，左手同时拿着面包吃，那么这两个动作就是并行执行的。</p>
<h5 id="共享-Sharing"><a href="#共享-Sharing" class="headerlink" title="共享(Sharing)"></a>共享(Sharing)</h5><p>资源共享即共享，是指系统中的资源可供内存中多个并发执行的进程共同使用。共享可分为以下两种资源共享方式。</p>
<p>(1）互斥共享方式</p>
<p>系统中的某些资源，如打印机、磁带机，虽然可供多个进程使用，但为使得所打印或记录的结果不致造成混淆，应规定在一段时间内只允许一个进程访问该资源。</p>
<p>为此，当进程A访问某个资源时，必须先提出请求，若此时该资源空闲，则系统便将之分配给进程A使用，此后有其他进程也要访问该资源时（只要A未用完）就必须等待。仅当进程A访问完并释放该资源后，才允许另一个进程对该资源进行访问。我们把这种资源共享方式称为互斥式共享，而把在一段时间内只允许一个进程访问的资源称为临界资源或独占资源。计算机系统中的大多数物理设备及某些软件中所用的栈、变量和表格，都属于临界资源，它们都要求被互斥地共享。</p>
<p>(2）同时访问方式</p>
<p>系统中还有另一类资源，这类资源允许在一段时间内由多个进程“同时”访问。这里所说的“同时”通常是宏观上的，而在微观上，这些进程可能是交替地对该资源进行访问即“分时共享”的。可供多个进程“同时”访问的典型资源是磁盘设备，一些用重入码编写的文件也可被“同时”共享，即允许若干个用户同时访问该文件。</p>
<p>注意，互斥共享要求一种资源在一段时间内（哪怕是一段很小的时间）只能满足一个请求，否则就会出现严重的问题，(你能想象打印机第一行打印A文档的内容、第二行打印B文档的内容的效果吗?）而同时访问共享通常要求一个请求分几个时间片段间隔地完成，其效果与连续完成的效果相同。</p>
<p>$\color{green}{\text{并发}}$和$\color{green}{\text{共享}}$是操作系统两个最基本的特征，两者之间互为存在的条件:①资源共享是以程序的并发为条件的，若系统不允许程序并发执行，则自然不存在资源共享问题;②若系统不能对资源共享实施有效的管理，则必将影响到程序的并发执行，甚至根本无法并发执行。</p>
<h5 id="虚拟-Virtual"><a href="#虚拟-Virtual" class="headerlink" title="虚拟(Virtual)"></a>虚拟(Virtual)</h5><p>虚拟是指把一个物理上的$\color{green}{\text{实体}}$变为若干$\color{green}{\text{逻辑}}$上的对应物。物理实体（前者）是实的，即实际存在的;而后者是虚的，是用户感觉上的事物。用于实现虚拟的技术，称为虚拟技术。操作系统中利用了多种虚拟技术来实现虚拟处理器、虚拟内存和虚拟外部设备等。</p>
<p>虚拟处理器技术是通过多道程序设计技术，采用让多道程序并发执行的方法，来分时使用一个处理器的。此时，虽然只有一个处理器，但它能同时为多个用户服务，使每个终端用户都感觉有一个中央处理器(CPU)在专门为它服务。利用多道程序设计技术把一个物理上的CPU虚拟为多个逻辑上的CPU，称为虚拟处理器。</p>
<p>类似地，可以采用虚拟存储器技术将一台机器的物理存储器变为虚拟存储器，以便从逻辑上扩充存储器的容量。当然，这时用户所感觉到的内存容量是虚的。我们把用户感觉到（但实际不存在)的存储器称为虚拟存储器。</p>
<p>还可采用虚拟设备技术将一台物理IO设备虚拟为多台逻辑上的I/O 设备，并允许每个用户占用一台逻辑上的IO设备，使原来仅允许在一段时间内由一个用户访问的设备（即临界资源)变为在一段时间内允许多个用户同时访问的共享设备。</p>
<p>因此，操作系统的虚拟技术可归纳为:时分复用技术，如处理器的分时共享;空分复用技术，如虚拟存储器。</p>
<h5 id="异步-Asynchronism"><a href="#异步-Asynchronism" class="headerlink" title="异步(Asynchronism)"></a>异步(Asynchronism)</h5><p>多道程序环境允许多个程序并发执行，但由于资源有限，进程的执行并不是一贯到底的，而是走走停停的，它以不可预知的速度向前推进，这就是进程的异步性。</p>
<p>异步性使得操作系统运行在一种随机的环境下，可能导致进程产生与时间有关的错误（就像对全局变量的访问顺序不当会导致程序出错一样)。然而，只要运行环境相同，操作系统就须保证多次运行进程后都能获得相同的结果。</p>
<h4 id="操作系统的目标和功能"><a href="#操作系统的目标和功能" class="headerlink" title="操作系统的目标和功能"></a>操作系统的目标和功能</h4><p>为了给多道程序提供良好的运行环境，操作系统应具有以下几方面的功能:处理机管理、存储器管理、设备管理和文件管理。为了方便用户使用操作系统，还必须向用户提供接口。同时，操作系统可用来扩充机器，以提供更方便的服务、更高的资源利用率。</p>
<p>我们用一个直观的例子来理解这种情况。例如，用户是雇主，操作系统是工人（用来操作机器)，计算机是机器（由处理机、存储器、设备、文件几个部件构成)，工人有熟练的技能，能够控制和协调各个部件的工作，这就是操作系统对资源的管理;同时，工人必须接收雇主的命令，这就是“接口”;有了工人，机器就能发挥更大的作用，因此工人就成了“扩充机器”。</p>
<h5 id="操作系统作为计算机系统资源的管理者"><a href="#操作系统作为计算机系统资源的管理者" class="headerlink" title="操作系统作为计算机系统资源的管理者"></a>操作系统作为计算机系统资源的管理者</h5><h6 id="处理机管理"><a href="#处理机管理" class="headerlink" title="处理机管理"></a>处理机管理</h6><p>在多道程序环境下，处理机的分配和运行都以进程（或线程）为基本单位，因而对处理机的管理可归结为对进程的管理。并发是指在计算机内同时运行多个进程，因此进程何时创建、何时撤销、如何管理、如何避免冲突、合理共享就是进程管理的最主要的任务。进程管理的主要功能包括进程控制、进程同步、进程通信、死锁处理、处理机调度等。</p>
<h6 id="存储器管理"><a href="#存储器管理" class="headerlink" title="存储器管理"></a>存储器管理</h6><p>存储器管理是为了给多道程序的运行提供良好的环境，方便用户使用及提高内存的利用率，主要包括内存分配与回收、地址映射、内存保护与共享和内存扩充等功能。</p>
<h6 id="文件管理"><a href="#文件管理" class="headerlink" title="文件管理"></a>文件管理</h6><p>计算机中的信息都是以文件的形式存在的，操作系统中负责文件管理的部分称为文件系统。文件管理包括文件存储空间的管理、目录管理及文件读写管理和保护等。</p>
<h6 id="设备管理"><a href="#设备管理" class="headerlink" title="设备管理"></a>设备管理</h6><p>设备管理的主要任务是完成用户的IO请求，方便用户使用各种设备，并提高设备的利用率，主要包括缓冲管理、设备分配、设备处理和虚拟设备等功能。</p>
<p>这些工作都由“工人”负责，“雇主”无须关注。</p>
<h5 id="操作系统作为用户与计算机硬件系统之间的接口"><a href="#操作系统作为用户与计算机硬件系统之间的接口" class="headerlink" title="操作系统作为用户与计算机硬件系统之间的接口"></a>操作系统作为用户与计算机硬件系统之间的接口</h5><p>为了让用户方便、快捷、可靠地操纵计算机硬件并运行自己的程序，操作系统还提供了用户接口。操作系统提供的接口主要分为两类:一类是命令接口，用户利用这些操作命令来组织和控制作业的执行;另一类是程序接口，编程人员可以使用它们来请求操作系统服务。</p>
<h6 id="命令接口"><a href="#命令接口" class="headerlink" title="命令接口"></a>命令接口</h6><p>使用命令接口进行作业控制的主要方式有两种，即联机控制方式和脱机控制方式。按作业控制方式的不同，可将命令接口分为联机命令接口和脱机命令接口。</p>
<p>$\color{green}{\text{联机命令接口}}$又称$\color{green}{\text{交互式命令接口}}$，适用于分时或实时系统的接口。它由一组键盘操作命令组成。用户通过控制台或终端输入操作命令，向系统提出各种服务要求。用户每输入一条命令，控制权就转给操作系统的命令解释程序，然后由命令解释程序解释并执行输入的命令，完成指定的功能。之后，控制权转回控制台或终端，此时用户又可输入下一条命令。联机命令接口可以这样理解:“雇主”说一句话，“工人”做一件事，并做出反馈，这就强调了交互性。</p>
<p>$\color{green}{\text{脱机命令接口}}$又称$\color{green}{\text{批处理命令接口}}$，适用于批处理系统，它由一组作业控制命令组成。脱机用户不能直接干预作业的运行，而应事先用相应的作业控制命令写成一份作业操作说明书，连同作业一起提交给系统。系统调度到该作业时，由系统中的命令解释程序逐条解释执行作业说明书上的命令，从而间接地控制作业的运行。脱机命令接口可以这样理解:“雇主”把要“工人”做的事写在清单上，“工人”按照清单命令逐条完成这些事，这就是批处理。</p>
<blockquote>
<p>$\color{green}{\text{联}}$机和脱机可以理解为「$\color{green}{\text{联}}$接机器」的「$\color{green}{\text{联}}$」不是「$\color{red}{\text{联}}$网」的「$\color{red}{\text{联}}$」;交互的状态不就是连接着机器的吗</p>
</blockquote>
<h6 id="程序接口"><a href="#程序接口" class="headerlink" title="程序接口"></a>程序接口</h6><p>程序接口由一组$\color{green}{\text{系统调用}}$（也称$\color{green}{\text{广义指令}}$）组成。用户通过在程序中使用这些系统调用来请求操作系统为其提供服务，如使用各种外部设备、申请分配和回收内存及其他各种要求。</p>
<p>当前最为流行的是图形用户界面(GUI),即图形接口。GUI最终是通过调用程序接口实现的，用户通过鼠标和键盘在图形界面上单击或使用快捷键，就能很方便地使用操作系统。严格来说，图形接口不是操作系统的一部分，但图形接口所调用的系统调用命令是操作系统的一部分。</p>
<h5 id="操作系统用作扩充机器"><a href="#操作系统用作扩充机器" class="headerlink" title="操作系统用作扩充机器"></a>操作系统用作扩充机器</h5><p>没有任何软件支持的计算机称为裸机，它仅构成计算机系统的物质基础，而实际呈现在用户面前的计算机系统是经过若干层软件改造的计算机。裸机在最里层，其外面是操作系统。操作系统所提供的资源管理功能和方便用户的各种服务功能，将裸机改造成功能更强、使用更方便的机器;因此，我们通常把覆盖了软件的机器称为扩充机器或虚拟机。</p>
<p>“工人”操作机器，机器就有更大的作用，于是“工人”便成了“扩充机器”。</p>
<p>注意，本课程所关注的内容是操作系统如何控制和协调处理机、存储器、设备和文件，而不关注接口和扩充机器，后两者读者只需要有个印象，能理解即可。</p>
<h3 id="操作系统的发展与分类"><a href="#操作系统的发展与分类" class="headerlink" title="操作系统的发展与分类"></a>操作系统的发展与分类</h3><h4 id="手工操作阶段（此阶段无操作系统"><a href="#手工操作阶段（此阶段无操作系统" class="headerlink" title="手工操作阶段（此阶段无操作系统)"></a>手工操作阶段（此阶段无操作系统)</h4><p>用户在计算机上算题的所有工作都要人工干预，如程序的装入、运行、结果的输出等。随着计算机硬件的发展，人机矛盾（速度和资源利用）越来越大，必须寻求新的解决办法。</p>
<p>手工操作阶段有两个突出的缺点:①用户独占全机，虽然不会出现因资源已被其他用户占用而等待的现象，但资源利用率低。②CPU等待手工操作，CPU的利用不充分。</p>
<p>唯一的解决办法就是用高速的机器代替相对较慢的手工操作来对作业进行控制。</p>
<h4 id="批处理阶段-操作系统开始出现）"><a href="#批处理阶段-操作系统开始出现）" class="headerlink" title="批处理阶段(操作系统开始出现）"></a>批处理阶段(操作系统开始出现）</h4><p>为了解决人机矛盾及CPU和IO设备之间速度不匹配的矛盾，出现了批处理系统。按发展历程又分为单道批处理系统、多道批处理系统（多道程序设计技术出现以后)。·</p>
<h5 id="单道批处理系统"><a href="#单道批处理系统" class="headerlink" title="单道批处理系统"></a>单道批处理系统</h5><p>系统对作业的处理是成批进行的，但内存中始终保持一道作业。单道批处理系统是在解决人机矛盾及CPU和IO设备速率不匹配的矛盾中形成的。单道批处理系统的主要特征如下:</p>
<p>1）自动性。在顺利的情况下，磁带上的一批作业能自动地逐个运行，而无须人工干预</p>
<p>2）顺序性。磁带上的各道作业顺序地进入内存，各道作业的完成顺序与它们进入内存的顺序在正常情况下应完全相同，亦即先调入内存的作业先完成。</p>
<p>3）单道性。内存中仅有一道程序运行，即监督程序每次从磁带上只调入一道程序进入内存运行，当该程序完成或发生异常情况时，才换入其后继程序进入内存运行。</p>
<p>此时面临的问题是:每次主机内存中仅存放一道作业，每当它在运行期间（注意这里是“运行时”而不是“完成后”)发出输入/输出请求后，高速的CPU便处于等待低速的IO完成的状态。为了进一步提高资源的利用率和系统的吞吐量，引入了多道程序技术。</p>
<h5 id="多道批处理系统"><a href="#多道批处理系统" class="headerlink" title="多道批处理系统"></a>多道批处理系统</h5><p>多道程序设计技术允许多个程序同时进入内存并允许它们在CPU中交替地运行，这些程序共享系统中的各种硬/软件资源。当一道程序因I/O请求而暂停运行时，CPU便立即转去运行另一道程序。它不采用某些机制来提高某一技术方面的瓶颈问题，而让系统的各个组成部分都尽量去“忙”，因此切换任务所花费的时间很少，可实现系统各部件之间的并行工作，使其整体在单位时间内的效率翻倍。</p>
<p>当然，多道批处理系统的设计和实现要比单道系统复杂很多，因为要充分利用各种资源，就要涉及各种资源的调度问题。</p>
<p>多道程序设计的特点是多道、宏观上并行、微观上串行。</p>
<p>1）多道。计算机内存中同时存放多道相互独立的程序。</p>
<p>2）宏观上并行。同时进入系统的多道程序都处于运行过程中，即它们先后开始各自的运行，但都未运行完毕。</p>
<p>3）微观上串行。内存中的多道程序轮流占有CPU，交替执行。</p>
<p>多道程序设计技术的实现需要解决下列问题:</p>
<p>1）如何分配处理器。</p>
<p>2）多道程序的内存分配问题。</p>
<p>3）IO设备如何分配。</p>
<p>4）如何组织和存放大量的程序和数据，以方便用户使用并保证其安全性与一致性。</p>
<p>在批处理系统中采用多道程序设计技术就形成了多道批处理操作系统。该系统把用户提交的作业成批地送入计算机内存，然后由作业调度程序自动地选择作业运行。</p>
<p>优点:资源利用率高，多道程序共享计算机资源，从而使各种资源得到充分利用;系统吞吐量大，CPU和其他资源保持“忙碌”状态。缺点:用户响应的时间较长;不提供人机交互能力，用户既不能了解自己的程序的运行情况，又不能控制计算机。</p>
<h4 id="分时操作系统"><a href="#分时操作系统" class="headerlink" title="分时操作系统"></a>分时操作系统</h4><p>所谓分时技术，是指把处理器的运行时间分成很短的时间片，按时间片轮流把处理器分配给各联机作业使用。若某个作业在分配给它的时间片内不能完成其计算，则该作业暂时停止运行，把处理器让给其他作业使用，等待下一轮再继续运行。由于计算机速度很快，作业运行轮转得也很快，因此给每个用户的感觉就像是自己独占一台计算机。</p>
<p>分时操作系统是指多个用户通过终端同时共享一台主机，这些终端连接在主机上，用户可以同时与主机进行交互操作而互不干扰。因此，实现分时系统最关键的问题是如何使用户能与自己的作业进行交互，即当用户在自己的终端上键入命令时，系统应能及时接收并及时处理该命令，再将结果返回用户。分时系统也是支持多道程序设计的系统，但它不同于多道批处理系统多道批处理是实现作业自动控制而无须人工干预的系统，而分时系统是实现人机交互的系统，这使得分时系统具有与批处理系统不同的特征。分时系统的主要特征如下:</p>
<p>1）同时性。同时性也称多路性，指允许多个终端用户同时使用一台计算机，即一台计算机与若干台终端相连接，终端上的这些用户可以同时或基本同时使用计算机。</p>
<p>2）交互性。用户能够方便地与系统进行人机对话，即用户通过终端采用人机对话的方式直接控制程序运行，与同程序进行交互。</p>
<p>3）独立性。系统中多个用户可以彼此独立地进行操作，互不干扰，单个用户感觉不到别人也在使用这台计算机，好像只有自己单独使用这台计算机一样。</p>
<p>4）及时性。用户请求能在很短时间内获得响应。分时系统采用时间片轮转方式使一台计算机同时为多个终端服务，使用户能够对系统的及时响应感到满意。</p>
<p>虽然分时操作系统较好地解决了人机交互问题，但在一些应用场合，需要系统能对外部的信息在规定的时间（比时间片的时间还短）内做出处理（比如飞机订票系统或导弹制导系统)，因此，实时操作系统应运而生。</p>
<h4 id="实时操作系统"><a href="#实时操作系统" class="headerlink" title="实时操作系统"></a>实时操作系统</h4><p>为了能在某个时间限制内完成某些紧急任务而不需要时间片排队，诞生了实时操作系统。这里的时间限制可以分为两种情况:若某个动作必须绝对地在规定的时刻（或规定的时间范围）发生，则称为硬实时系统，如飞行器的飞行自动控制系统，这类系统必须提供绝对保证，让某个特定的动作在规定的时间内完成。若能够接受偶尔违反时间规定且不会引起任何永久性的损害，则称为软实时系统，如飞机订票系统、银行管理系统。</p>
<p>在实时操作系统的控制下，计算机系统接收到外部信号后及时进行处理，并在严格的时限内处理完接收的事件。实时操作系统的主要特点是 $\color{green}{\text{及时性}}$ 和 $\color{green}{\text{可靠性}}$ 。</p>
<ul>
<li>系统架构师的考试中涉及到，什么是实时系统，实时系统的主要特征有哪些</li>
</ul>
<h4 id="网络操作系统和分布式计算机系统"><a href="#网络操作系统和分布式计算机系统" class="headerlink" title="网络操作系统和分布式计算机系统"></a>网络操作系统和分布式计算机系统</h4><p>网络操作系统把计算机网络中的各台计算机有机地结合起来，提供一种统一、经济而有效的使用各台计算机的方法，实现各台计算机之间数据的互相传送。网络操作系统最主要的特点是网络中各种资源的共享及各台计算机之间的通信。</p>
<p>分布式计算机系统是由多台计算机组成并满足下列条件的系统:系统中任意两台计算机通过通信方式交换信息;系统中的每台计算机都具有同等的地位，即没有主机也没有从机;每台计算机上的资源为所有用户共享;系统中的任意台计算机都可以构成一个子系统，并且还能重构;任何工作都可以分布在几台计算机上，由它们并行工作、协同完成。用于管理分布式计算机系统的操作系统称为分布式计算机系统。该系统的主要特点是:分布性和并行性。分布式操作系统与网络操作系统的本质不同是，分布式操作系统中的若干计算机相互协同完成同一任务。</p>
<h4 id="个人计算机操作系统"><a href="#个人计算机操作系统" class="headerlink" title="个人计算机操作系统"></a>个人计算机操作系统</h4><p>个人计算机操作系统是目前使用最广泛的操作系统，它广泛应用于文字处理、电子表格、游戏中，常见的有 Windows、Linux和 Macintosh等。操作系统的发展历程如图1.1所示。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210726161521.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210726161521.png';" /></details>

<p>此外，还有嵌入式操作系统、服务器操作系统、智能手机操作系统等。</p>
<h3 id="操作系统的运行环境"><a href="#操作系统的运行环境" class="headerlink" title="操作系统的运行环境"></a>操作系统的运行环境</h3><h4 id="操作系统的运行机制"><a href="#操作系统的运行机制" class="headerlink" title="操作系统的运行机制"></a>操作系统的运行机制</h4><blockquote>
<p>初学者需要弄清楚一个问题，即计算机“指令”和高级语言的“代码”是不同的。我们一般所说的“编写代码”指的是用高级语言〈如C、Java等）来编写程序。但CPU看不懂这些高级语言程序的含义，为了让这些程序能够顺利执行，就需要把它们“翻译”成CPU能懂的机器语言，即一条条“指令”(这个“翻译”的过程称为“编译”)。所谓执行程序，其实就是CPU根据一条条指令的指示来执行一个个具体的操作。</p>
</blockquote>
<p>计算机系统中，通常CPU执行两种不同性质的程序:一种是操作系统内核程序;另一种是用户自编程序（即系统外层的应用程序，或简称“应用程序”)。对操作系统而言，这两种程序的作用不同，前者是后者的管理者，因此“管理程序”(即内核程序）要执行一些特权指令，而“被管理程序”(即用户自编程序）出于安全考虑不能执行这些指令。所谓特权指令，是指计算机中不允许用户直接使用的指令，如IO指令、置中断指令，存取用于内存保护的寄存器、送程序状态字到程序状态字寄存器等的指令。在具体实现上，将CPU的状态划分为用户态(目态）和核心态（又称管态、内核态)。可以理解为CPU内部有一个小开关，当小开关为1时，CPU处于核心态，此时CPU可以执行特权指令;当小开关为0时，CPU处于用户态，此时CPU只能执行非特权指令。用户自编程序运行在用户态，操作系统内核程序运行在核心态。</p>
<p>在软件工程思想和结构化程序设计方法影响下诞生的现代操作系统，几乎都是层次式的结构。操作系统的各项功能分别被设置在不同的层次上。一些与硬件关联较紧密的模块，如时钟管理、中断处理、设备驱动等处于最低层。其次是运行频率较高的程序，如进程管理、存储器管理和设备管理等。这两部分内容构成了操作系统的内核。这部分内容的指令操作工作在核心态。</p>
<p>内核是计算机上配置的底层软件，是计算机功能的延伸。不同系统对内核的定义稍有区别，大多数操作系统的内核包括4方面的内容。</p>
<h5 id="时钟管理"><a href="#时钟管理" class="headerlink" title="时钟管理"></a>时钟管理</h5><p>在计算机的各种部件中，时钟是最关键的设备。时钟的第一功能是计时，操作系统需要通过时钟管理，向用户提供标准的系统时间。另外，通过时钟中断的管理，可以实现进程的切换。例如，在分时操作系统中采用时间片轮转调度，在实时系统中按截止时间控制运行，在批处理系统中通过时钟管理来衡量一个作业的运行程度等。因此，系统管理的方方面面无不依赖于时钟。</p>
<h5 id="中断机制"><a href="#中断机制" class="headerlink" title="中断机制"></a>中断机制</h5><p>引入中断技术的初衷是提高多道程序运行环境中CPU 的利用率，而且主要是针对外部设备的。后来逐步得到发展，形成了多种类型，成为操作系统各项操作的基础。例如，键盘或鼠标信息的输入、进程的管理和调度、系统功能的调用、设备驱动、文件访问等，无不依赖于中断机制。可以说，现代操作系统是靠中断驱动的软件。</p>
<p>中断机制中，只有一小部分功能属于内核，它们负责保护和恢复中断现场的信息，转移控制权到相关的处理程序。这样可以减少中断的处理时间，提高系统的并行处理能力。</p>
<h5 id="原语"><a href="#原语" class="headerlink" title="原语"></a>原语</h5><p>按层次结构设计的操作系统，底层必然是一些可被调用的公用小程序，它们各自完成一个规定的操作。它们的特点如下:</p>
<p>1）处于操作系统的最低层，是最接近硬件的部分。</p>
<p>2)这些程序的运行具有原子性，其操作只能一气呵成(主要从系统安全性和便于管理考虑)。</p>
<p>3）这些程序的运行时间都较短，而且调用频繁。</p>
<p>通常把具有这些特点的程序称为原语(Atomic Operation)。定义原语的直接方法是关闭中断，让其所有动作不可分割地完成后再打开中断。</p>
<p>系统中的设备驱动、CPU切换、进程通信等功能中的部分操作都可定义为原语，使它们成为内核的组成部分。</p>
<h5 id="系统控制的数据结构及处理"><a href="#系统控制的数据结构及处理" class="headerlink" title="系统控制的数据结构及处理"></a>系统控制的数据结构及处理</h5><p>系统中用来登记状态信息的数据结构很多，如作业控制块、进程控制块(PCB)、设备控制块、各类链表、消息队列、缓冲区、空闲区登记表、内存分配表等。为了实现有效的管理,系统需要一些基本的操作，常见的操作有以下3种:</p>
<p>1)进程管理。进程状态管理、进程调度和分派、创建与撤销进程控制块等。</p>
<p>2）存储器管理。存储器的空间分配和回收、内存信息保护程序、代码对换程序等。</p>
<p>3）设备管理。缓冲区管理、设备分配和回收等。</p>
<p>从上述内容可以了解，核心态指令实际上包括系统调用类指令和一些针对时钟、中断和原语的操作指令。</p>
<h4 id="中断和异常的概念"><a href="#中断和异常的概念" class="headerlink" title="中断和异常的概念"></a>中断和异常的概念</h4><blockquote>
<p>建议结合《计算机组成原理考研复习指导》第7章学习，那里的讲解更详细。</p>
</blockquote>
<p>在操作系统中引入核心态和用户态这两种工作状态后，就需要考虑这两种状态之间如何切换。操作系统内核工作在核心态，而用户程序工作在用户态。系统不允许用户程序实现核心态的功能，而它们又必须使用这些功能。因此，需要在核心态建立一些“门”，以便实现从用户态进入核心态。在实际操作系统中，CPU运行上层程序时唯一能进入这些“门”的途径就是通过中断或异常。发生中断或异常时，运行用户态的CPU会立即进入核心态，这是通过硬件实现的(例如,用一个特殊寄存器的一位来表示CPU所处的工作状态，0表示核心态，1表示用户态。若要进入核心态，则只需将该位置0即可)。中断是操作系统中非常重要的一个概念，对一个运行在计算机上的实用操作系统而言，缺少了中断机制，将是不可想象的。原因是，操作系统的发展过程大体上就是一个想方设法不断提高资源利用率的过程，而提高资源利用率就需要在程序并未使用某种资源时，把它对那种资源的占有权释放，而这一行为就需要通过中断实现。</p>
<h5 id="中断和异常的定义"><a href="#中断和异常的定义" class="headerlink" title="中断和异常的定义"></a>中断和异常的定义</h5><p>中断(Interruption)也称外中断，指来自CPU执行指令以外的事件的发生，如设备发出的IO结束中断，表示设备输入/输出处理已经完成，希望处理机能够向设备发下一个输入/输出请求，同时让完成输入/输出后的程序继续运行。时钟中断，表示一个固定的时间片已到，让处理机处理计时、启动定时运行的任务等。这一类中断通常是与当前指令执行无关的事件，即它们与当前处理机运行的程序无关。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210726162307.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210726162307.png';" /></details>

<p>异常（Exception）也称内中断、例外或陷入(trap)，指源自CPU执行指令内部的事件,如程序的非法操作码、地址越界、算术溢出、虚存系统的缺页及专门的陷入指令等引起的事件。对异常的处理一般要依赖于当前程序的运行现场，而且异常不能被屏蔽，一旦出现应立即处理。关于内中断和外中断的联系与区别如图1.2所示。</p>
<blockquote>
<p>异常不能被屏蔽，一旦出现应立即处理???</p>
</blockquote>
<h5 id="中断处理的过程"><a href="#中断处理的过程" class="headerlink" title="中断处理的过程"></a>中断处理的过程</h5><p>不同计算机的中断(指外中断)处理过程各具特色，就其多数而论，中断处理流程如图1.3所示。各阶段处理流程的描述如下:</p>
<details><summary>图1.3 中断处理流程</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210729154445.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210729154445.png';" /></details>

<p>1）关中断。CPU响应中断后，首先要保护程序的现场状态，在保护现场的过程中，CPU不应响应更高级中断源的中断请求。否则，若现场保存不完整，在中断服务程序结束后，也就不能正确地恢复并继续执行现行程序。</p>
<p>2）保存断点。为保证中断服务程序执行完毕后能正确地返回到原来的程序，必须将原来的程序的断点（即程序计数器PC）保存起来。</p>
<p>3）中断服务程序寻址。其实质是取出中断服务程序的入口地址送入程序计数器PC。</p>
<p>4)保存现场和屏蔽字。进入中断服务程序后，首先要保存现场，现场信息一般是指程序状态字寄存器PSWR和某些通用寄存器的内容。</p>
<p>5）开中断。允许更高级中断请求得到响应。</p>
<p>6）执行中断服务程序。这是中断请求的目的。</p>
<p>7）关中断。保证在恢复现场和屏蔽字时不被中断。</p>
<p>8）恢复现场和屏蔽字。将现场和屏蔽字恢复到原来的状态。</p>
<p>9)开中断、中断返回。中断服务程序的最后一条指令通常是一条中断返回指令，使其返回到原程序的断点处，以便继续执行原程序。</p>
<p>其中，1 ~ 3步是在CPU进入中断周期后，由硬件自动（中断隐指令）完成的;4 ~ 9步由中断服务程序完成。恢复现场是指在中断返回前，必须将寄存器的内容恢复到中断处理前的状态，这部分工作由中断服务程序完成。中断返回由中断服务程序的最后一条中断返回指令完成。</p>
<h4 id="系统调用"><a href="#系统调用" class="headerlink" title="系统调用"></a>系统调用</h4><p>所谓系统调用，是指用户在程序中调用操作系统所提供的一些子功能，系统调用可视为特殊的公共子程序。系统中的各种共享资源都由操作系统统一掌管，因此在用户程序中，凡是与资源有关的操作（如存储分配、进行IO传输及管理文件等)，都必须通过系统调用方式向操作系统提出服务请求，并由操作系统代为完成。通常，一个操作系统提供的系统调用命令有几十条乃至上百条之多。这些系统调用按功能大致可分为如下几类。</p>
<ul>
<li>设备管理。完成设备的请求或释放，以及设备启动等功能。文件管理。完成文件的读、写、创建及删除等功能。</li>
<li>进程控制。完成进程的创建、撤销、阻塞及唤醒等功能。</li>
<li>进程通信。完成进程之间的消息传递或信号传递等功能。</li>
<li>内存管理。完成内存的分配、回收以及获取作业占用内存区大小及始址等功能。</li>
</ul>
<p>显然，系统调用相关功能涉及系统资源管理、进程管理之类的操作，对整个系统的影响非常大，因此必定需要使用某些特权指令才能完成，所以系统调用的处理需要由操作系统内核程序负责完成，要运行在核心态。用户程序可以执行陷入指令(又称$\color{green}{\text{访管指令}}$或$\color{green}{\text{trap指令}}$）来发起系统调用，请求操作系统提供服务。可以这么理解，用户程序执行“陷入指令”，相当于把CPU成低用权主动交给操作系统内核程序（CPU 状态会从用户态进入核心态)，之后操作系统内核程序再对系统调用请求做出相应处理。处理完成后，操作系统内核程序又会把CPU的使用权还给用户程序(即CPU状态会从核心态回到用户态)。这么设计的目的是:用户程序不能直接执行对系统影响非常大的操作，必须通过系统调用的方式请求操作系统代为执行，以便保证系统的稳定性和安全性，防止用户程序随意更改或访问重要的系统资源，影响其他进程的运行。</p>
<p>这样，操作系统的运行环境就可以理解为:用户通过操作系统运行上层程序（如系统提供的命令解释程序或用户自编程序)，而这个上层程序的运行依赖于操作系统的底层管理程序提供服务支持，当需要管理程序服务时，系统则通过硬件中断机制进入核心态，运行管理程序;也可能是程序运行出现异常情况，被动地需要管理程序的服务，这时就通过异常处理来进入核心态。管理程序运行结束时，用户程序需要继续运行，此时通过相应的保存的程序现场退出中断处理程序或异常处理程序，返回断点处继续执行，如图1.4所示。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210726162621.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210726162621.png';" /></details>


<p>在操作系统这一层面上，我们关心的是系统核心态和用户态的软件实现与切换，对于硬件层面的具体理解，可以结合“计算机组成原理”课程中有关中断的内容进行学习。</p>
<p>下面列举一些由用户态转向核心态的例子:</p>
<p>1）用户程序要求操作系统的服务，即系统调用。</p>
<p>2）发生一次中断。</p>
<p>3）用户程序中产生了一个错误状态。</p>
<p>4）用户程序中企图执行一条特权指令。</p>
<p>5）从核心态转向用户态由一条指令实现，这条指令也是特权命令，一般是中断返回指令。</p>
<blockquote>
<p>注意:由用户态进入核心态，不仅状态需要切换，而且所用的堆栈也可能需要由用户堆栈切换为系统堆栈，但这个系统堆栈也是属于该进程的。</p>
</blockquote>
<p>若程序的运行由用户态转到核心态，则会用到访管指令，访管指令是在用户态使用的，所以它不可能是特权指令。</p>
<h3 id="操作系统的体系结构"><a href="#操作系统的体系结构" class="headerlink" title="操作系统的体系结构"></a>操作系统的体系结构</h3><h4 id="大内核和微内核"><a href="#大内核和微内核" class="headerlink" title="大内核和微内核"></a>大内核和微内核</h4><p>操作系统的体系结构是一个开放的问题。如上文所述，操作系统在核心态为应用程序提供公共的服务，那么操作系统在核心态应该提供什么服务、怎样提供服务﹖有关这一问题的回答形成了两种主要的体系结构:大内核和微内核。</p>
<p>大内核系统将操作系统的主要功能模块都作为一个紧密联系的整体运行在核心态，从而为应用提供高性能的系统服务。因为各管理模块之间共享信息，能有效利用相互之间的有效特性.所以具有无可比拟的性能优势。</p>
<p>但随着体系结构和应用需求的不断发展，需要操作系统提供的服务越来越多，而且接口形式越来越复杂，操作系统的设计规模急剧增长，操作系统也面临着“软件危机”困境。为此，操作系统设计人员试图按照复杂性、时间常数、抽象级别等因素，将操作系统内核分成基本进程管理、虚存、IO与设备管理、IPC、文件系统等几个层次，继而定义层次之间的服务结构，提高操作系统内核设计上的模块化。但是，由于层次之间的交互关系错综复杂，定义清晰的层次间接口非常困难，复杂的交互关系也使得层次之间的界限极其模糊。</p>
<p>为解决操作系统的内核代码难以维护的问题，提出了微内核的体系结构。它将内核中最基本的功能（如进程管理等）保留在内核，而将那些不需要在核心态执行的功能移到用户态执行,从而降低了内核的设计复杂性。那些移出内核的操作系统代码根据分层的原则被划分成若干服务程序，它们的执行相互独立，交互则都借助于微内核进行通信。</p>
<p>微内核结构有效地分离了内核与服务、服务与服务，使得它们之间的接口更加清晰，维护的代价大大降低，各部分可以独立地优化和演进，从而保证了操作系统的可靠性。</p>
<p>微内核结构的最大问题是性能问题，因为需要频繁地在核心态和用户态之间进行切换，操作系统的执行开销偏大。因此有的操作系统将那些频繁使用的系统服务又移回内核，从而保证系统性能。但相当多的实验数据表明，体系结构不是引起性能下降的主要因素，体系结构带来的性能提升足以弥补切换开销带来的缺陷。为减少切换开销，也有人提出将系统服务作为运行库链接到用户程序的一种解决方案，这样的体系结构称为$\color{green}{\text{库操作系统}}$。</p>
<blockquote>
<p><code>体系结构带来的性能提升足以弥补切换开销带来的缺陷</code>在说啥?</p>
</blockquote>
<h3 id="本章疑难点"><a href="#本章疑难点" class="headerlink" title="本章疑难点"></a>本章疑难点</h3><h4 id="并行性与并发性的区别和联系"><a href="#并行性与并发性的区别和联系" class="headerlink" title="并行性与并发性的区别和联系"></a>并行性与并发性的区别和联系</h4><p>并行性和并发性是既相似又有区别的两个概念。并行性是指两个或多个事件在同一时刻发生，并发性是指两个或多个事件在同一时间间隔内发生。</p>
<p>在多道程序环境下，并发性是指在一段时间内，宏观上有多个程序同时运行，但在单处理器系统中每个时刻却仅能有一道程序执行，因此微观上这些程序只能分时地交替执行。若在计算机系统中有多个处理器，则这些可以并发执行的程序便被分配到多个处理器上，实现并行执行，即利用每个处理器来处理一个可并发执行的程序。</p>
<blockquote>
<p>咬文嚼字一下，并行依靠多处理器支持，如果两个任务挂在到两个不同的处理器那么就能并行执行<br>考虑java的线程机制，一个继承了thread的线程，在调用start的时候$\color{green}{\text{启动}}$一个线程，那么就实现了并$\color{green}{\text{发}}$(发车，启动)<br>java <a target="_blank" rel="noopener" href="https://www.cnblogs.com/agilestyle/p/11421515.html">run和start的区别</a></p>
</blockquote>
<h4 id="特权指令与非特权指令"><a href="#特权指令与非特权指令" class="headerlink" title="特权指令与非特权指令"></a>特权指令与非特权指令</h4><p>所谓特权指令，是指有特殊权限的指令，由于这类指令的权限最大，使用不当将导致整个系统崩溃，如清内存、置时钟、分配系统资源、修改虚存的段表或页表、修改用户的访问权限等。若所有程序都能使用这些指令，则系统一天死机n次就不足为奇。为保证系统安全，这类指令只能用于操作系统或其他系统软件，不直接提供给用户使用。因此，特权指令必须在核心态执行。实际上，CPU在核心态下可以执行指令系统的全集。形象地说，特权指令是那些儿童不宜的东西，而非特权指令是老少皆宜的东西。</p>
<p>为了防止用户程序中使用特权指令，用户态下只能使用非特权指令，核心态下可以使用全部指令。在用户态下使用特权指令时，将产生中断以阻止用户使用特权指令。所以把用户程序放在用户态下运行，而操作系统中必须使用特权指令的那部分程序在核心态下运行，保证了计算机系<br>统的安全可靠。从用户态转换为核心态的唯一途径是中断或异常。</p>
<h4 id="访管指令与访管中断"><a href="#访管指令与访管中断" class="headerlink" title="访管指令与访管中断"></a>访管指令与访管中断</h4><p>访管指令是一条可以在用户态下执行的指令。在用户程序中，因要求操作系统提供服务而有意识地使用访管指令，从而产生一个中断事件（自愿中断)，将操作系统转换为核心态，称为访管中断。访管中断由访管指令产生，程序员使用访管指令向操作系统请求服务。</p>
<p>为什么要在程序中引入访管指令呢?这是因为用户程序只能在用户态下运行。若用户程序想要完成在用户态下无法完成的工作，该怎么办﹖解决这个问题要靠访管指令。访管指令本身不是特权指令，其基本功能是让程序拥有“自愿进管”的手段，从而引起访管中断。</p>
<p>处于用户态的用户程序使用访管指令时，系统根据访管指令的操作数执行访管中断处理程序，访管中断处理程序将按系统调用的操作数和参数转到相应的例行子程序。完成服务功能后，退出中断，返回到用户程序断点继续执行。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://ednow.github.io/2021/07/24/%E7%8E%8B%E9%81%93-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-ch6-%E5%BA%94%E7%94%A8%E5%B1%82/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ednow">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ednow">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/24/%E7%8E%8B%E9%81%93-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-ch6-%E5%BA%94%E7%94%A8%E5%B1%82/" class="post-title-link" itemprop="url">王道-计算机网络-ch6-应用层</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-07-24 09:09:49" itemprop="dateCreated datePublished" datetime="2021-07-24T09:09:49+08:00">2021-07-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-10-10 22:16:56" itemprop="dateModified" datetime="2021-10-10T22:16:56+08:00">2021-10-10</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%80%83%E7%A0%94/" itemprop="url" rel="index"><span itemprop="name">考研</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="王道"><a href="#王道" class="headerlink" title="王道"></a>王道</h1><h2 id="应用层"><a href="#应用层" class="headerlink" title="应用层"></a>应用层</h2><p>【考纲内容】</p>
<p>(一）网络应用模型</p>
<p>客户/服务器模型;P2P模型</p>
<p>(二)域名系统(DNS)</p>
<p>层次域名空间;域名服务器;域名解析过程</p>
<p>(三）文件传输协议(FTP)</p>
<p>FTP的工作原理;控制连接与数据连接</p>
<p>(四）电子邮件（E-mail)</p>
<p>电子邮件系统的组成结构;电子邮件格式与MIME;SMTP与POP3</p>
<p>(五）万维网( www)</p>
<p>wwW的概念与组成结构;HTTP</p>
<p>【复习提示】</p>
<p>本章内容既可以以选择题的形式考查，也可以结合其他章节的内容出综合题。所以牢固掌握本章的几个典型应用层协议是关键。我们生活中的很多网络应用都是建立在这些协议的基础上的，因此在学习时要注意联系实际，提高学习的兴趣，才会获得更好的效果。</p>
<h3 id="网络应用模型"><a href="#网络应用模型" class="headerlink" title="网络应用模型"></a>网络应用模型</h3><h4 id="客户-服务器模型"><a href="#客户-服务器模型" class="headerlink" title="客户/服务器模型"></a>客户/服务器模型</h4><p>在客户/服务器(Client/Server，C/S）模型中，有一个总是打开的主机称为服务器，它服务许多来自其他称为客户机的主机请求。其工作流程如下:</p>
<p>1）服务器处于接收请求的状态。</p>
<p>2）客户机发出服务请求，并等待接收结果。</p>
<p>3）服务器收到请求后，分析请求，进行必要的处理，得到结果并发送给客户机。</p>
<p>客户程序必须知道服务器程序的地址，客户机上一般不需要特殊的硬件和复杂的操作系统。而服务器上运行的软件则是专门用来提供某种服务的程序，可同时处理多个远程或本地客户的要求。系统启动后即自动调用并一直不断地运行着，被动地等待并接收来自各地客户的请求。因此，服务器程序不需要知道客户程序的地址。</p>
<p>客户/服务器模型最主要的特征是:客户是服务请求方，服务器是服务提供方。如 Web应用程序，其中总是打开的 Web服务器服务于运行在客户机上的浏览器的请求。当Web服务器接收到来自客户机对某对象的请求时，它向该客户机发送所请求的对象以做出响应。常见的使用客户/服务器模型的应用包括Web、文件传输协议（FTP)、远程登录和电子邮件等。</p>
<p>客户/服务器模型的主要特点还有:</p>
<p>1）网络中各计算机的地位不平等，服务器可以通过对用户权限的限制来达到管理客户机的目的，使它们不能随意存储/删除数据，或进行其他受限的网络活动。整个网络的管理工作由少数服务器担当，因此网络的管理非常集中和方便。</p>
<p>2）客户机相互之间不直接通信。例如，在 Web应用中两个浏览器并不直接通信。</p>
<p>3）可扩展性不佳。受服务器硬件和网络带宽的限制，服务器支持的客户机数有限。</p>
<h4 id="P2P模型"><a href="#P2P模型" class="headerlink" title="P2P模型"></a>P2P模型</h4><p>不难看出，在C/S模型中(见图6.1)，服务器性能的好坏决定了整个系统的性能，当大量用户请求服务时，服务器就必然成为系统的瓶颈。P2P模型（见图6.2）的思想是整个网络中的传输内容不再被保存在中心服务器上，每个结点都同时具有下载、上传的功能，其权利和义务都是大体对等的。</p>
<details><summary>图6.1客户/服务器模型，图6.2 P2P模型</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005140045.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005140045.png';" /></details>

<p>在P2P模型中，各计算机没有固定的客户和服务器划分。相反，任意一对计算机一-称为对等方(Peer)，直接相互通信。实际上，P2P模型从本质上来看仍然使用客户/服务器方式，每个结点既作为客户访问其他结点的资源,也作为服务器提供资源给其他结点访问。当前比较流行的P2P应用有PPlive、Bittorrent和电驴等。</p>
<p>与C/S模型相比，P2P模型的优点主要体现如下:</p>
<p>1）减轻了服务器的计算压力，消除了对某个服务器的完全依赖，可以将任务分配到各个结点上，因此大大提高了系统效率和资源利用率（例如，播放流媒体时对服务器的压力过大，而通过P2P模型，可以利用大量的客户机来提供服务)。</p>
<p>2）多个客户机之间可以直接共享文档。</p>
<p>3）可扩展性好，传统服务器有响应和带宽的限制，因此只能接受一定数量的请求。4)网络健壮性强，单个结点的失效不会影响其他部分的结点。</p>
<p>P2P模型也有缺点。在获取服务的同时，还要给其他结点提供服务，因此会占用较多的内存，影响整机速度。例如，经常进行P2P下载还会对硬盘造成较大的损伤。据某互联网调研机构统计，当前P2P程序已占互联网50%～90%的流量，使网络变得非常拥塞，因此各大ISP（互联网服务提供商，如电信、网通等）通常都对P2P应用持反对态度。</p>
<h3 id="域名系统（DNS"><a href="#域名系统（DNS" class="headerlink" title="域名系统（DNS)"></a>域名系统（DNS)</h3><p>域名系统(Domain Name System，DNS）是因特网使用的命名系统，用来把便于人们记忆的具有特定含义的主机名(如 <a target="_blank" rel="noopener" href="http://www.cskaoyan.com)转换为便于机器处理的p地址.相对于ip地址,人们更喜欢使用具有特定含义的字符串来标识因特网上的计算机.值得注意的是,dns/">www.cskaoyan.com)转换为便于机器处理的P地址。相对于IP地址，人们更喜欢使用具有特定含义的字符串来标识因特网上的计算机。值得注意的是，DNS</a> 系统采用客户/服务器模型，其协议运行在UDP之上，使用53号端口。</p>
<p>从概念上可将DNS 分为3部分:层次域名空间、域名服务器和解析器。</p>
<h4 id="层次域名空间"><a href="#层次域名空间" class="headerlink" title="层次域名空间"></a>层次域名空间</h4><p>因特网采用层次树状结构的命名方法。采用这种命名方法，任何一个连接到因特网的主机或路由器，都有一个唯一的层次结构名称，即域名(Domain Name)。域(Domain）是名字空间中一个可被管理的划分。域还可以划分为子域，而子域还可以继续划分为子域的子域，这样就形成了顶级域、二级域、三级域等。每个域名都由标号序列组成，而各标号之间用点(“.”)隔开。一个典型的例子如图6.3所示，它是王道论坛用于提供wwW服务的计算机 （Web服务器）的域名，它由三个标号组成，其中标号com是顶级域名，标号cskaoyan是二级域名，标号www是三级域名。</p>
<details><summary>图6.3
一个域名的例子</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005140241.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005140241.png';" /></details>

<p>关于域名中的标号有以下几点需要注意:</p>
<p>1）标号中的英文不区分大小写。</p>
<p>2）标号中除连字符(-）外不能使用其他的标点符号。</p>
<p>3）每个标号不超过63个字符，多标号组成的完整域名最长不超过255个字符。4）级别最低的域名写在最左边，级别最高的顶级域名写在最右边。</p>
<p>顶级域名(Top Level Domain，TLD）分为如下三大类:</p>
<p>1）国家顶级域名(nTLD)。国家和某些地区的域名，如“.cn”表示中国，“.us”表示美国，“.uk”表示英国。</p>
<p>2）通用顶级域名(gTLD)。常见的有“.com”(公司)、“.net”(网络服务机构)、“.org”(非营利性组织）和“.gov”(国家或政府部门）等。</p>
<p>3）基础结构域名。这种顶级域名只有一个，即 arpa，用于反向域名解析，因此又称反向域名。</p>
<p>国家顶级域名下注册的二级域名均由该国家自行确定。图6.4展示了域名空间的树状结构。</p>
<details><summary>图6.4域名空间的树状结构
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005140352.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005140352.png';" /></details>


<p>在域名系统中，每个域分别由不同的组织进行管理。每个组织都可以将它的域再分成一定数目的子域，并将这些子域委托给其他组织去管理。例如，管理CN域的中国将EDU.CN子域授权给中国教育和科研计算机网(CERNET）来管理。</p>
<h4 id="域名服务器"><a href="#域名服务器" class="headerlink" title="域名服务器"></a>域名服务器</h4><p>因特网的域名系统被设计成一个联机分布式的数据库系统，并采用客户/服务器模型。域名到IP地址的解析是由运行在域名服务器上的程序完成的，一个服务器所负责管辖的(或有权限的范围称为区（不以“域”为单位)，各单位根据具体情况来划分自己管辖范围的区，但在一个<br>中的所有结点必须是能够连通的，每个区设置相应的仪限跌白服方茄充ID城外的解析，而且还必机的域名到IP地址的映射。每个域名服务器不但能够进行一些域名到IP地址的解析，而且还必须具有连向其他域名服务器的信息。当自己不能进行域名到IP地址的转换时，能够知道到什么地方去找其他域名服务器。</p>
<p>DNS使用了大量的域名服务器，它们以层次方式组织。没有一台域名服务器具有因特网上所有主机的映射，相反，该映射分布在所有的 DNS上。采用分布式设计的 DNS，是一个在因特网上实现分布式数据库的精彩范例。主要有4种类型的域名服务器。</p>
<h5 id="根域名服务器"><a href="#根域名服务器" class="headerlink" title="根域名服务器"></a>根域名服务器</h5><p>根域名服务器是最高层次的域名服务器，所有的根域名服务器都知道所有的顶级域名服务器的IP地址。根域名服务器也是最重要的域名服务器，不管是哪个本地域名服务器，若要对因特网上任何一个域名进行解析，只要自己无法解析，就首先要求助于根域名服务器。因特网上有13个根域名服务器，尽管我们将这13个根域名服务器中的每个都视为单个服务器，但每个“服务器”实际上是冗余服务器的集群，以提供安全性和可靠性。需要注意的是，根域名服务器用来管辖顶级域(如.com)，通常它并不直接把待查询的域名直接转换成P地址，而是告诉本地域名服务器下一步应当找哪个顶级域名服务器进行查询。</p>
<h5 id="顶级域名服务器"><a href="#顶级域名服务器" class="headerlink" title="顶级域名服务器"></a>顶级域名服务器</h5><p>这些域名服务器负责管理在该顶级域名服务器注册的所有二级域名。收到 DNS查询请求时，就给出相应的回答（可能是最后的结果，也可能是下一步应当查找的域名服务器的IP地址)。</p>
<h5 id="授权域名服务器（权限域名服务器"><a href="#授权域名服务器（权限域名服务器" class="headerlink" title="授权域名服务器（权限域名服务器)"></a>授权域名服务器（权限域名服务器)</h5><p>每台主机都必须在授权域名服务器处登记。为了更加可靠地工作，一台主机最好至少有两个授权域名服务器。实际上，许多域名服务器都同时充当本地域名服务器和授权域名服务器。授权域名服务器总能将其管辖的主机名转换为该主机的IP地址。</p>
<h5 id="本地域名服务器"><a href="#本地域名服务器" class="headerlink" title="本地域名服务器"></a>本地域名服务器</h5><p>本地域名服务器对域名系统非常重要。每个因特网服务提供者(ISP)，或一所大学，甚至一所大学中的各个系，都可以拥有一个本地域名服务器。当一台主机发出 DNS 查询请求时，这个查询请求报文就发送给该主机的本地域名服务器。事实上，我们在Windows系统中配置“本地连接”时，就需要填写DNS地址，这个地址就是本地DNS（域名服务器）的地址。</p>
<p>DNS的层次结构如图6.5所示。</p>
<details><summary>图6.5 DNS的层次结构</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005140753.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005140753.png';" /></details>

<h4 id="域名解析过程"><a href="#域名解析过程" class="headerlink" title="域名解析过程"></a>域名解析过程</h4><p>域名解析是指把域名映射成为P地址或把P地址映射成域名的过程。前者称为正向解析，后者称为反向解析。当客户端需要域名解析时，通过本机的DNS客户端构造一个DNS请求报文，以UDP数据报方式发往本地域名服务器。</p>
<p>域名解析有两种方式:递归查询和递归与迭代相结合的查询。</p>
<p>递归查询的过程如图 6.6(a)所示，本地域名服务器只需向根域名服务器查询一次，后面的几次查询都是递归地在其他几个域名服务器之间进行的[ 步骤③~⑥ ]。在步骤⑦中，本地域名服务器从根域名服务器得到了所需的I地址，最后在步骤⑧中，本地域名服务器把查询结果告诉发起查询的主机。由于该方法给根域名服务造成的负载过大，所以在实际中几乎不使用。</p>
<p>常用递归与迭代相结合的查询方式如图6.6(b)所示，该方式分为两个部分。</p>
<h5 id="主机向本地域名服务器的查询采用的是递归查询"><a href="#主机向本地域名服务器的查询采用的是递归查询" class="headerlink" title="主机向本地域名服务器的查询采用的是递归查询"></a>主机向本地域名服务器的查询采用的是递归查询</h5><p>也就是说，如果本地主机所询问的本地域名服务器不知道被查询域名的P地址，那么本地域名服务器就以DNS客户的身份，向根域名服务器继续发出查询请求报文（即替该主机继续查询)，而不是让该主机自己进行下一步的查询。两种查询方式的这一步是相同的。</p>
<details><summary>图6.6两种域名解析方式工作原理</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005140853.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005140853.png';" /></details>

<h5 id="本地域名服务器向根域名服务器的查询采用迭代查询"><a href="#本地域名服务器向根域名服务器的查询采用迭代查询" class="headerlink" title="本地域名服务器向根域名服务器的查询采用迭代查询"></a>本地域名服务器向根域名服务器的查询采用迭代查询</h5><p>当根域名服务器收到本地域名服务器发出的迭代查询请求报文时，要么给出所要查询的IP地址，要么告诉本地域名服务器:“你下一步应当可哪个坝级域石服?同‘样，而级域名服务器收域名服务器向这个顶级域名服务器进行后续的食闻，如图 0.0(D)P7g下一步成向哪个权限域名到查询报文后，要么给出所要查询的P地址，要么告诉本地域名服方’I代纪态海的主机。服务器查询。最后，知道所要解析的域名的P地址后，把这个结果返回给发起查询的主机。</p>
<p>下面举例说明域名解析的过程。假定某客户机想获知域名为y.abc.com主机的IP地址，域名解析的过程（(共使用了8个UDP报文）如下:</p>
<p>${\textstyle\unicode{x2460}}$  客户机向其本地域名服务器发出 DNS请求报文（递归查询)。</p>
<p>②本地域名服务器收到请求后，查询本地缓存，若没有该记录，则以DNS客户的身份向根域名服务器发出解析请求报文（迭代查询)。</p>
<p>③根域名服务器收到请求后，判断该域名属于.com 域，将对应的顶级域名服务器dns.com的IP地址返回给本地域名服务器。</p>
<p>④本地域名服务器向顶级域名服务器dns.com发出解析请求报文（迭代查询)。</p>
<p>${\textstyle\unicode{x2464}}$  顶级域名服务器dns.com收到请求后，判断该域名属于abc.com域，因此将对应的授权域名服务器dns.abc.com的IP地址返回给本地域名服务器。</p>
<p>⑥本地域名服务器向授权域名服务器dns.abc.com发起解析请求报文（迭代查询)。</p>
<p>⑦授权域名服务器dns.abc.com收到请求后，将查询结果返回给本地域名服务器。</p>
<p>${\textstyle\unicode{x2467}}$  本地域名服务器将查询结果保存到本地缓存，同时返回给客户机。</p>
<p>为了提高DNS的查询效率，并减少因特网上的 DNS查询报文数量，在域名服务器中广泛地使用了高速缓存。当一个 DNS服务器接收到 DNS查询结果时，它能将该DNS信息缓存在高速缓存中。这样，当另一个相同的域名查询到达该DNS服务器时，该服务器就能够直接提供所要求的IP地址，而不需要再去向其他DNS 服务器询问。因为主机名和IP地址之间的映射不是永久的，所以DNS 服务器将在一段时间后丢弃高速缓存中的信息。</p>
<h3 id="文件传输协议（FTP）"><a href="#文件传输协议（FTP）" class="headerlink" title="文件传输协议（FTP）"></a>文件传输协议（FTP）</h3><h4 id="FTP的工作原理"><a href="#FTP的工作原理" class="headerlink" title="FTP的工作原理"></a>FTP的工作原理</h4><p>文件传输协议（File Transfer Protocol，FTP)是因特网上使用得最广泛的文件传输协议。提供交互式的访问，允许客户指明文件的类型与格式，并允许文件具有存取权限。它屏蔽了各计算机系统的细节，因而适合于在异构网络中的任意计算机之间传送文件。</p>
<p>FTP提供以下功能:</p>
<p>①提供不同种类主机系统（硬、软件体系等都可以不同)之间的文件传输能力。</p>
<p>②以用户权限管理的方式提供用户对远程FTP服务器上的文件管理能力。</p>
<p>③以匿名FTP的方式提供公用文件共享的能力。</p>
<p>FTP采用客户!服务器的工作方式，它使用TCP可靠的传输服务。一个FTP服务器进程可同时为多个客户进程提供服务。FTP的服务器进程由两大部分组成:一个主进程，负责接收新的请求;另外有若干从属进程，负责处理单个请求。其工作步骤如下:</p>
<p>①打开熟知端口21（控制端口)，使客户进程能够连接上。②等待客户进程发连接请求。</p>
<p>③启动从属进程来处理客户进程发来的请求。主进程与从属进程并发执行，从属进程对客户进程的请求处理完毕后即终止。</p>
<p>④回到等待状态，继续接收其他客户进程的请求。</p>
<p>FTP服务器必须在整个会话期间保留用户的状态信息。特别是服务器必须把指定的用户账户与控制连接联系起来，服务器必须追踪用户在远程目录树上的当前位置。</p>
<h4 id="控制连接与数据连接"><a href="#控制连接与数据连接" class="headerlink" title="控制连接与数据连接"></a>控制连接与数据连接</h4><p>FTP在工作时使用两个并行的TCP连接（见图6.7):一个是控制连接（端口号21)，一个是数据连接（端口号20)。使用两个不同的端口号可使协议更加简单和更容易实现。</p>
<details><summary>图6.7控制连接和数据连接</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005141352.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005141352.png';" /></details>

<h5 id="控制连接"><a href="#控制连接" class="headerlink" title="控制连接"></a>控制连接</h5><p>服务器监听21号端口，等待客户连接，建立在这个端口上的连接称为控制连接，控制连接用来传输控制信息（如连接请求、传送请求等)，并且控制信息都以7位ASCII格式传送。FTP客户发出的传送请求，通过控制连接发送给服务器端的控制进程，但控制连接并不用来传送文件在传输文件时还可以使用控制连接（如客户在传输中途发一个中止传输的命令)，因此控制连接在整个会话期间一直保持打开状态。</p>
<h5 id="数据连接"><a href="#数据连接" class="headerlink" title="数据连接"></a>数据连接</h5><p>服务器端的控制进程在接收到FTP客户发来的文件传输请求后，就创建“数据传送进程”和“数据连接”。数据连接用来连接客户端和服务器端的数据传送进程，数据传送进程实际完成文件的传送，在传送完毕后关闭“数据传送连接”并结束运行。</p>
<p>数据连接有两种传输模式:主动模式PORT 和被动模式PASV。PORT 模式的工作原理:客户端连接到服务器的21端口，登录成功后要读取数据时，客户端随机开放一个端口，并发送命令告知服务器，服务器收到PORT 命令和端口号后，通过20端口和客户端开放的端口连接，发送数据。PASV模式的不同点是，客户端要读取数据时，发送PASV命令到服务器，服务器在本地随机开放一个端口，并告知客户端，客户端再连接到服务器开放的端口进行数据传输。可见，是用PORT模式还是PASV模式,选择权在客户端。简单概括为，主动模式传送数据是“服务器”连接到“客户端”的端口;被动模式传送数据是“客户端”连接到“服务器”的端口。</p>
<p>因为FTP使用了一个分离的控制连接，所以也称FTP的控制信息是带外（Out-of-band)传送的。使用FTP时，若要修改服务器上的文件，则需要先将此文件传送到本地主机，然后再将修改后的文件副本传送到原服务器，来回传送耗费很多时间。网络文件系统(NFS）采用另一种思路，它允许进程打开一个远程文件，并能在该文件的某个特定位置开始读写数据。这样，NFS可使用户复制一个大文件中的一个很小的片段，而不需要复制整个大文件。</p>
<h3 id="电子邮件"><a href="#电子邮件" class="headerlink" title="电子邮件"></a>电子邮件</h3><h4 id="电子邮件系统的组成结构"><a href="#电子邮件系统的组成结构" class="headerlink" title="电子邮件系统的组成结构"></a>电子邮件系统的组成结构</h4><p>自从有了因特网，电子邮件就在因特网上流行起来。电子邮件是一种异步通信方式，通信时不需要双方同时在场。电子邮件把邮件发送到收件人使用的邮件服务器，并放在其中的收件人邮箱中，收件人可以随时上网到自己使用的邮件服务器进行读取。</p>
<p>一个电子邮件系统应具有图6.8所示的三个最主要的组成构件，即用户代理（User Agent)、邮件服务器和电子邮件使用的协议，如 SMTP、POP3（或IMAP）等。</p>
<details><summary>图6.8电子邮件系统最主要的组成构件</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005141605.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005141605.png';" /></details>

<p>用户代理(UA): 用户与电子邮件系统的接口。用户代理向用户提供-一个很友好的接口来发送和接收邮件，用户代理至少应当具有撰写、显示和邮件处理的功能。通常情况下，用户代理就是一个运行在PC.上的程序(电子邮件客户端软件)，常见的有Outlook 和Foxmail等。</p>
<p>邮件服务器:它的功能是发送和接收邮件，同时还要向发信人报告邮件传送的情况(已交付、被拒绝、丢失等)。邮件服务器采用客户/服务器方式工作，但它必须能够同时充当客户和服务器。例如，当邮件服务器A向邮件服务器B发送邮件时，A就作为SMTP客户，而B是SMTP服务<br>器;反之，当B向A发送邮件时，B就是SMTP客户，而A就是SMTP服务器。</p>
<p>邮件发送协议和读取协议:邮件发送协议用于用户代理向邮件服务器发送邮件或在邮件服务器之间发送邮件,如SMTP;邮件读取协议用于用户代理从邮件服务器读取邮件，如POP3。注意，SMTP用的是“推”(Push)的通信方式，即用户代理向邮件服务器发送邮件及在邮件服务器之间<br>发送邮件时，SMTP客户将邮件“推”送到SMTP服务器。而POP3用的是“拉”(Pull)的通信方式，即用户读取邮件时，用户代理向邮件服务器发出请求，“拉”取用户邮箱中的邮件。</p>
<p>电子邮件的发送、接收过程可简化为如图6.9所示。</p>
<details><summary>图6.9电子邮件的发送、接收过程</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005141701.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005141701.png';" /></details>

<p>下面简单介绍电子邮件的收发过程。</p>
<p>①发信人调用用户代理来撰写和编辑要发送的邮件。用户代理用SMTP把邮件传送给发送端邮件服务器。</p>
<p>②发送端邮件服务器将邮件放入邮件缓存队列中，等待发送。运行在发送端邮件服务器的SMTP客户进程，发现邮件缓存中有待发送的邮件，就向运行在接收端邮件服务器的SMTP服务器进程发起建立TCP连接。</p>
<p>④TCP连接建立后，SMTP客户进程开始向远程SMTP服务器进程发送邮件。当所有待发送邮件发完后，SMTP就关闭所建立的TCP连接。</p>
<p>${\textstyle\unicode{x2464}}$  运行在接收端邮件服务器中的SMTP服务器进程收到邮件后，将邮件放入收信人的用户邮箱，等待收信人在方便时进行读取。</p>
<p>⑥收信人打算收信时，调用用户代理，使用POP3(或IMAP）协议将自己的邮件从接收端邮件服务器的用户邮箱中取回（如果邮箱中有来信的话)。</p>
<h4 id="电子邮件格式与-MIME"><a href="#电子邮件格式与-MIME" class="headerlink" title="电子邮件格式与 MIME"></a>电子邮件格式与 MIME</h4><h5 id="电子邮件格式"><a href="#电子邮件格式" class="headerlink" title="电子邮件格式"></a>电子邮件格式</h5><p>一个电子邮件分为信封和内容两大部分，邮件内容又分为 $\color{green}{\text{首部}}$ 和 $\color{green}{\text{主体}}$ 两部分。RFC 822规定了邮件的首部格式，而邮件的主体部分则让用户自由撰写。用户写好首部后，邮件系统自动地将信封所需的信息提取出来并写在信封上，用户不需要亲自填写信封上的信息。</p>
<p>邮件内容的首部包含一些首部行，每个首部行由一个关键字后跟冒号再后跟值组成。有些关键字是必需的，有些则是可选的。最重要的关键字是To:和 Subject:。</p>
<p>To是必需的关键字，后面填入一个或多个收件人的电子邮件地址。电子邮件地址的规定格式为:收件人邮箱名@邮箱所在主机的域名，如 <a href="mailto:&#97;&#x62;&#x63;&#x40;&#x63;&#115;&#107;&#x61;&#111;&#x79;&#x61;&#110;&#x2e;&#99;&#x6f;&#x6d;">&#97;&#x62;&#x63;&#x40;&#x63;&#115;&#107;&#x61;&#111;&#x79;&#x61;&#110;&#x2e;&#99;&#x6f;&#x6d;</a>，其中收信人邮箱名即用户名.abc在cskaoyan.com这个邮件服务器上必须是唯一的。这也就保证了<a href="mailto:&#97;&#x62;&#99;&#x40;&#x63;&#x73;&#107;&#97;&#111;&#121;&#x61;&#x6e;&#46;&#99;&#111;&#x6d;">&#97;&#x62;&#99;&#x40;&#x63;&#x73;&#107;&#97;&#111;&#121;&#x61;&#x6e;&#46;&#99;&#111;&#x6d;</a>这个邮件地址在整个因特网上是唯一的。</p>
<p>Subject是可选关键字，是邮件的主题，反映了邮件的主要内容。</p>
<p>当然，还有一个必填的关键字是From，但它通常由邮件系统自动填入。首部与主体之间用一个空行进行分割。典型的邮件内容如下:</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005141850.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005141850.png';" /></details>

<h5 id="多用途网际邮件扩充-MIME"><a href="#多用途网际邮件扩充-MIME" class="headerlink" title="多用途网际邮件扩充(MIME)"></a>多用途网际邮件扩充(MIME)</h5><p>由于SMTP只能传送一定长度的ASCII码，许多其他非英语国家的文字(如中文、俄文，甚至带重音符号的法文或德文）就无法传送，且无法传送可执行文件及其他二进制对象，因此提出了多用途网络邮件扩充( Multipurpose Internet MailExtensions，MIME)。</p>
<p>MIME并未改动SMTP或取代它。MIME的意图是继续使用目前的格式，但增加了邮件主体的结构，并定义了传送非ASCII码的编码规则。也就是说，MIME邮件可在现有的电子邮件程序和协议下传送。MIME与SMTP的关系如图6.10所示。</p>
<p>MIME主要包括以下三部分内容:</p>
<p>①5个新的邮件首部字段，包括MIME 版本、内容描述、内容标识、传送编码和内容类型。</p>
<p>②定义了许多邮件内容的格式，对多媒体电子邮件的表示方法进行了标准化。</p>
<p>③定义了传送编码，可对任何内容格式进行转换，而不会被邮件系统改变。</p>
<details><summary>图6.10 SMTP与MIME的关系</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005141956.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005141956.png';" /></details>

<h4 id="SMTP-和-POP3"><a href="#SMTP-和-POP3" class="headerlink" title="SMTP 和 POP3"></a>SMTP 和 POP3</h4><h5 id="SMTP"><a href="#SMTP" class="headerlink" title="SMTP"></a>SMTP</h5><p>简单邮件传输协议（Simple Mail Transfer Protocol，SMTP）是一种提供可靠且有效的电子邮件传输的协议，它控制两个相互通信的SMTP进程交换信息。由于SMTP使用客户/服务器方式,因此负责发送邮件的SMTP进程就是SMTP客户,而负责接收邮件的SMTP进程就是SMTP服务器。SMTP用的是TCP连接，端口号为25。SMTP通信有以下三个阶段。</p>
<h6 id="连接建立"><a href="#连接建立" class="headerlink" title="连接建立"></a>连接建立</h6><p>发件人的邮件发送到发送方邮件服务器的邮件缓存中后，SMTP客户就每隔一定时间对邮件缓存扫描一次。如发现有邮件，就使用SMTP的熟知端口号(25）与接收方邮件服务器的SMTP服务器建立TCP连接。连接建立后，接收方SMTP服务器发出220 Service ready(服务就绪)。然后SMTP客户向SMTP服务器发送HELO命令，附上发送方的主机名。</p>
<p>SMTP不使用中间的邮件服务器。TCP连接总是在发送方和接收方这两个邮件服务器之间直接建立，而不管它们相隔多远，不管在传送过程中要经过多少个路由器。当接收方邮件服务器因故障暂时不能建立连接时，发送方的邮件服务器只能等待一段时间后再次尝试连接。</p>
<h6 id="邮件传送"><a href="#邮件传送" class="headerlink" title="邮件传送"></a>邮件传送</h6><p>连接建立后，就可开始传送邮件。邮件的传送从MAIL 命令开始，MAIL命令后面有发件.的地址。如 MAIL FROM:<a href="mailto:&#104;&#111;&#111;&#112;&#100;&#111;&#103;&#64;&#104;&#x75;&#115;&#116;&#46;&#101;&#x64;&#117;&#x2e;&#99;&#x6e;">&#104;&#111;&#111;&#112;&#100;&#111;&#103;&#64;&#104;&#x75;&#115;&#116;&#46;&#101;&#x64;&#117;&#x2e;&#99;&#x6e;</a>。若SMTP服务器已准备好接收邮件，则回250 OK。接着SMTP客户端发送一个或多个RCPT(收件人recipient 的缩写)命令，格式为RCPTO:&lt;收件人地址&gt;。每发送一个RCPT命令，都应有相应的信息从SMTP服务器返回，如 250 OK或550 No such user here（无此用户)。</p>
<p>RCPT命令的作用是，先弄清接收方系统是否已做好接收邮件的准备，然后才发送邮件，以便不至于发送了很长的邮件后才知道地址错误，进而避免浪费通信资源。</p>
<p>获得OK的回答后，客户端就使用DATA命令，表示要开始传输邮件的内容。正常情况下,SMTP服务器回复的信息是354 Start mail input; end with &lt; CRLF &gt;.&lt; CRLF &gt;。&lt; CRLF &gt;表示回车换行。此时SMTP客户端就可开始传送邮件内容，.并用&lt; CRLF &gt;.&lt; CRLF &gt;表示邮件内容的结束。</p>
<p>(3）连接释放</p>
<p>邮件发送完毕后，SMTP客户应发送QUIT命令。SMTP服务器返回的信息是221（服务关闭)，表示SMTP同意释放TCP连接。邮件传送的全部过程就此结束。</p>
<h6 id="连接释放"><a href="#连接释放" class="headerlink" title="连接释放"></a>连接释放</h6><p>邮件发送完毕后，SMTP客户应发送QUIT 命令。SMTP服务器返回的信息是221(服务关闭)，表示SMTP同意释放TCP连接。邮件传送的全部过程就此结束。</p>
<h5 id="POP3和IMAP"><a href="#POP3和IMAP" class="headerlink" title="POP3和IMAP"></a>POP3和IMAP</h5><p>邮局协议（Post Office Protocol，POP）是一个非常简单但功能有限的邮件读取协议，现在使用的是它的第3个版本POP3。POP3采用的是“拉”(Pull）的通信方式，当用户读取邮件时，用户代理向邮件服务器发出请求，“拉”取用户邮箱中的邮件。</p>
<p>POP也使用客户/服务器的工作方式，在传输层使用TCP，端口号为110。接收方的用户代理上必须运行POP客户程序，而接收方的邮件服务器上则运行POP服务器程序。POP有两种工作方式:“下载并保留”和“下载并删除”。在“下载并保留”方式下，用户从邮件服务器上读取邮件后，邮件依然会保存在邮件服务器上，用户可再次从服务器上读取该邮件;而使用“下载并删除”方式时，邮件一旦被读取，就被从邮件服务器上删除，用户不能再次从服务器上读取。</p>
<p>另一个邮件读取协议是因特网报文存取协议（IMAP)，它比POP复杂得多，IMAP为用户提供了创建文件夹、在不同文件夹之间移动邮件及在远程文件夹中查询邮件等联机命令,为此IMA服务器维护了会话用户的状态信息。IMAP的另一特性是允许用户代理只获取报文的某些部分，例如可以只读取一个报文的首部，或多部分MIME报文的一部分。这非常适用于低带宽的情况,用户可能并不想取回邮箱中的所有邮件，尤其是包含很多音频或视频的大邮件。</p>
<p>此外，随着万维网的流行，目前出现了很多基于万维网的电子邮件，如Hotmail、Gmail 这种电子邮件的特点是，用户浏览器与Hotmail或Gmail的邮件服务器之间的邮件发送或接收使用的是HTTP，而仅在不同邮件服务器之间传送邮件时才使用SMTP。</p>
<h3 id="万维网-WWW"><a href="#万维网-WWW" class="headerlink" title="万维网(WWW)"></a>万维网(WWW)</h3><h4 id="WWW的概念与组成结构"><a href="#WWW的概念与组成结构" class="headerlink" title="WWW的概念与组成结构"></a>WWW的概念与组成结构</h4><p>万维网（World Wide Web，wWW)是一个分布式、联机式的信息存储空间，在这个空间中:一样有用的事物称为一样“资源”，并由一个全域“统一资源定位符”(URL)标识。这些资源通过超文本传输协议（HTTP）传送给使用者，而后者通过单击链接来获取资源。</p>
<p>万维网使用链接的方法能非常方便地从因特网上的一个站点访问另一个站点（即“链接到另一个站点”)，从而主动地按需获取丰富的信息。超文本标记语言（HyperText Markup Language,HTML)使得万维网页面的设计者可以很方便地用一个超链接从本页面的某处链接到因特网上的任何一个万维网页面，并能够在自己的计算机屏幕上显示这些页面。</p>
<p>万维网的内核部分是由三个标准构成的:</p>
<p>1） $\color{green}{\text{统一资源定位符}}$ （URL)。负责标识万维网上的各种文档，并使每个文档在整个万维网的范围内具有唯一的标识符URL。</p>
<p>2） $\color{green}{\text{超文本传输协议}}$ （HTTP)。一个应用层协议，它使用TCP连接进行可靠的传输，HTTP是万维网客户程序和服务器程序之间交互所必须严格遵守的协议。</p>
<p>3） $\color{green}{\text{超文本标记语言}}$ （HTML)。一种文档结构的标记语言，它使用一些约定的标记对页面上的各种信息（包括文字、声音、图像、视频等)、格式进行描述。</p>
<p>URL是对可以从因特网上得到的资源的位置和访问方法的一种简洁表示。URL相当于一个文件名在网络范围的扩展。URL的一般形式是:</p>
<p style='text-align:center'> < 协议 >://< 主机 >:< 端口 >/< 路径 > </p>

<p>&lt;协议&gt;指用什么协议来获取万维网文档，常见的协议有 http、ftp 等;&lt;主机&gt;是存放资源的主机在因特网中的域名或P地址;&lt;端口&gt;和&lt;路径&gt;有时可省略。在URL 中不区分大小写。</p>
<p>万维网以客户/服务器方式工作。浏览器是在用户主机上的万维网客户程序，而万维网文档所驻留的主机则运行服务器程序，这台主机称为万维网服务器。客户程序向服务器程序发出请求服务器程序向客户程序送回客户所要的万维网文档。工作流程如下:</p>
<p>1）Web用户使用浏览器（指定URL)与Web服务器建立连接，并发送浏览请求。</p>
<p>2)Web服务器把URL转换为文件路径，并返回信息给Web浏览器。</p>
<p>3）通信完成，关闭连接。</p>
<p>万维网是无数个网络站点和网页的集合，它们在一起构成了因特网最主要的部分（因特网也包括电子邮件、Usenet和新闻组)。</p>
<h4 id="超文本传输协议（HTTP"><a href="#超文本传输协议（HTTP" class="headerlink" title="超文本传输协议（HTTP)"></a>超文本传输协议（HTTP)</h4><p>HTTP定义了浏览器（万维网客户进程）怎样向万维网服务器请求万维网文档，以及服务器怎样把文档传送给浏览器。从层次的角度看，HTTP是面向事务的（Transaction-oriented）应用层协议，它规定了在浏览器和服务器之间的请求和响应的格式与规则，是万维网上能够可靠地交换文件（包括文本、声音、图像等各种多媒体文件）的重要基础。</p>
<details><summary>图6.11万维网的工作过程
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005142512.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005142512.png';" /></details>

<h5 id="HTTP的操作过程"><a href="#HTTP的操作过程" class="headerlink" title="HTTP的操作过程"></a>HTTP的操作过程</h5><p>从协议执行过程来说，浏览器要访问wwW服务器时，首先要完成对 wWW服务器的域名解析。一旦获得了服务器的P地址，浏览器就通过TCP向服务器发送连接建立请求。</p>
<p>万维网的大致工作过程如图6.11所示。每个万维网站点都有一个服务器进程，它不断地监听TCP的端口80（默认)，当监听到连接请求后便与浏览器建立TCP连接。然后，浏览器就向服务器发送请求获取某个 Web页面的HTTP请求。服务器收到请求后，将构建所请求Web页的必需信息，并通过HTTP响应返回给浏览器。浏览器再将信息进行解释，然后将Web页显示给用户。最后，TCP连接释放。</p>
<p>在浏览器和服务器之间的请求与响应的交互，必须遵循规定的格式和规则，这些格式和规则就是HTTP。因此HTTP有两类报文:请求报文（从Web客户端向Web服务器发送服务请求)和响应报文（从Web 服务器对Web客户端请求的回答)。</p>
<p>用户单击鼠标后所发生的事件按顺序如下（以访问清华大学的网站为例);</p>
<p>1）浏览器分析链接指向页面的URL (<a target="_blank" rel="noopener" href="http://www.tsinghua.edu.cn/">http://www.tsinghua.edu.cn</a> /chn/index.htm)。</p>
<p>2）浏览器向DNS请求解析<a target="_blank" rel="noopener" href="http://www.tsinghua.edu.cn的ip地址./">www.tsinghua.edu.cn的IP地址。</a></p>
<p>3）域名系统DNS解析出清华大学服务器的IP地址。</p>
<p>4）浏览器与该服务器建立TCP连接（默认端口号为80)。</p>
<p>5）浏览器发出HTTP请求:GET /chn/index.htm。</p>
<p>6）服务器通过HTTP响应把文件index.htm发送给浏览器。</p>
<p>7）释放TCP连接。</p>
<p>8）浏览器解释文件index.htm，并将 Web页显示给用户。</p>
<h5 id="HTTP的特点"><a href="#HTTP的特点" class="headerlink" title="HTTP的特点"></a>HTTP的特点</h5><p>HTTP使用TCP作为运输层协议，保证了数据的可靠传输。HTTP不必考虑数据在传输过程中被丢弃后又怎样被重传。但是，HTTP本身是 $\color{green}{\text{无连接}}$ 的(务必注意)。也就是说，虽然HTTP使用了TCP连接，但通信的双方在交换HTTP报文之前不需要先建立HTTP连接。</p>
<p>HTTP是无状态的。也就是说，同一个客户第二次访问同一个服务器上的页面时，服务器的响应与第一次被访问时的相同。因为服务器并不记得曾经访问过的这个客户，也不记得为该客户曾经服务过多少次。</p>
<p>HTTP的无状态特性简化了服务器的设计，使服务器更容易支持大量并发的HTTP请求。在实际应用中，通常使用Cookie 加数据库的方式来跟踪用户的活动(如记录用户最近浏览的商品等)。Cookie的工作原理:当用户浏览某个使用Cookie 的网站时，该网站服务器就为用户产生一个唯一的识别码，如“123456”,接着在给用户的响应报文中添加一个Set-cookie的首部行“Setcookie: 123456”。用户收到响应后，就在它管理的特定Cookie文件中添加这个服务器的主机名和Cookie识别码，当用户继续浏览这个网站时，会取出这个网站的识别码，并放入请求报文的Cookie首部行“Cookie: 123456”，服务器根据请求报文中的Cookie识别码就能从数据库中查询到该用户的活动记录，进而执行一些个性化的工作，如根据用户的历史浏览记录向其推荐新产品等。</p>
<p>HTTP既可以使用非持久连接，也可以使用持久连接（HTTP/1.1支持)。</p>
<p>对于非持久连接，每个网页元素对象（如JPEG图形、Flash等)的传输都需要单独建立一个TCP连接，如图6.12所示(第三次握手的报文段中捎带了客户对万维网文档的请求)。也就是说，请求一个万维网文档所需的时间是该文档的传输时间(与文档大小成正比)加上两倍往返时间RTT(一个RTT用于TCP连接，另一个RTT用于请求和接收文档)。每个对象引用都导致2×RTT的开销，此外每次建立新的TCP连接都要分配缓存和变量，使万维网服务器的负担很重。</p>
<p>所谓持久连接，是指万维网服务器在发送响应后仍然保持这条连接，使同一个客户（浏览器)和该服务器可以继续在这条连接上传送后续的HTTP请求和响应报文，如图6.13所示。</p>
<details><summary>图6.12请求一个万维网文档所需的时间
图6.13 使用持久连接（非流水线)</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005143215.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005143215.png';" /></details>

<p>持久连接又分为非流水线和流水线两种方式。对于非流水线方式，客户在收到前一个响应后才能发出下一个请求，服务器发送完一个对象后，其TCP连接就处于空闲状态，浪费了服务器资源。HTTP/1.1 的默认方式是使用流水线的持久连接，这种情况下，客户每遇到一个对象引用就立即发出一个请求，因而客户可以逐个地连续发出对各个引用对象的请求。如果所有的请求和响应都是连续发送的，那么所有引用的对象共计经历1个RTT延迟，而不是像非流水线方式那样，每个引用都必须有1个RTT延迟。这种方式减少了TCP 连接中的空闲时间,提高了效率。</p>
<h5 id="HTTP的报文结构"><a href="#HTTP的报文结构" class="headerlink" title="HTTP的报文结构"></a>HTTP的报文结构</h5><p>HTTP是面向文本的（Text-Oriented)，因此报文中的每个字段都是一些ASCII码串，并且每个字段的长度都是不确定的。有两类HTTP报文:</p>
<ul>
<li>请求报文:从客户向服务器发送的请求报文，如图6.14(a)所示。</li>
<li>响应报文:从服务器到客户的回答，如图6.14(b)所示。</li>
</ul>
<details><summary>图6.14 HTTP的报文结构
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005143346.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005143346.png';" /></details>

<p>HTTP请求报文和响应报文都由三个部分组成。从图6.14可以看出，这两种报文格式的区别就是开始行不同。</p>
<p>开始行:用于区分是请求报文还是响应报文。在请求报文中的开始行称为请求行，而在响应报文中的开始行称为状态行。开始行的三个字段之间都以空格分隔，最后的“CR”和“LF”分别代表“回车”和“换行”。请求报文的“请求行”有三个内容:方法、请求资源的URL及HTTP的版本。其中，“方法”是对所请求对象进行的操作，这些方法实际上也就是一些命令。表6.1 给出了HTTP请求报文中常用的几个方法。</p>
<p>首部行:用来说明浏览器、服务器或报文主体的一些信息。首部可以有几行,但也可以不使用。在每个首部行中都有首部字段名和它的值，每一行在结束的地方都要有“回车”和“换行”。整个首部行结束时，还有一空行将首部行和后面的实体主体分开。</p>
<p>实体主体:在请求报文中一般不用这个字段，而在响应报文中也可能没有这个字段。图6.15所示为使用Wireshark捕获的HTTP请求报文的示例，下面结合前几章的内容对请求报文（图中下部分）进行分析。</p>
<p>根据帧的结构定义，在图6.15所示的以太网数据帧中，第1<del>6个字节为目的MAC地址（默认网关地址)，即 O0-Of-e2-3f-27-3f;第7</del>12个字节为本机 MAC地址，即00-27-13-67-73-8d;第13<del>14个字节08～00为类型字段，表示上层使用的是P数据报协议。第15</del>34个字节（共20B）为P数据报的首部，其中第27～30个字节为源P地址，即 db-df-d2-70，转换成十进制为219.223.210.112;第31~34个字节为目的P地址，即 71-69-4c-O0a,转换成十进制为113.105.78.10.第35～54个字节(共20B）为TCP报文段的首部。</p>
<p>从第55个字节开始才是TCP数据部分（阴影部分)，即从应用层传递下来的数据（本例中即请求报文)，GET对应请求行的方法，/face/20.gif对应请求行的URL，HTTP/1.1对应请求行的版本，左边数字是对应字符的ASCII码，如’G’=Ox47、’E’=Ox45、”T’=Ox54等。图6.15的请求报文中首部行字段内容的含义，建议读者自行了解，也可以自己动手抓包分析。</p>
<details><summary>表6.1 HTTP请求报文中常用的几个方法
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005143426.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005143426.png';" /></details>


<details><summary>图6.15使用 Wireshark捕获的HTTP请求报文的示例
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005143525.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005143525.png';" /></details>

<p>右下角开始的<code>“…??.&#39; .gs…E..%..@.@..0…pgi”</code>等是上面介绍过的第1～54个字节中对应的ASCII码字符，而这些字符在这里不代表任何意义。</p>
<p>常见应用层协议小结如表6.2所示。</p>
<details><summary>表6.2常见应用层协议小结
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005143555.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005143555.png';" /></details>

<h3 id="本章小结及疑难点"><a href="#本章小结及疑难点" class="headerlink" title="本章小结及疑难点"></a>本章小结及疑难点</h3><h4 id="如何理解客户进程端口号与服务器进程端口号"><a href="#如何理解客户进程端口号与服务器进程端口号" class="headerlink" title="如何理解客户进程端口号与服务器进程端口号?"></a>如何理解客户进程端口号与服务器进程端口号?</h4><p>通常我们所说的熟知端口号是指应用层协议在服务器端的默认端口号，而客户端进程的端口号是由客户端进程任意指定的（临时的)。</p>
<p>当客户进程向服务器进程发出建立连接请求时，要寻找连接服务器进程的熟知端口号，同时还要告诉服务器进程自己的临时端口号。接着，服务器进程就用自己的熟知端口号与客户进程所提供的端口号建立连接。</p>
<h4 id="互联网、因特网和万维网的区别是什么"><a href="#互联网、因特网和万维网的区别是什么" class="headerlink" title="互联网、因特网和万维网的区别是什么?"></a>互联网、因特网和万维网的区别是什么?</h4><p>互联网(（internet）泛指由多个计算机网络按照一定的通信协议相互连接而成的一个大型计算机网络。</p>
<p>因特网（Internet〉是指在ARPA网基础上发展而来的世界上最大的全球性互联网络。因特网和其他类似的由计算机相互连接而成的大型网络系统，都可算是“互联网”，因特网只是互联网中最大的一个。</p>
<p>万维网是无数个网络站点和网页的集合，它们一起构成了因特网最主要的部分。(因特网也包括电子邮件、Usenet和新闻组)。</p>
<h4 id="域名的高速缓存是什么"><a href="#域名的高速缓存是什么" class="headerlink" title="域名的高速缓存是什么?"></a>域名的高速缓存是什么?</h4><p>每个域名服务器都维护一个高速缓存，存放最近用过的名字以及从何处获得名字映射信息的记录，可大大减轻根域名服务器的负荷，使因特网上的 DNS 查询请求和回答报文的数量大为减少。为保持高速缓存中的内容正确，域名服务器应为每项内容设置计时器，并处理超过合理时间的项（如每个项目只存放两天)。当权限域名服务器回答一个查询请求时，在响应中都指明绑定有效存在的时间值。增加此时间值可减少网络开销，减少此时间值可以提高域名转换的准确性。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://ednow.github.io/2021/07/24/%E7%8E%8B%E9%81%93-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-ch5-%E4%BC%A0%E8%BE%93%E5%B1%82/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ednow">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ednow">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/24/%E7%8E%8B%E9%81%93-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-ch5-%E4%BC%A0%E8%BE%93%E5%B1%82/" class="post-title-link" itemprop="url">王道-计算机网络-ch5-传输层</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-07-24 09:09:39" itemprop="dateCreated datePublished" datetime="2021-07-24T09:09:39+08:00">2021-07-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-11-20 22:35:29" itemprop="dateModified" datetime="2021-11-20T22:35:29+08:00">2021-11-20</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%80%83%E7%A0%94/" itemprop="url" rel="index"><span itemprop="name">考研</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="王道"><a href="#王道" class="headerlink" title="王道"></a>王道</h1><h2 id="传输层"><a href="#传输层" class="headerlink" title="传输层"></a>传输层</h2><p>【考纲内容】</p>
<p>(一）传输层提供的服务</p>
<p>传输层的功能;传输层寻址与端口;无连接服务和面向连接服务</p>
<p>(二）UDP</p>
<p>UDP数据报;UDP校验</p>
<p>(三）TCP</p>
<p>TCP段;TCP连接管理;TCP可靠传输;TCP流量控制与拥塞控制</p>
<p>【复习提示】</p>
<p>传输层是整个网络体系结构中的关键层次。要求掌握传输层在计算机网络中的地位、功能、工作方式及原理等，掌握UDP 及 TCP（如首部格式、可靠传输、流量控制、拥塞控制、连接管理等)。其中，TCP报文分析、流量控制与拥塞控制机制，出选择题、综合题的概率均较大，因此要将其工作原理透彻掌握，以便能在具体的题目中灵活运用。</p>
<h3 id="传输层提供的服务"><a href="#传输层提供的服务" class="headerlink" title="传输层提供的服务"></a>传输层提供的服务</h3><h4 id="传输层的功能"><a href="#传输层的功能" class="headerlink" title="传输层的功能"></a>传输层的功能</h4><p>从通信和信息处理的角度看，传输层向它上面的应用层提供通信服务，它属于面向通信部分的最高层，同时也是用户功能中的最低层。</p>
<p>传输层位于网络层之上，它为运行在不同主机上的进程之间提供了逻辑通信，而网络层提供主机之间的逻辑通信。显然，即使网络层协议不可靠（网络层协议使分组丢失、混乱或重复)，传输层同样能为应用程序提供可靠的服务。</p>
<p>从图5.1可以看出，网络的边缘部分的两台主机使用网络核心部分的功能进行端到端的通信时，只有主机的协议栈才有传输层和应用层，而路由器在转发分组时都只用到下三层的功能〈即在通信子网中没有传输层，传输层只存在于通信子网以外的主机中)。</p>
<p>传输层的功能如下:</p>
<p>1）传输层提供应用进程之间的 $\color{green}{\text{逻辑通信}}$ （即端到端的通信)。与网络层的区别是，网络层提供的是主机之间的逻辑通信。</p>
<p>从网络层来说，通信的双方是两台主机，IP数据报的首部给出了这两台主机的P地址。但“两台主机之间的通信”实际上是两台主机中的应用进程之间的通信，应用进程之间的通信又称端到端的逻辑通信。这里“逻辑通信”的意思是:传输层之间的通信好像是沿水平方向传送数据，但事实上这两个传输层之间并没有一条水平方向的物理连接。</p>
<p>2） $\color{green}{\text{复用和分用}}$ 。复用是指发送方不同的应用进程都可使用同一个传输层协议传送数据;分用是指接收方的传输层在剥去报文的首部后能够把这些数据正确交付到目的应用进程</p>
<blockquote>
<p>注意:网络层也有复用分用的功能，但网络层的复用是指发送方不同协议的数据都可以封装成IP数据报发送出去，分用是指接收方的网络层在剥去首部后把数据交付给相应的协议。</p>
</blockquote>
<details><summary>图5.1 传输层为相互通信的进程提供逻辑通信</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005104814.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005104814.png';" /></details>

<p>3）传输层还要对收到的报文进行差错检测（首部和数据部分)。而网络层只检查IP数据报的首部，不检验数据部分是否出错。</p>
<p>4）提供两种不同的传输协议，即面向连接的TCP和无连接的UDP。而网络层无法同时实现两种协议（即在网络层要么只提供面向连接的服务，如虚电路;要么只提供无连接服务，如数据报，而不可能在网络层同时存在这两种方式)。</p>
<p>传输层向高层用户屏蔽了低层网络核心的细节（如网络拓扑、路由协议等)，它使应用进程看见的是好像在两个传输层实体之间有一条端到端的逻辑通信信道，这条逻辑通信信道对上层的表现却因传输层协议不同而有很大的差别。当传输层采用面向连接的TCP时，尽管下面的网络是不可靠的（只提供尽最大努力的服务)，但这种逻辑通信信道就相当于一条全双工的可靠信道。但当传输层采用无连接的UDP时，这种逻辑通信信道仍然是一条不可靠信道。</p>
<h4 id="传输层的寻址与端口"><a href="#传输层的寻址与端口" class="headerlink" title="传输层的寻址与端口"></a>传输层的寻址与端口</h4><h5 id="端口的作用"><a href="#端口的作用" class="headerlink" title="端口的作用"></a>端口的作用</h5><p>端口能够让应用层的各种应用进程将其数据通过端口向下交付给传输层，以及让传输层知道应当将其报文段中的数据向上通过端口交付给应用层相应的进程。端口是传输层服务访问点(TSAP)，它在传输层的作用类似于IP地址在网络层的作用或MAC地址在数据链路层的作用，只不过IP地址和MAC地址标识的是主机，而端口标识的是主机中的应用进程。</p>
<p>$\color{red}{\text{数据链路层}}$ 的SAP是 $\color{green}{\text{MAC地址}}$ ， $\color{red}{\text{网络层}}$ 的SAP是 $\color{green}{\text{IP地址}}$ ， $\color{red}{\text{传输层}}$ 的SAP是 $\color{green}{\text{端口}}$ 。</p>
<p>在协议栈层间的抽象的协议端口是软件端口，它与路由器或交换机上的硬件端口是完全不同的概念。硬件端口是不同硬件设备进行交互的接口，而软件端口是应用层的各种协议进程与传输实体进行层间交互的一种地址。传输层使用的是 $\color{green}{\text{软件端口}}$ 。</p>
<h5 id="端口号"><a href="#端口号" class="headerlink" title="端口号"></a>端口号</h5><p>应用进程通过端口号进行标识，端口号长度为16bit，能够表示65536( $2^{16}$ )个不同的端口号。</p>
<p>端口号只具有本地意义，即端口号只标识本计算机应用层中的各进程，在因特网中不同计算机的相同端口号是没有联系的。根据端口号范围可将端口分为两类:</p>
<p>1）服务器端使用的端口号。它又分为两类，最重要的一类是熟知端口号，数值为0～1023,IANA(互联网地址指派机构）把这些端口号指派给了TCP/IP最重要的一些应用程序，让所有的用户都知道。另一类称为登记端口号，数值为1024～49151。它是供没有熟知端口号的应用程序使用的，使用这类端口号必须在IANA登记，以防止重复。</p>
<p>一些常用的熟知端口号如下:</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005105118.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005105118.png';" /></details>

<p>2）客户端使用的端口号，数值为49152～65535。由于这类端口号仅在客户进程运行时才动态地选择，因此又称 $\color{green}{\text{短暂端口号}}$ （也称临时端口)。通信结束后，刚用过的客户端口号就不复存在，从而这个端口号就可供其他客户进程以后使用。</p>
<h5 id="套接字"><a href="#套接字" class="headerlink" title="套接字"></a>套接字</h5><p>在网络中通过IP地址来标识和区别不同的主机，通过端口号来标识和区分一台主机中的不同应用进程，端口号拼接到P地址即构成套接字Socket。在网络中采用发送方和接收方的套接字来识别端点。套接字，实际上是一个通信端点，即</p>
<p>套接字Socket = (IP地址:端口号)</p>
<p>它唯一地标识网络中的一台主机和其上的一个应用（进程)。</p>
<p>在网络通信中，主机A发给主机B的报文段包含目的端口号和源端口号，源端口号是“返回地址”的一部分，即当B需要发回一个报文段给A时，B到A的报文段中的目的端口号便是A到B的报文段中的源端口号（完全的返回地址是A的IP地址和源端口号)。</p>
<h4 id="无连接服务与面向连接服务"><a href="#无连接服务与面向连接服务" class="headerlink" title="无连接服务与面向连接服务"></a>无连接服务与面向连接服务</h4><p>面向连接服务就是在通信双方进行通信之前，必须先建立连接，在通信过程中，整个连接的情况一直被实时地监控和管理。通信结束后，应该释放这个连接。</p>
<p>无连接服务是指两个实体之间的通信不需要先建立好连接，需要通信时，直接将信息发送到“网络”中，让该信息的传递在网上尽力而为地往目的地传送。</p>
<p>TCP/IP协议族在IP层之上使用了两个传输协议:一个是面向连接的传输控制协议（TCP),采用TCP时，传输层向上提供的是一条全双工的可靠逻辑信道;另一个是无连接的用户数据报协议（UDP)，采用UDP时，传输层向上提供的是一条不可靠的逻辑信道。</p>
<p>TCP提供面向连接的服务，在传送数据之前必须先建立连接，数据传送结束后要释放连接。TCP不提供广播或组播服务。由于TCP提供面向连接的可靠传输服务，因此不可避免地增加了许多开销，如确认、流量控制、计时器及连接管理等。这不仅使协议数据单元的头部增大很多，还要占用许多的处理机资源。因此TCP主要适用于可靠性更重要的场合，如文件传输协议（FTP),超文本传输协议（HTTP)、远程登录（TELNET）等。</p>
<p>UDP是一个无连接的非可靠传输层协议。它在P之上仅提供两个附加服务:多路复用和对数据的错误检查。IP知道怎样把分组投递给一台主机，但不知道怎样把它们投递给主机上的具体应用。UDP在传送数据之前不需要先建立连接，远程主机的传输层收到UDP报文后，不需要给出任何确认。由于UDP比较简单，因此执行速度比较快、实时性好。使用UDP的应用主要包括小文件传送协议（TFTP)、DNS、SNMP和实时传输协议（RTP)。</p>
<blockquote>
<p>注意:</p>
</blockquote>
<blockquote>
<p>1）IP数据报和UDP数据报的区别:IP数据报在网络层要经过路由的存储转发;而UDP数据报在传输层的端到端的逻辑信道中传输，封装成IP数据报在网络层传输时，UDP数据报的信息对路由是不可见的。</p>
</blockquote>
<blockquote>
<p>2）TCP和网络层虚电路的区别:TCP报文段在传输层抽象的逻辑信道中传输，对路由器不可见;虚电路所经过的交换结点都必须保存虚电路状态信息。在网络层若采用虚电路方式，则无法提供无连接服务;而传输层采用TCP不影响网络层提供无连接服务。</p>
</blockquote>
<h3 id="UDP"><a href="#UDP" class="headerlink" title="UDP"></a>UDP</h3><h4 id="UDP数据报"><a href="#UDP数据报" class="headerlink" title="UDP数据报"></a>UDP数据报</h4><h5 id="UDP概述"><a href="#UDP概述" class="headerlink" title="UDP概述"></a>UDP概述</h5><p>UDP仅在IP的数据报服务之上增加了两个最基本的服务:复用和分用以及差错检测。如果应用开发者选择UDP而非 TCP，那么应用程序几乎直接与IP打交道。为什么应用开发者宁愿在UDP之上构建应用，也不选择TCP？既然TCP提供可靠的服务，而UDP不提供，那么TCP总是首选吗？答案是否定的，因为有很多应用更适合用UDP，主要因为UDP具有如下优点:</p>
<p>1）UDP无须建立连接。因此UDP不会引入建立连接的时延。试想如果 DNS运行在TCP而非UDP上，那么DNS的速度会慢很多。HTTP使用TCP而非UDP，是因为对于基于文本数据的 Web 网页来说，可靠性是至关重要的。</p>
<p>2）无连接状态。TCP需要在端系统中维护连接状态。此连接状态包括接收和发送缓存、拥塞控制参数和序号与确认号的参数。而UDP不维护连接状态，也不跟踪这些参数。因此某些专用应用服务器使用UDP时，一般都能支持更多的活动客户机。</p>
<p>3）分组首部开销小。TCP有 $\color{green}{\text{20B}}$ 的首部开销，而UDP仅有 $\color{green}{\text{8B}}$ 的开销。</p>
<p>4）应用层能更好地控制要发送的数据和发送时间。UDP没有拥塞控制，因此网络中的拥塞不会影响主机的发送效率。某些 $\color{green}{\text{实时应用}}$ 要求以稳定的速度发送，能容忍一些数据的丢失，但不允许有较大的时延,而UDP正好满足这些应用的需求。</p>
<p>5）UDP支持一对一、一对多、多对一和多对多的交互通信。</p>
<p>UDP常用于一次性传输较少数据的网络应用，如 DNS、SNMP等，因为对于这些应用，若采用TCP，则将为连接创建、维护和拆除带来不小的开销。UDP也常用于多媒体应用(如IP电话、实时视频会议、流媒体等)，显然，可靠数据传输对这些应用来说并不是最重要的，但TCP的拥塞控制会导致数据出现较大的延迟，这是它们不可容忍的。</p>
<p>UDP不保证可靠交付，但这并不意味着应用对数据的要求是不可靠的，所有维护可靠性的工作可由用户在应用层来完成。应用开发者可根据应用的需求来灵活设计自己的可靠性机制。</p>
<p>UDP是面向报文的。发送方UDP对应用层交下来的报文,在添加首部后就向下交付给IP层，一次发送一个报文，既不合并，也不拆分，而是保留这些报文的边界;接收方UDP对IP层交上来UDP数据报，在去除首部后就原封不动地交付给上层应用进程，一次交付一个完整的报文。因此报文不可分割，是UDP数据报处理的最小单位。因此，应用程序必须选择合适大小的报文,若报文太长，UDP把它交给IP层后，可能会导致分片;若报文太短，UDP把它交给IP层后，会使IP数据报的首部的相对长度太大，两者都会降低IP层的效率。</p>
<h5 id="UDP的首部格式"><a href="#UDP的首部格式" class="headerlink" title="UDP的首部格式"></a>UDP的首部格式</h5><p>UDP数据报包含两部分:UDP首部和用户数据。UDP首部有8B，由4个字段组成，每个字段的长度都是2B，如图5.2所示。各字段意义如下:</p>
<p>1） $\color{green}{\text{源端口}}$ 。源端口号。在需要对方回信时选用，不需要时可用全0。</p>
<p>2） $\color{green}{\text{目的端口}}$ 。目的端口号。这在终点交付报文时必须使用到。</p>
<p>3） $\color{green}{\text{长度}}$ 。UDP数据报的长度（包括 $\color{green}{\text{首部}}$ 和 $\color{green}{\text{数据}}$ )，其最小值是8（仅有首部)。</p>
<p>4） $\color{green}{\text{校验和}}$ 。检测UDP数据报在传输中是否有错。有错就丢弃。该字段是可选的，当源主机不想计算校验和时，则直接令该字段为全0。</p>
<details><summary>图5.2UDP数据报格式
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005105726.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005105726.png';" /></details>

<p>当传输层从IP层收到UDP数据报时,就根据首部中的目的端口，把UDP 数据报通过相应的端口上交给应用进程，如图5.3所示。</p>
<p>如果接收方UDP发现收到的报文中的目的端口号不正确（即不存在对应于端口号的应用进程)，那么就丢弃该报文，并由ICMP发送“端口不可达”差错报文给发送方。</p>
<details><summary>图5.3UDP基于端口的分用
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005105758.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005105758.png';" /></details>


<h4 id="UDP校验"><a href="#UDP校验" class="headerlink" title="UDP校验"></a>UDP校验</h4><p>在计算校验和时,要在UDP数据报之前增加12B的伪首部,伪首部并不是UDP的真正首部。只是在计算校验和时，临时添加在UDP数据报的前面，得到一个临时的UDP数据报。校验和就是按照这个临时的UDP 数据报来计算的。伪首部既不向下传送又不向上递交，而只是为了计算校验和。图5.4给出了UDP数据报的伪首部各字段的内容。</p>
<p>UDP校验和的计算方法和IP数据报首部校验和的计算方法相似。但不同的是，IP数据报的校验和只检验IP数据报的首部，但UDP的校验和则检查首部和数据部分。</p>
<details><summary>图5.4 UDP数据报的首部和伪首部
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005105849.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005105849.png';" /></details>

<p>发送方首先把全零放入校验和字段并添加伪首部，然后把UDP数据报视为许多16位的字串接起来。若UDP数据报的数据部分不是偶数个字节，则要在数据部分末尾填入一个全零字节(但此字节不发送)。然后按二进制反码计算出这些16位字的和，将此和的二进制反码写入校验和字段，并发送。接收方把收到的 UDP 数据报加上伪首部（如果不为偶数个字节，那么还需要补上全零字节）后，按二进制反码求这些16位字的和。当无差错时其结果应为全1，否则就表明有差错出现，接收方就应该丢弃这个UDP数据报。</p>
<p>图5.5给出了一个计算UDP校验和的例子。本例中，UDP数据报的长度是15B(不含伪首部)，因此需要添加一个全0字节。</p>
<details><summary>图5.5计算UDP校验和的例子
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005105938.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005105938.png';" /></details>

<blockquote>
<p>注意:</p>
</blockquote>
<blockquote>
<p>1)校验时，若UDP数据报部分的长度不是偶数个字节，则需填入一个全0字节，如图5.5所示。但是此字节和伪首部一样，是不发送的。</p>
</blockquote>
<blockquote>
<p>2）如果UDP校验和校验出UDP数据报是错误的，那么可以丢弃，也可以交付给上层，但是需要附上错误报告，即告诉上层这是错误的数据报。</p>
</blockquote>
<blockquote>
<p>3)通过伪首部，不仅可以检查源端口号、目的端口号和UDP用户数据报的数据部分，还可以检查IP数据报的源IP地址和目的地址。</p>
</blockquote>
<p>这种简单的差错检验方法的检错能力并不强，但它的好处是简单、处理速度快。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211009212931.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211009212931.png';" /></details>

<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/yueyadao/article/details/85042511">参考文献</a></p>
<h3 id="TCP协议"><a href="#TCP协议" class="headerlink" title="TCP协议"></a>TCP协议</h3><h4 id="TCP协议的特点"><a href="#TCP协议的特点" class="headerlink" title="TCP协议的特点"></a>TCP协议的特点</h4><p>TCP是在不可靠的P层之上实现的可靠的数据传输协议，它主要解决传输的可靠、有序、无丢失和不重复问题。TCP是TCP/IP体系中非常复杂的一个协议，主要特点如下:</p>
<p>1）TCP是面向连接的传输层协议。</p>
<p>2）每条TCP连接只能有两个端点，每条TCP 连接只能是点对点的（一对一)。</p>
<p>3）TCP提供可靠的交付服务，保证传送的数据无差错、不丢失、不重复且有序。</p>
<p>4）TCP提供全双工通信，允许通信双方的应用进程在任何时候都能发送数据，为此TCP连接的两端都设有发送缓存和接收缓存，用来临时存放双向通信的数据。</p>
<p>发送缓存用来暂时存放以下数据:①发送应用程序传送给发送方TCP准备发送的数据;②TCP已发送但尚未收到确认的数据。接收缓存用来暂时存放以下数据:①按序到达但尚未被接收应用程序读取的数据;②不按序到达的数据。</p>
<p>5）TCP是面向 $\color{green}{\text{字节流}}$ 的，虽然应用程序和TCP的交互是一次一个数据块（大小不等)，但TCP把应用程序交下来的数据仅视为一连串的无结构的字节流。</p>
<p>TCP和UDP在发送报文时所采用的方式完全不同。UDP报文的长度由发送应用进程决定，而TCP报文的长度则根据接收方给出的窗口值和当前网络拥塞程度来决定。如果应用进程传送到TCP缓存的数据块太长，TCP就把它划分得短一些再传送;如果太短，TCP也可以等到积累足够多的字节后再组成报文段发送出去。关于TCP报文的长度问题，后面还会详细讨论。</p>
<h4 id="TCP报文段"><a href="#TCP报文段" class="headerlink" title="TCP报文段"></a>TCP报文段</h4><p>TCP传送的数据单元称为报文段。TCP报文段既可以用来运载数据，又可以用来建立连接、释放连接和应答。一个TCP报文段分为首部和数据两部分，整个TCP报文段作为TP数据报的数据部分封装在IP数据报中，如图5.6所示。其首部的前20B是固定的。TCP报文段的首部最短为20B，后面有4N字节是根据需要而增加的选项，通常长度为4B的整数倍。</p>
<p>TCP的全部功能体现在其首部的各个字段中，各字段意义如下:</p>
<p>1） $\color{green}{\text{源端口和目的端口}}$ 。各占2B。端口是运输层与应用层的服务接口，运输层的复用和分用功能都要通过端口实现。</p>
<p>2） $\color{green}{\text{序号}}$ 。占4B，范围为0~ $2^{32}$ -1，共 $ 2^{23}$ 个序号。TCP是面向字节流的(即TCP传送时是逐个字节传送的)，所以TCP连接传送的字节流中的每个字节都按顺序编号。序号字段的值指的是本报文段所发送的数据的第一个字节的序号。</p>
<p>例如，一报文段的序号字段值是301，而携带的数据共有100B，表明本报文段的数据的最后一个字节的序号是400，因此下一个报文段的数据序号应从401开始。</p>
<details><summary>图5.6TCP报文段
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005110342.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005110342.png';" /></details>

<p>3）确认号。占4B，是期望收到对方下一个报文段的第一个数据字节的序号。若确认号为N,则表明到序号N-1为止的所有数据都已正确收到。</p>
<p>例如,B正确收到了A发送过来的一个报文段，其序号字段是501，而数据长度是200B(序号501～700)，这表明B正确收到了A发送的到序号700为止的数据。因此B期望收到A的下一个数据序号是701，于是B在发送给A的确认报文段中把确认号置为701。</p>
<p>4）数据偏移（即首部长度)。占4位，这里不是IP数据报分片的那个数据偏移，而是表示首部长度（首部中还有长度不确定的选项字段)，它指出TCP报文段的数据起始处距离TCP报文段的起始处有多远。“数据偏移”的单位是32位(以4B为计算单位)。因此当此字段的值为15时，达到TCP首部的最大长度60B。</p>
<p>5）保留。占6位，保留为今后使用，但目前应置为0。</p>
<p>6）紧急位URG。URG= 1时，表明紧急指针字段有效。它告诉系统此报文段中有紧急数据，应尽快传送（相当于高优先级的数据)。但URG需要和紧急指针配合使用，即数据从第一个字节到紧急指针所指字节就是紧急数据。</p>
<p>7）确认位ACK。仅当ACK =1时确认号字段才有效。当ACK=0时， $\color{green}{\text{确认号无效}}$ 。</p>
<p>TCP规定，在连接建立后所有传送的报文段都必须把ACK置1。</p>
<p>8）推送位PSH(Push)。接收方TCP收到PSH= 1的报文段，就尽快地交付给接收应用进程,而不再等到整个缓存都填满后再向上交付。</p>
<p>9）复位位RST (Reset)。RST=1时，表明TCP连接中出现严重差错（如主机崩溃或其他原因)，必须释放连接，然后再重新建立运输连接。</p>
<p>10）同步位SYN。同步SYN=1表示这是一个连接请求或连接接受报文。</p>
<p>当SYN=1，ACK=0时，表明这是一个 $\color{green}{\text{连接请求报文}}$ ，对方若同意建立连接，则应在响应报文中使用SYN=1，ACK= 1。</p>
<p>11）终止位FIN (Finish)。用来释放一个连接。当FIN= 1时，表明此报文段的发送方的数据已发送完毕，并要求释放传输连接。</p>
<p>12）窗口。占2B，范围为0~ $2^{16}$ -1。它指出现在允许对方发送的数据量，接收方的数据缓存空间是有限的，因此用窗口值作为接收方让发送方设置其发送窗口的依据。</p>
<p>例如，设确认号是701，窗口字段是1000。这表明，从701号算起，发送此报文段的一方还有接收1000字节数据（字节序号为701～1700）的接收缓存空间。</p>
<p>13）校验和。占 2B。校验和字段检验的范围包括首部和数据两部分。在计算校验和时，和UDP一样，要在TCP报文段的前面加上12B 的伪首部(只需将UDP伪首部的第4个字段，即协议字段的17改成6，其他的和UDP一样)。</p>
<p>14）紧急指针。占2B。紧急指针仅在 URG= 1时才有意义，它指出在本报文段中紧急数据共有多少字节（紧急数据在报文段数据的最前面)。</p>
<p>15）选项。长度可变。TCP最初只规定了一种选项，即最大报文段长度(Maximum Segment Size,MSS)。MSS是TCP报文段中的数据字段的最大长度（注意仅仅是数据字段)。</p>
<p>16）填充。这是为了使整个首部长度是4B的整数倍。</p>
<h4 id="TCP连接管理"><a href="#TCP连接管理" class="headerlink" title="TCP连接管理"></a>TCP连接管理</h4><p>TCP是面向连接的协议，因此每个TCP连接都有三个阶段:连接建立、数据传送和连接释放。TCP连接的管理就是使运输连接的建立和释放都能正常进行。</p>
<p>在TCP连接建立的过程中，要解决以下三个问题:</p>
<p>1）要使每一方能够确知对方的存在。</p>
<p>2）要允许双方协商一些参数（如最大窗口值、是否使用窗口扩大选项、时间戳选项及服务质量等)。</p>
<p>3）能够对运输实体资源（如缓存大小、连接表中的项目等）进行分配。</p>
<p>TCP把连接作为最基本的抽象，每条TCP连接有两个端点，TCP连接的端点不是主机，不是主机的IP地址，不是应用进程,也不是传输层的协议端口。TCP连接的端口即为套接字(socket)或插口，每条TCP连接唯一地被通信的两个端点（即两个套接字）确定。</p>
<p>TCP连接的建立采用客户/服务器方式。主动发起连接建立的应用进程称为客户(Client)，而被动等待连接建立的应用进程称为服务器(Server）。</p>
<h5 id="TCP连接的建立"><a href="#TCP连接的建立" class="headerlink" title="TCP连接的建立"></a>TCP连接的建立</h5><p>连接的建立经历以下3个步骤，通常称为三次握手，如图5.7所示。</p>
<details><summary>图5.7用“三次握手”建立TCP连接
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005110903.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005110903.png';" /></details>

<p>连接建立前，服务器进程处于LISTEN（收听）状态，等待客户的连接请求。</p>
<p>第一步:客户机的TCP首先向服务器的TCP发送连接请求报文段。这个特殊报文段的首部中的同步位SYN置1，同时选择一个初始序号seq= $x$ 。TCP规定，SYN报文段不能携带数据，但要消耗掉一个序号。这时，TCP客户进程进入SYN-SENT（ $\color{green}{\text{同步已发送}}$ ）状态。</p>
<p>第二步:服务器的TCP收到连接请求报文段后，如同意建立连接，则向客户机发回确认，并为该TCP连接分配缓存和变量。在确认报文段中，把SYN位和ACK位都置1，确认号是ack= $x$ +1，同时也为自己选择一个初始序号seq=y。注意，确认报文段不能携带数据，但也要消耗掉个序号。这时，TCP服务器进程进入SYN-RCVD(同步收到）状态。</p>
<p>第三步:当客户机收到确认报文段后，还要向服务器给出确认，并为该TCP 连接分配缓存和变量。确认报文段的ACK位置1，确认号 ack= $y$ +1，序号seq= $x$ +1。该报文段可以携带数据若不携带数据则不消耗序号。这时，TCP客户进程进入ESTABLISHED(已建立连接）状态。</p>
<p>成功进行以上三步后，就建立了TCP连接，接下来就可以传送应用层数据。TCP提供的是全双工通信，因此通信双方的应用进程在任何时候都能发送数据。</p>
<p>另外，值得注意的是，服务器端的资源是在完成第二次握手时分配的，而客户端的资源是在完成第三次握手时分配的，这就使得服务器易于受到SYN洪泛攻击。</p>
<h5 id="TCP连接的释放"><a href="#TCP连接的释放" class="headerlink" title="TCP连接的释放"></a>TCP连接的释放</h5><p>天下没有不散的筵席，TCP同样如此。参与TCP连接的两个进程中的任何一个都能终止该连接。TCP连接释放的过程通常称为 $\color{green}{\text{四次握手}}$ ，如图5.8所示。</p>
<p>第一步:客户机打算关闭连接时，向其TCP发送连接释放报文段，并停止发送数据，主动关闭TCP连接，该报文段的终止位FIN置1，序号seq= $u$ ，它等于前面已传送过的数据的最后一个字节的序号加1，FIN 报文段即使不携带数据，也消耗掉一个序号。这时，TCP客户进程进入FIN-WAIT-1( $\color{green}{\text{终止等待1}}$ )状态。TCP是全双工的，即可以想象为一条TCP连接上有两条数据通路,发送FIN的一端不能再发送数据，即关闭了其中一条数据通路，但对方还可以发送数据。</p>
<p>第二步:服务器收到连接释放报文段后即发出确认，确认号ack = $u$ +1，序号 seq =  $v$ ，等于它前面已传送过的数据的最后一个字节的序号加1。然后服务器进入CLOSE-WAIT(关闭等待)状态。此时，从客户机到服务器这个方向的连接就释放了，TCP连接处于半关闭状态。但服务器若发送数据，客户机仍要接收，即从服务器到客户机这个方向的连接并未关闭。</p>
<details><summary>图5.8用“四次握手”释放TCP连接
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005111256.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005111256.png';" /></details>

<p>第三步:若服务器已没有要向客户机发送的数据，就通知TCP释放连接，此时其发出FIN=1的连接释放报文段。设该报文段的序号为w(在半关闭状态服务器可能又发送了一些数据),还须重复上次已发送的确认号ack =  $u$  +1。这时服务器进入LAST-ACK(最后确认）状态。</p>
<p>第四步:客户机收到连接释放报文段后，必须发出确认。把确认报文段中的确认位ACK 置1，确认号ack =  $w$  +1，序号seq =  $u$  +1。此时TCP连接还未释放，必须经过时间等待计时器设置的时间2MSL(最长报文段寿命）后，客户机才进入CLOSED（连接关闭）状态。</p>
<p>对上述TCP连接建立和释放的总结如下:</p>
<p>1）连接建立。分为3步:</p>
<p>${\textstyle\unicode{x2460}}$  SYN=1，seq =  $x$ 。</p>
<p>${\textstyle\unicode{x2461}}$ SYN=1，ACK=1，seq= $y$ ，ack = $x$ +1。</p>
<p>③ ACK=1，seq= $x$ + 1，ack = $y$  +1。</p>
<p>2）释放连接。分为4步:</p>
<p>${\textstyle\unicode{x2460}}$  FIN=1，seq =  $u$ 。</p>
<p>${\textstyle\unicode{x2461}}$  ACK= 1，seq=  $w$ , ack =  $u$ +1。</p>
<p>③FIN= 1，ACK=1, seq= $w$ ，ack=  $u$ +1。</p>
<p>${\textstyle\unicode{x2463}}$ ACK= 1，seq=  $u$ +1，ack =  $w$ + 1。</p>
<p>选择题喜欢考查（关于连接和释放的题目，ACK、SYN、FIN一定等于1)，请牢记。</p>
<h4 id="TCP可靠传输"><a href="#TCP可靠传输" class="headerlink" title="TCP可靠传输"></a>TCP可靠传输</h4><p>TCP 的任务是在IP层不可靠的、尽力而为服务的基础上建立一种可靠数据传输服务。TCP提供的可靠数据传输服务保证接收方进程从缓存区读出的字节流与发送方发出的字节流完全一样。TCP使用了 $\color{green}{\text{校验}}$ 、 $\color{green}{\text{序号}}$ 、 $\color{green}{\text{确认}}$ 和 $\color{green}{\text{重传}}$ 等机制来达到这一目的。其中，TCP 的校验机制与UDP校验一样，这里不再赘述。</p>
<h5 id="序号"><a href="#序号" class="headerlink" title="序号"></a>序号</h5><p>TCP首部的序号字段用来保证数据能有序提交给应用层，TCP把数据视为一个无结构但有序的字节流，序号建立在传送的字节流之上，而不建立在报文段之上。</p>
<p>TCP连接传送的数据流中的每个字节都编上一个序号。序号字段的值是指本报文段所发送的数据的第一个字节的序号。如图5.9所示，假设A和B之间建立了一条TCP连接,A的发送缓存区中共有10B，序号从О开始标号，第一个报文包含第0～2个字节，则该TCP报文段的序号是0，第二个报文段的序号是3。</p>
<details><summary>图5.9A的发送缓存区中的数据划分成TCP段
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005111948.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005111948.png';" /></details>


<h5 id="确认"><a href="#确认" class="headerlink" title="确认"></a>确认</h5><p>TCP首部的确认号是期望收到对方的下一个报文段的数据的第一个字节的序号。在图5.9中,如果接收方B已收到第一个报文段,此时B希望收到的下一个报文段的数据是从第3个字节开始的，那么B发送给A的报文中的确认号字段应为3。发送方缓存区会继续存储那些已发送但未收到确认的报文段，以便在需要时重传。</p>
<p>TCP 默认使用累计确认，即TCP只确认数据流中至第一个丢失字节为止的字节。例如，在图5.8中，接收方B收到了A发送的包含字节0<del>2及字节6</del>7的报文段。由于某种原因，B还未收到字节3~5的报文段，此时B仍在等待字节3(和其后面的字节)，因此B到A的下一个报文段将确认号字段置为3。</p>
<h5 id="重传"><a href="#重传" class="headerlink" title="重传"></a>重传</h5><p>有两种事件会导致TCP对报文段进行重传:超时和冗余ACK。</p>
<p>（1）超时</p>
<p>TCP每发送一个报文段，就对这个报文段设置一次计时器。计时器设置的重传时间到期但还未收到确认时，就要重传这一报文段。</p>
<p>由于TCP的下层是一个互联网环境，IP 数据报所选择的路由变化很大，因而传输层的往返时延的方差也很大。为了计算超时计时器的重传时间，TCP采用一种自适应算法，它记录一个报文段发出的时间，以及收到相应确认的时间，这两个时间之差称为报文段的往返时间(Round-TripTime，RTT)。TCP保留了RTT的一个加权平均往返时间 $RTT_S$ ，它会随新测量RTT样本值的变化而变化。显然，超时计时器设置的超时重传时间（Retransmission Time-Out，RTO）应略大于 $RTT_S$ ，但也不能大太多，否则当报文段丢失时，TCP不能很快重传，导致数据传输时延大。</p>
<p>（2）冗余ACK（冗余确认)</p>
<p>超时触发重传存在的一个问题是超时周期往往太长。所幸的是，发送方通常可在超时事件发生之前通过注意所谓的冗余ACK来较好地检测丢包情况。冗余ACK 就是再次确认某个报文段的ACK，而发送方先前已经收到过该报文段的确认。例如，发送方A发送了序号为1、2、3、4、5的TCP报文段，其中2号报文段在链路中丢失，它无法到达接收方B。因此3、4、5号报文段对于B来说就成了失序报文段。TCP规定每当比期望序号大的失序报文段到达时,就发送一个冗余ACK，指明下一个期待字节的序号。在本例中，3、4、5号报文到达B，但它们不是B所期望收到的下一个报文，于是B就发送3个对1号报文段的冗余ACK，表示自己期望接收2号报文段。TCP规定当发送方收到对同一个报文段的3个冗余ACK时，就可以认为跟在这个被确认报文段之后的报文段已经丢失。就前面的例子而言，当A收到对于1号报文段的3个冗余ACK时，它可以认为2号报文段已经丢失，这时发送方A可以立即对2号报文执行重传,这种技术通常称为 $\color{green}{\text{快速重传}}$ 。当然,冗余ACK还被用在拥塞控制中，这将在后面的内容中讨论。</p>
<h4 id="TCP流量控制"><a href="#TCP流量控制" class="headerlink" title="TCP流量控制"></a>TCP流量控制</h4><p>TCP提供流量控制服务来消除发送方(（发送速率太快）使接收方缓存区溢出的可能性，因此可以说流量控制是一个速度匹配服务（匹配发送方的发送速率与接收方的读取速率)。</p>
<p>TCP提供一种基于滑动窗口协议的流量控制机制，滑动窗口的基本原理已在第3章的数据链路层介绍过，这里要介绍的是TCP如何使用窗口机制来实现流量控制。</p>
<p>在通信过程中，接收方根据自己接收缓存的大小，动态地调整发送方的发送窗口大小，这称为接收窗口rwnd，即调整TCP报文段首部中的“窗口”字段值，来限制发送方向网络注入报文的速率。同时，发送方根据其对当前网络拥塞程度的估计而确定的窗口值，这称为拥塞窗口 cwnd(后面会讲到)，其大小与网络的带宽和时延密切相关。</p>
<p>例如，在通信中，有效数据只从A发往B，而B仅向A发送确认报文，这时B可以通过设置确认报文段首部的窗口字段来将rwnd通知给A。rwnd即接收方允许连续接收的最大能力，单位是字节。发送方A总是根据最新收到的rwnd值来限制自己发送窗口的大小，从而将未确认的数据量控制在rwnd大小之内，保证A不会使B的接收缓存溢出。当然，A的发送窗口的实际大小取 rwnd和 cwnd 中的最小值。图5.10中的例子说明了如何利用滑动窗口机制进行流量控制设A向B发送数据，在连接建立时，B告诉A:“我的接收窗口rwnd = 400(字节)”。接收方进行了三次流量控制，这三个报文段都设置了ACK=1，只有在ACK=1时确认号字段才有意义。第一次把窗口减小到rwnd =300，第二次把窗口减小到rwnd = 100，最后把窗口减小到rwnd =0，即不允许发送方再发送数据。这使得发送方暂停发送的状态将持续到B重新发出一个新的窗口值为止。</p>
<details><summary>图5.10利用可变窗口进行流量控制举例
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005112304.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005112304.png';" /></details>

<p>传输层和数据链路层的流量控制的区别是:传输层定义端到端用户之间的流量控制，数据链路层定义两个中间的相邻结点的流量控制。另外，数据链路层的滑动窗口协议的窗口大小不能 $\color{green}{\text{动态变化}}$ ，传输层的则可以动态变化。</p>
<h4 id="TCP拥塞控制"><a href="#TCP拥塞控制" class="headerlink" title="TCP拥塞控制"></a>TCP拥塞控制</h4><p>拥塞控制是指防止过多的数据注入网络,保证网络中的路由器或链路不致过载。出现拥塞时，端点并不了解拥塞发生的细节，对通信连接的端点来说，拥塞往往表现为通信时延的增加。</p>
<p>拥塞控制与流量控制的区别:拥塞控制是让网络能够承受现有的网络负荷，是一个全局性的过程，涉及所有的主机、所有的路由器，以及与降低网络传输性能有关的所有因素。相反，流量控制往往是指点对点的通信量的控制，是个端到端的问题（接收端控制发送端)，它所要做的是抑制发送端发送数据的速率，以便使接收端来得及接收。当然，拥塞控制和流量控制也有相似的地方，即它们都通过控制发送方发送数据的速率来达到控制效果。</p>
<p>例如，某个链路的传输速率为10Gb/s，某大型机向一台PC 以1Gb/s的速率传送文件，显然网络的带宽是足够大的，因而不存在拥塞问题，但如此高的发送速率将导致PC可能来不及接收，因此必须进行流量控制。但若有100万台PC在此链路上以1Mb/s的速率传送文件，则现在的问题就变为网络的负载是否超过了现有网络所能承受的范围。</p>
<p>因特网建议标准定义了进行拥塞控制的4种算法: $\color{green}{\text{慢开始}}$ 、 $\color{green}{\text{拥塞避免}}$ 、 $\color{green}{\text{快重传}}$ 和 $\color{green}{\text{快恢复}}$ 。发送方在确定发送报文段的速率时，既要根据接收方的接收能力，又要从全局考虑不要使网络发生拥塞。因此，TCP协议要求发送方维护以下两个窗口:</p>
<p>1）接收窗口rwnd，接收方根据目前接收缓存大小所许诺的最新窗口值，反映接收方的容量。由接收方根据其放在TCP报文的首部的窗口字段通知发送方。</p>
<p>2）拥塞窗口cwnd，发送方根据自己估算的网络拥塞程度而设置的窗口值，反映网络的当前容量。只要网络未出现拥塞，拥塞窗口就再增大一些，以便把更多的分组发送出去。但只要网络出现拥塞，拥塞窗口就减小一些，以减少注入网络的分组数。</p>
<p>发送窗口的上限值应取接收窗口rwnd和拥塞窗口 cwnd中较小的一个，即</p>
<p style='text-align:center'>发送窗口的上限值= min[rwnd, cwnd]</p>

<p>接收窗口的大小可根据TCP报文首部的窗口字段通知发送方，而发送方如何维护拥塞窗口呢?这就是下面讲解的慢开始和拥塞避免算法。</p>
<blockquote>
<p>注意:这里假设接收方总是有足够大的缓存空间，因而发送窗口大小由网络的拥塞程度决定，也就是说，可以将发送窗口等同为拥塞窗口。</p>
</blockquote>
<h5 id="慢开始和拥塞避免"><a href="#慢开始和拥塞避免" class="headerlink" title="慢开始和拥塞避免"></a>慢开始和拥塞避免</h5><p>（1）慢开始算法</p>
<p>在TCP刚刚连接好并开始发送TCP报文段时，先令拥塞窗口cwnd = 1，即一个最大报文段长度MSS。每收到一-个对新报文段的确认后，将cwnd加1，即增大一个MSS。用这样的方法逐步增大发送方的cwnd，可使分组注入网络的速率更加合理。</p>
<p>例如，A向B发送数据，发送方先置拥塞窗口cwnd = 1，A发送第一个报文段，A收到B对第一个报文段的确认后，把 cwnd 从1增大到2;于是A接着发送两个报文段，A收到B对这两个报文段的确认后，把cwnd从2增大到4，下次就可一次发送4个报文段。</p>
<p>慢开始的“慢” $\color{red}{\text{并不是}}$ 指拥塞窗口cwnd的 $\color{green}{\text{增长速率慢}}$ ，而是指在TCP开始发送报文段时先设置cwnd = 1，使得发送方在开始时只发送一个报文段（目的是试探一下网络的拥塞情况)，然后再逐渐增大cwnd，这对防止网络出现拥塞是一个非常有力的措施。使用慢开始算法后，每经过一个传输轮次（即往返时延RTT)，cwnd就会加倍，即 cwnd的大小指数式增长。这样，慢开始直把cwnd增大到一个规定的 $\color{green}{\text{慢开始门限}}$ ssthresh（阈值)，然后改用 $\color{green}{\text{拥塞避免}}$ 算法。</p>
<p>（2）拥塞避免算法</p>
<p>拥塞避免算法的思路是让拥塞窗口cwnd缓慢增大，具体做法是:每经过一个往返时延RTT就把发送方的拥塞窗口cwnd加1,而不是加倍,使拥塞窗口cwnd按线性规律缓慢增长(即加法增大),这比慢开始算法的拥塞窗口增长速率要缓慢得多。</p>
<p>根据cwnd的大小执行不同的算法，可归纳如下:</p>
<ul>
<li>当cwnd &lt; ssthresh时，使用慢开始算法。</li>
<li>当cwnd &gt; ssthresh 时，停止使用慢开始算法而改用拥塞避免算法。</li>
<li>当cwnd = ssthresh时，既可使用慢开始算法，又可使用拥塞避免算法（通常做法)。</li>
</ul>
<p>（3）网络拥塞的处理</p>
<p>无论在慢开始阶段还是在拥塞避免阶段，只要发送方判断网络出现拥塞（ $\color{green}{\text{未按时收到确认}}$ )，就要把慢开始门限ssthresh 设置为出现拥塞时的发送方的cwnd值的一半(但不能小于2)。然后把拥塞窗口cwnd重新设置为1，执行慢开始算法。这样做的目的是迅速减少主机发送到网络中的分组数，使得发生拥塞的路由器有足够时间把队列中积压的分组处理完。</p>
<p>慢开始和拥塞避免算法的实现过程如图5.11所示。</p>
<details><summary>图5.11 慢开始和拥塞避免算法的实现过程
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005123540.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005123540.png';" /></details>

<ul>
<li>初始时，拥塞窗口置为1，即 cwnd = 1，慢开始门限置为16，即ssthresh = 16。</li>
<li>慢开始阶段，cwnd 的初值为1，以后发送方每收到一个确认ACK，cwnd值加1，也即经过每个传输轮次(RTT)，cwnd 呈指数规律增长。当拥塞窗口 cwnd 增长到慢开始门限ssthresh时(即当cwnd = 16时)，就改用拥塞避免算法，cwnd按线性规律增长。</li>
<li>假定cwnd = 24时网络出现超时，更新ssthresh值为12(即变为超时时cwnd值的一半),cwnd重置为1，并执行慢开始算法，当cwnd = 12时，改为执行拥塞避免算法。</li>
</ul>
<blockquote>
<p>注意:在慢开始（指数级增长）阶段，若2cwnd &gt; ssthresh，则下一个RTT后的cwnd等于ssthresh，而不等于2cwnd，即 cwnd 不能跃过ssthresh 值。如图5.11所示，在第16个轮次时cwnd、ssthresh = 12，在第17个轮次时cwnd= 12，而不等于16。</p>
</blockquote>
<p>在慢开始和拥塞避免算法中使用了“乘法减小”和“加法增大”方法。“乘法减小”是指不论是在慢开始阶段还是在拥塞避免阶段，只要出现超时（即很可能出现了网络拥塞)，就把慢开始门限值ssthresh 设置为当前拥塞窗口的一半（并执行慢开始算法)。当网络频繁出现拥塞时，ssthresh值就下降得很快，以大大减少注入网络的分组数。而“加法增大”是指执行拥塞避免算法后，在收到对所有报文段的确认后（即经过一个RTT)，就把拥塞窗口cwnd增加一个 MSS 大小，使拥塞窗口缓慢增大，以防止网络过早出现拥塞。</p>
<p>拥塞避免并不能完全避免拥塞。利用以上措施要完全避免网络拥塞是不可能的。拥塞避免是指在拥塞避免阶段把拥塞窗口控制为按线性规律增长，使网络比较不容易出现拥塞。</p>
<h5 id="快重传和快恢复"><a href="#快重传和快恢复" class="headerlink" title="快重传和快恢复"></a>快重传和快恢复</h5><p>快重传和快恢复算法是对慢开始和拥塞避免算法的改进。</p>
<p>（1）快重传</p>
<p>在上一节介绍的TCP可靠传输机制中，快重传技术使用了冗余ACK来检测丢包的发生。同样，冗余ACK也用于网络拥塞的检测（丢了包当然意味着网络可能出现了拥塞)。快重传并非取消重传计时器，而是在某些情况下可更早地重传丢失的报文段。</p>
<p>当发送方连续收到三个重复的 ACK报文时，直接重传对方尚未收到的报文段，而不必等待那个报文段设置的重传计时器超时。</p>
<p>（2）快恢复</p>
<p>快恢复算法的原理如下:当发送方连续收到三个冗余ACK（即重复确认）时，执行“乘法减小”算法，把慢开始门限ssthresh 设置为此时发送方cwnd的一半。这是为了预防网络发生拥塞。但发送方现在认为网络很可能没有发生（严重〉拥塞，否则就不会有几个报文段连续到达接收方，也不会连续收到重复确认。因此与慢开始不同之处是它把 cwnd值设置为慢开始门限ssthresh 改变后的数值，然后开始执行拥塞避免算法(“加法增大”)，使拥塞窗口缓慢地线性增大。</p>
<p>由于跳过了拥塞窗口cwnd 从1起始的慢开始过程，所以被称为快恢复。快恢复算法的实现过程如图5.12所示，作为对比，虚线为慢开始的处理过程。</p>
<details><summary>图5.12快恢复算法的实现过程
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005123854.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005123854.png';" /></details>

<p>在流量控制中，发送方发送数据的量由接收方决定，而在拥塞控制中，则由发送方自己通过检测网络状况来决定。实际上，慢开始、拥塞避免、快重传和快恢复几种算法是同时应用在拥塞控制机制中。四种算法使用的总结:在TCP连接建立和网络出现超时时，采用慢开始和拥塞避免算法;当发送方接收到冗余ACK时，采用快重传和快恢复算法。</p>
<p>在本节的最后，再次提醒读者:接收方的缓存空间总是有限的。因此，发送方发送窗口的实际大小由流量控制和拥塞控制共同决定。当题目中同时出现接收窗口(rwnd)和拥塞窗口(cwnd)时，发送方实际的发送窗口大小是由rwnd和 cwnd中较小的那一个确定的。</p>
<h3 id="本章小结及疑难点"><a href="#本章小结及疑难点" class="headerlink" title="本章小结及疑难点"></a>本章小结及疑难点</h3><h4 id="MSS设置得太大或太小会有什么影响"><a href="#MSS设置得太大或太小会有什么影响" class="headerlink" title="MSS设置得太大或太小会有什么影响?"></a>MSS设置得太大或太小会有什么影响?</h4><p>规定最大报文段MSS的大小并不是考虑到接收方的缓存可能放不下TCP报文段。实际上，MSS与接收窗口没有关系。TCP的报文段的数据部分，至少要加上40B 的首部（TCP首部至少20B和IP首部至少20B)，才能组装成一个P数据报。若选择较小的MSS值，网络的利用率就很低。设想在极端情况下，当TCP报文段中只含有1B的数据时，在P层传输的数据报的开销至少有40B。这样，网络的利用率就不会超过1/41。到了数据链路层还要加上一些开销，网络的利用率进一步降低。但反过来，若TCP报文段很长，那么在P层传输时有可能要分解成多个短数据报片，在终端还要把收到的各数据报片装配成原来的TCP报文段。传输有差错时，还要进行重传。这些都会使开销增大。</p>
<p>因此，MSS应尽量大一些，只要在IP层传输时不要再分片就行。由于IP数据报所经历的路径是动态变化的，在一条路径上确定的不需要分片的MSS，如果改走另一条路径，就可能需要进行分片。因此，最佳的MSS是很难确定的。MSS的默认值为536B，因此在因特网上的所有主机都能接收的报文段长度是536＋20×TCP固定首部长度=556B。</p>
<h4 id="为何不采用“三次握手”释放连接，且发送最后一次握手报文后要等待2MSL-的时间呢"><a href="#为何不采用“三次握手”释放连接，且发送最后一次握手报文后要等待2MSL-的时间呢" class="headerlink" title="为何不采用“三次握手”释放连接，且发送最后一次握手报文后要等待2MSL 的时间呢?"></a>为何不采用“三次握手”释放连接，且发送最后一次握手报文后要等待2MSL 的时间呢?</h4><p>原因有两个:</p>
<p>1）保证A发送的最后一个确认报文段能够到达B。如果A不等待2MSL，若A返回的最后确认报文段丢失，则B不能进入正常关闭状态，而A此时已经关闭，也不可能再重传。</p>
<p>2）防止出现“已失效的连接请求报文段”。A在发送最后一个确认报文段后，再经过2MSL可保证本连接持续的时间内所产生的所有报文段从网络中消失。造成错误的情形与下文(疑难点6）不采用“两次握手”建立连接所述的情形相同。</p>
<blockquote>
<p>注意:服务器结束TCP连接的时间要比客户机早一些，因为客户机最后要等待2MSL后才可进入 CLOSED状态。</p>
</blockquote>
<h4 id="如何判定此确认报文段是对原来的报文段的确认，还是对重传的报文段的确认"><a href="#如何判定此确认报文段是对原来的报文段的确认，还是对重传的报文段的确认" class="headerlink" title="如何判定此确认报文段是对原来的报文段的确认，还是对重传的报文段的确认?"></a>如何判定此确认报文段是对原来的报文段的确认，还是对重传的报文段的确认?</h4><p>由于对于一个重传报文的确认来说，很难分辨它是原报文的确认还是重传报文的确认，使用修正的Karn算法作为规则:在计算平均往返时间RTT时，只要报文段重传了，就不采用其往返时间样本，且报文段每重传一次，就把RTO增大一些。</p>
<h4 id="TCP使用的是GBN还是选择重传"><a href="#TCP使用的是GBN还是选择重传" class="headerlink" title="TCP使用的是GBN还是选择重传?"></a>TCP使用的是GBN还是选择重传?</h4><p>这是一个有必要弄清的问题。前面讲过，TCP使用累计确认，这看起来像是GBN的风格。但是，正确收到但失序的报文并不会丢弃，而是缓存起来，并且发送冗余ACK指明期望收到的下一个报文段，这是TCP方式和 GBN的显著区别。例如，A发送了N个报文段，其中第k (k &lt;  N)个报文段丢失，其余N-1个报文段正确地按序到达接收方B。使用GBN时，A需要重传分组k，及所有后继分组k+1,k+2,…,N。相反，TCP却至多重传一个报文段，即报文段k。另外,TCP中提供一个SACK (Selective ACK）选项，即 $\color{green}{\text{选择确认选项}}$ 。使用选择确认选项时，TCP看起来就和SR非常相似。因此，TCP的差错恢复机制可视为GBN和SR协议的混合体。</p>
<h4 id="为什么超时时间发生时cwnd被置为1，而收到3个冗余ACK时cwnd减半"><a href="#为什么超时时间发生时cwnd被置为1，而收到3个冗余ACK时cwnd减半" class="headerlink" title="为什么超时时间发生时cwnd被置为1，而收到3个冗余ACK时cwnd减半?"></a>为什么超时时间发生时cwnd被置为1，而收到3个冗余ACK时cwnd减半?</h4><p>大家可以从如下角度考虑。超时事件发生和收到3个冗余ACK，哪个意味着网络拥塞程度更严重﹖通过分析不难发现，在收到3个冗余ACK的情况下，网络虽然拥塞，但至少还有ACK报文段能被正确交付。而当超时发生时，说明网络可能已经拥塞得连 ACK 报文段都传输不了,发送方只能等待超时后重传数据。因此，超时时间发生时，网络拥塞更严重，那么发送方就应该最大限度地抑制数据发送量，所以cwnd置为1;收到3个冗余ACK时，网络拥塞不是很严重,发送方稍微抑制一下发送的数据量即可，所以cwnd 减半。</p>
<h4 id="为什么不采用“两次握手”建立连接呢"><a href="#为什么不采用“两次握手”建立连接呢" class="headerlink" title="为什么不采用“两次握手”建立连接呢?"></a>为什么不采用“两次握手”建立连接呢?</h4><p>这主要是为了防止两次握手情况下已失效的连接请求报文段突然又传送到服务器而产生错误。考虑下面这种情况。客户A向服务器B发出TCP连接请求，第一个连接请求报文在网络的某个结点长时间滞留，A超时后认为报文丢失，于是再重传一次连接请求，B收到后建立连接。数据传输完毕后双方断开连接。而此时，前一个滞留在网络中的连接请求到达服务器B，而B认为A又发来连接请求，此时若使用“三次握手”，则B向A返回确认报文段，由于是一个失效的请求，因此A不予理睬，建立连接失败。若采用的是“两次握手”，则这种情况下B认为传输连接已经建立，并一直等待A传输数据，而A此时并无连接请求，因此不予理睬，这样就造成了B的资源白白浪费。</p>
<h4 id="是否-TCP和UDP都需要计算往返时间RTT"><a href="#是否-TCP和UDP都需要计算往返时间RTT" class="headerlink" title="是否 TCP和UDP都需要计算往返时间RTT?"></a>是否 TCP和UDP都需要计算往返时间RTT?</h4><p>往返时间RTT仅对传输层TCP协议才很重要,因为TCP要根据RTT的值来设置超时计时器的超时时间。</p>
<p>UDP没有确认和重传机制，因此RTT对UDP没有什么意义。</p>
<p>因此，不能笼统地说“往返时间RTT对传输层来说很重要”，因为只有TCP才需要计算RTT，而UDP不需要计算RTT。</p>
<h4 id="为什么TCP在建立连接时不能每次都选择相同的、固定的初始序号"><a href="#为什么TCP在建立连接时不能每次都选择相同的、固定的初始序号" class="headerlink" title="为什么TCP在建立连接时不能每次都选择相同的、固定的初始序号?"></a>为什么TCP在建立连接时不能每次都选择相同的、固定的初始序号?</h4><p>1）假定主机A和B频繁地建立连接，传送一些TCP报文段后，再释放连接，然后又不断地建立新的连接、传送报文段和释放连接。</p>
<p>2）假定每次建立连接时，主机A都选择相同的、固定的初始序号，如选择1。</p>
<p>3）假定主机A发出的某些TCP报文段在网络中会滞留较长时间，以致主机A超时重传这些TCP报文段。</p>
<p>4）假定有一些在网络中滞留时间较长的TCP报文段最后终于到达主机B，但这时传送该报文段的那个连接早已释放，而在到达主机B时的TCP连接是一条新的TCP 连接。</p>
<p>这样，工作在新的TCP连接的主机B就有可能会接收在旧的连接传送的、已无意义的、过时的TCP报文段(因为这个TCP报文段的序号有可能正好处在当前新连接所用的序号范围之中)，结果产生错误。</p>
<p>因此，必须使得迟到的TCP报文段的序号不处在新连接所用的序号范围之中。</p>
<p>这样，TCP在建立新的连接时所选择的初始序号一定要和前面的一些连接所用过的序号不同。因此，不同的TCP连接不能使用相同的初始序号。</p>
<h4 id="假定在一个互联网中，所有链路的传输都不出现差错，所有结点也都不会发生故障。试问在这种情况下，TCP的“可靠交付”的功能是否就是多余的"><a href="#假定在一个互联网中，所有链路的传输都不出现差错，所有结点也都不会发生故障。试问在这种情况下，TCP的“可靠交付”的功能是否就是多余的" class="headerlink" title="假定在一个互联网中，所有链路的传输都不出现差错，所有结点也都不会发生故障。试问在这种情况下，TCP的“可靠交付”的功能是否就是多余的?"></a>假定在一个互联网中，所有链路的传输都不出现差错，所有结点也都不会发生故障。试问在这种情况下，TCP的“可靠交付”的功能是否就是多余的?</h4><p>不是多余的。TCP的“可靠交付”功能在互联网中起着至关重要的作用。至少在以下的情况下，TCP的“可靠交付”功能是必不可少的。</p>
<p>1）每个IP数据报独立地选择路由，因此在到达目的主机时有可能出现失序。</p>
<p>2）由于路由选择的计算出现错误，导致P数据报在互联网中转圈。最后数据报首部中的生存时间（TTL)的数值下降到零。这个数据报在中途就被丢失。</p>
<p>3）某个路由器突然出现很大的通信量，以致路由器来不及处理到达的数据报。因此有的数据报被丢弃。</p>
<p>以上列举的问题表明:必须依靠TCP的“可靠交付”功能才能保证在目的主机的目的进程中接收到正确的报文。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://ednow.github.io/2021/07/24/%E7%8E%8B%E9%81%93-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-ch4-%E7%BD%91%E7%BB%9C%E5%B1%82/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ednow">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ednow">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/24/%E7%8E%8B%E9%81%93-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-ch4-%E7%BD%91%E7%BB%9C%E5%B1%82/" class="post-title-link" itemprop="url">王道-计算机网络-ch4-网络层</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-07-24 09:09:27" itemprop="dateCreated datePublished" datetime="2021-07-24T09:09:27+08:00">2021-07-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-10-09 20:19:10" itemprop="dateModified" datetime="2021-10-09T20:19:10+08:00">2021-10-09</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%80%83%E7%A0%94/" itemprop="url" rel="index"><span itemprop="name">考研</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="王道"><a href="#王道" class="headerlink" title="王道"></a>王道</h1><h2 id="网络层"><a href="#网络层" class="headerlink" title="网络层"></a>网络层</h2><p>【考纲内容】</p>
<p>（一）网络层的功能</p>
<p>异构网络互联;路由与转发; $\color{red}{\text{SDN基本概念}}$ (2022新增)；拥塞控制</p>
<p>（二）路由算法</p>
<p>静态路由与动态路由;距离-向量路由算法;链路状态路由算法;层次路由</p>
<p>（三）IPv4</p>
<p>IPv4分组;IPv4地址与NAT;子网划分与子网掩码、CIDR、路由聚合、ARP、DHCP与ICMP</p>
<p>（四）IPv6</p>
<p>IPv6的主要特点;IPv6地址</p>
<p>（五）路由协议<br>自治系统;域内路由与域间路由;RIP路由协议;OSPF路由协议;BGP路由协议</p>
<p>（六）IP组播</p>
<p>组播的概念;IP组播地址</p>
<p>（七）移动IP</p>
<p>移动IP的概念;移动IP通信过程</p>
<p>（八）网络层设备</p>
<p>路由器的组成和功能;路由表与路由转发</p>
<p>【复习提示】</p>
<p>本章是历年考查的重中之重，尤其是结合第3章、第5章、第6章出综合题的概率很大。其中IPv4以及路由的相关知识点是核心，历年真题都有涉及，因此必须牢固掌握其原理，也要多做题，以便灵活应用。本章的其他知识点，如IP组播、移动IP、IPv6也要有所了解。</p>
<ul>
<li>$\color{red}{\text{Q：}}$ <a target="_blank" rel="noopener" href="https://blog.csdn.net/baichoufei90/article/details/106135952">组播和广播的区别</a></li>
<li>$\color{green}{\text{A：}}$ 组播：1对一组计算机</li>
</ul>
<p>RIP和OSPF在概念上存在一点微妙的互补，导致有点混乱</p>
<ul>
<li>RIP只向相邻路由器交换信息，但交换整个自治系统的信息</li>
<li>OSPF向所有路由器发送信息，只发送相邻路由器的链路状态信息</li>
</ul>
<h3 id="网络层的功能"><a href="#网络层的功能" class="headerlink" title="网络层的功能"></a>网络层的功能</h3><p>互联网在网络层的 $\color{red}{\text{设计思路}}$ 是，向上只提供 $\color{green}{\text{简单灵活的}}$ 、 $\color{green}{\text{无连接的}}$ 、 $\color{green}{\text{尽最大努力交付}}$ 的 $\color{green}{\text{数据报服务}}$ 。也就是说，所传送的分组可能出错、去失、重复、天序或超的，这肌使守内给THA中器比较简单，而且价格低廉。如果主机中的进程之间的通信需要是可靠的，那么可以由更高层的传输层来负责。采用这种设计思路的好处是:网络的造价大大降低，运行方式灵活，能够适应多种应用。互联网能够发展到今日的规模，充分证明了当初采用这种设计思想的正确性。</p>
<h4 id="异构网络互联"><a href="#异构网络互联" class="headerlink" title="异构网络互联"></a>异构网络互联</h4><p>要在全球范围内把数以百万计的网络互联起来，并且能够互相通信，是一项非常复杂的任务，此时需要解决许多问题，比如不同的寻址方案、不同的网络接入机制、不同的差错处理方法、不同的路由选择机制等。用户的需求是多样的，没有一种单一的网络能适应所有用户的需求。网络层所要完成的任务之一就是使这些异构的网络实现互联。</p>
<p>所谓网络互联，是指将两个以上的计算机网络，通过一定的方法，用一种或多种通信处理设备（即中间设备）相互连接起来，以构成更大的网络系统。中间设备又称 $\color{green}{\text{中间系统}}$ 或 $\color{green}{\text{中继系统}}$ 。根据所在的层次，中继系统分为以下4种:</p>
<p>1） $\color{red}{\text{物理层中继系统}}$ :中继器，集线器（Hub)。</p>
<p>2） $\color{red}{\text{数据链路层中继系统}}$ :网桥或交换机。</p>
<p>3） $\color{red}{\text{网络层中继系统}}$ :路由器。</p>
<p>4） $\color{red}{\text{网络层以上的中继系统}}$ :网关。</p>
<p>使用物理层或数据链路层的中继系统时，只是把一个网络扩大了，而从网络层的角度看，它仍然是同一个网络，一般并不称为网络互联。因此网络互联通常是指用路由器进行网络互联和路由选择。路由器是一台专用计算机，用于在互联网中进行路由选择。</p>
<blockquote>
<p>注意:由于历史原因，许多有关TCP/IP的文献也把网络层的路由器称为网关。</p>
</blockquote>
<p>TCP/IP体系在网络互联上采用的做法是在网络层(即IP层）采用标准化协议，但相互连接的网络可以是异构的。图4.1(a)表示用许多计算机网络通过一些路由器进行互联。由于参加互联的计算机网络都使用相同的网际协议(Internet Protocol，IP)，因此可以把互联后的计算机网络视为如图4.1(b)所示的一个虚拟IP网络。</p>
<details><summary>图4.1 IP网的概念</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004121112.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004121112.png';" /></details>

<p>虚拟互联网络也就是逻辑互联网络，即互联起来的各种物理网络的异构性本来是客观存在的，但是通过P就可以使这些性能各异的网络在网络层上看起来好像是一个统一的网络。这种使用IP的虚拟互联网络可简称为IP网络。</p>
<p>使用虚拟互联网络的好处是:当互联网上的主机进行通信时，就好像在一个网络上通信一样，而看不见互联的具体的网络异构细节（如具体的编址方案、路由选择协议等)。</p>
<h4 id="路由与转发"><a href="#路由与转发" class="headerlink" title="路由与转发"></a>路由与转发</h4><p>路由器主要完成两个功能:一是路由选择（确定哪一条路径)，二是分组转发（当一个分组到达时所采取的动作)。前者是根据特定的路由选择协议构造出路由表，同时经常或定期地和相邻路由器交换路由信息而不断地更新和维护路由表。后者处理通过路由器的数据流，关键操作是转发表查询、转发及相关的队列管理和任务调度等。</p>
<p>1） $\color{green}{\text{路由选择}}$ 。指按照复杂的分布式算法，根据从各相邻路由器所得到的关于整个网络拓扑的变化情况，动态地改变所选择的路由。</p>
<p>2） $\color{green}{\text{分组转发}}$ 。指路由器根据转发表将用户的IP数据报从合适的端口转发出去。</p>
<p>路由表是根据路由选择算法得出的，而转发表是从路由表得出的。转发表的结构应当使查找过程最优化，路由表则需要对网络拓扑变化的计算最优化。在讨论路由选择的原理时，往往不去区分转发表和路由表，而是笼统地使用路由表一词。</p>
<h4 id="拥塞控制"><a href="#拥塞控制" class="headerlink" title="拥塞控制"></a>拥塞控制</h4><p>在通信子网中，因出现过量的分组而引起网络性能下降的现象称为拥塞。例如，某个路由器所在链路的带宽为RB/s，如果IP分组只从它的某个端口进入，那么其速率为 $r_{in}$ B/s。当 $r_{in}$ =R时，可能看起来是件“好事”，因为链路带宽被充分利用。但是，如图4.2所示，当分组到达路由器的速率接近R时，平均时延急剧增加，并且会有大量的分组被丢弃（路由器端口的缓冲区是有限的)，整个网络的吞吐量会骤降，源与目的地之间的平均时延也会变得近乎无穷大。</p>
<details><summary>图4.2分组发送速率与时延的关系</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004121321.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004121321.png';" /></details>

<p>判断网络是否进入拥塞状态的方法是，观察网络的吞吐量与网络负载的关系:如果随着网络负载的增加，网络的吞吐量明显小于正常的吞吐量，那么网络就可能已进入“轻度拥塞”状态;如果网络的吞吐量随着网络负载的增大而下降，那么网络就可能已进入拥塞状态;如果网络的负载继续增大，而网络的吞吐量下降到零，那么网络就可能已进入死锁状态。</p>
<p>为避免拥塞现象的出现，要采用能防止拥塞的一系列方法对子网进行拥塞控制。拥塞控制主要解决的问题是如何获取网络中发生拥塞的信息，从而利用这些信息进行控制，以避免由于拥塞而出现分组的丢失，以及严重拥塞而产生网络死锁的现象。</p>
<p>拥塞控制的作用是确保子网能够承载所达到的流量，这是一个全局性的过程，涉及各方面的行为:主机、路由器及路由器内部的转发处理过程等。单一地增加资源并不能解决拥塞。</p>
<p>流量控制和拥塞控制的区别:流量控制往往是指在发送端和接收端之间的点对点通信量的控制。流量控制所要做的是抑制发送端发送数据的速率，以便使接收端来得及接收。而拥塞控制必须确保通信子网能够传送待传送的数据，是一个全局性的问题，涉及网络中所有的主机、路由器及导致网络传输能力下降的所有因素。</p>
<p>拥塞控制的方法有两种:</p>
<p>1） $\color{green}{\text{开环控制}}$ 。在设计网络时事先将有关发生拥塞的因素考虑周到，力求网络在工作时不产生拥塞。这是一种静态的预防方法。一旦整个系统启动并运行，中途就不再需要修改。开环控制手段包括确定何时可接收新流量、何时可丢弃分组及丢弃哪些分组，确定何种调度策略等。所有这些手段的共性是，在做决定时不考虑当前网络的状态。</p>
<p>2） $\color{green}{\text{闭环控制}}$ 。事先不考虑有关发生拥塞的各种因素，采用监测网络系统去监视，及时检测哪里发生了拥塞，然后将拥塞信息传到合适的地方，以便调整网络系统的运行，并解决出现的问题。闭环控制是基于反馈环路的概念，是一种动态的方法。</p>
<h3 id="路由算法"><a href="#路由算法" class="headerlink" title="路由算法"></a>路由算法</h3><h4 id="静态路由与动态路由"><a href="#静态路由与动态路由" class="headerlink" title="静态路由与动态路由"></a>静态路由与动态路由</h4><p>路由器转发分组是通过路由表转发的，而路由表是通过各种算法得到的。从能否随网络的通信量或拓扑自适应地进行调整变化来划分，路由算法可以分为如下两大类。</p>
<p>$\color{green}{\text{静态路由算法}}$ （又称非自适应路由算法)。指由网络管理员手工配置的路由信息。当网络的拓扑结构或链路的状态发生变化时，网络管理员需要手工去修改路由表中相关的静态路由信息。它不能及时适应网络状态的变化，对于简单的小型网络，可以采用静态路由。</p>
<p>$\color{green}{\text{动态路由算法}}$ （又称自适应路由算法)。指路由器上的路由表项是通过相互连接的路由器之间彼此交换信息，然后按照一定的算法优化出来的，而这些路由信息会在一定时间间隙里不断更新，以适应不断变化的网络，随时获得最优的寻路效果。</p>
<p>静态路由算法的特点是简便和开销较小，在拓扑变化不大的小网络中运行效果很好。动态路由算法能改善网络的性能并有助于流量控制;但算法复杂，会增加网络的负担，有时因对动态变化的反应太快而引起振荡，或反应太慢而影响网络路由的一致性，因此要仔细设计动态路由算法，以发挥其优势。常用的动态路由算法可分为 $\color{red}{\text{两类}}$ : $\color{green}{\text{距离-向量路由算法}}$ 和 $\color{green}{\text{链路状态路由算法}}$ 。</p>
<h4 id="距离-向量路由算法"><a href="#距离-向量路由算法" class="headerlink" title="距离-向量路由算法"></a>距离-向量路由算法</h4><p>在距离-向量路由算法中,所有结点都定期地将它们的整个路由选择表传送给所有与之直接相邻的结点。这种路由选择表包含:</p>
<ul>
<li>每条路径的目的地（另一结点)。</li>
<li>路径的代价（也称距离)。</li>
</ul>
<blockquote>
<p>注意:这里的距离是一个抽象的概念，如RIP就将距离定义为“跳数”。跳数指从源端口到达目的端口所经过的路由器个数，每经过一个路由器，跳数加1。</p>
</blockquote>
<p>在这种算法中，所有结点都必须参与距离向量交换，以保证路由的有效性和一致性，也就是说，所有的结点都监听从其他结点传来的路由选择更新信息，并在下列情况下更新它们的路由选择表:</p>
<p>1）被通告一条新的路由，该路由在本结点的路由表中不存在，此时本地系统加入这条新的路由。</p>
<p>2）发来的路由信息中有一条到达某个目的地的路由，该路由与当前使用的路由相比，有较短的距离（较小的代价)。此种情况下，就用经过发送路由信息的结点的新路由替换路由表中到达那个目的地的现有路由。</p>
<p>距离-向量路由算法的实质是，迭代计算一条路由中的站段数或延迟时间，从而得到到达一个目标的最短（最小代价）通路。它要求每个结点在每次更新时都将它的全部路由表发送给所有相邻的结点。显然，更新报文的大小与通信子网的结点个数成正比，大的通信子网将导致很大的更新报文。由于更新报文发给直接邻接的结点，所以所有结点都将参加路由选择信息交换。基于这些原因，在通信子网上传送的路由选择信息的数量很容易变得非常大。</p>
<p>最常见的距离-向量路由算法是 $\color{green}{\text{RIP算法}}$ ，它采用“跳数”作为距离的度量。</p>
<h4 id="链路状态路由算法"><a href="#链路状态路由算法" class="headerlink" title="链路状态路由算法"></a>链路状态路由算法</h4><p>链路状态路由算法要求每个参与该算法的结点都具有完全的网络拓扑信息，它们执行下述两项任务。第一，主动测试所有邻接结点的状态。两个共享一条链接的结点是相邻结点，它们连接到同一条链路，或者连接到同一广播型物理网络。第二，定期地将链路状态传播给所有其他结点(或称路由结点)。典型的链路状态算法是 $\color{green}{\text{OSPF算法}}$ 。</p>
<p>在一个链路状态路由选择中，一个结点检查所有直接链路的状态，并将所得的状态信息发送给网上的所有其他结点，而不是仅送给那些直接相连的结点。每个结点都用这种方式从网上所有其他的结点接收包含直接链路状态的路由选择信息。</p>
<p>每当链路状态报文到达时，路由结点便使用这些状态信息去更新自己的网络拓扑和状态“视野图”，一旦链路状态发生变化,结点就对更新的网络图利用Dijkstra $\color{green}{\text{最短路径算法}}$ 重新计算路由,从单一的源出发计算到达所有目的结点的最短路径。</p>
<p>链路状态路由算法主要有三个特征:</p>
<p>1）向本自治系统中所有路由器发送信息，这里使用的方法是泛洪法，即路由器通过所有端口向所有相邻的路由器发送信息。而每个相邻路由器又将此信息发往其所有相邻路由器(但不再发送给刚刚发来信息的那个路由器)。</p>
<p>2）发送的信息是与路由器相邻的所有路由器的链路状态，但这只是路由器所知道的部分信息。所谓“链路状态”，是指说明本路由器与哪些路由器相邻及该链路的“度量”。对于OSPF 算法，链路状态的“度量”主要用来表示费用、距离、时延、带宽等。</p>
<p>3）只有当链路状态发生变化时，路由器才向所有路由器发送此信息。</p>
<p>由于一个路由器的 $\color{green}{\text{链路状态}}$ 只涉及 $\color{green}{\text{相邻路由器}}$ 的连通状态，而与整个互联网的规模并无直接关系，因此链路状态路由算法可以用于大型的或路由信息变化聚敛的互联网环境。</p>
<p>链路状态路由算法的主要优点是，每个路由结点都使用同样的原始状态数据独立地计算路径，而不依赖中间结点的计算;链路状态报文不加改变地传播，因此采用该算法易于查找故障。当一个结点从所有其他结点接收到报文时，它可以在本地立即计算正确的通路，保证一步汇聚。最后，由于链路状态报文仅运载来自单个结点关于直接链路的信息，其大小与网络中的路由结点数目无关，因此链路状态算法比距离-向量算法有更好的规模可伸展性。</p>
<p>距离-向量路由算法与链路状态路由算法的比较:在距离-向量路由算法中，每个结点仅与它的直接邻居交谈，它为它的邻居提供从自己到网络中所有其他结点的最低费用估计。在链路状态路由算法中，每个结点通过广播的方式与所有其他结点交谈，但它仅告诉它们与它直接相连的链路的费用。相较之下，距离-向量路由算法有可能遇到 $\color{green}{\text{路由环路}}$ 等问题。</p>
<h4 id="层次路由"><a href="#层次路由" class="headerlink" title="层次路由"></a>层次路由</h4><p>当网络规模扩大时，路由器的路由表成比例地增大。这不仅会消耗越来越多的路由器缓冲区空间，而且需要用更多CPU时间来扫描路由表，用更多的带宽来交换路由状态信息。因此路由选择必须按照层次的方式进行。</p>
<p>因特网将整个互联网划分为许多较小的自治系统（注意一个自治系统中包含很多局域网)，每个自治系统有权自主地决定本系统内应采用何种路由选择协议。如果两个自治系统需要通信，那么就需要一种在两个自治系统之间的协议来屏蔽这些差异。据此，因特网把路由选择协议划分为两大类:</p>
<p>1）一个自治系统内部所使用的路由选择协议称为 $\color{green}{\text{内部网关协议}}$ (IGP)，也称域内路由选择，具体的协议有RIP和 OSPF等。</p>
<p>2）自治系统之间所使用的路由选择协议称为 $\color{green}{\text{外部网关协议}}$ （EGP)（Exterior Gateway Protocol），也称域间路由选择，用在不同自治系统的路由器之间交换路由信息，并负责为分组在不同自治系统之间选择最优的路径。具体的协议有BGP。</p>
<p>使用层次路由时，OSPF将一个自治系统再划分为若干区域(Area)，每个路由器都知道在本区域内如何把分组路由到目的地的细节，但不用知道其他区域的内部结构。</p>
<p>采用分层次划分区域的方法虽然会使交换信息的种类增多，但也会使OSPF协议更加复杂。但这样做却能使每个区域内部交换路由信息的通信量大大减小，因而使OSPF协议能够用于规模很大的自治系统中。</p>
<h3 id="IPv4"><a href="#IPv4" class="headerlink" title="IPv4"></a>IPv4</h3><ul>
<li>可以配合期末的总结</li>
</ul>
<h4 id="IPv4分组"><a href="#IPv4分组" class="headerlink" title="IPv4分组"></a>IPv4分组</h4><p>IPv4即现在普遍使用的IP（版本4)。IP定义数据传送的基本单元-—IP分组及其确切的数据格式。IP也包括一套规则，指明分组如何处理、错误怎样控制。特别是IP还包含非可靠投递的思想，以及与此关联的分组路由选择的思想。</p>
<h5 id="IPv4分组的格式"><a href="#IPv4分组的格式" class="headerlink" title="IPv4分组的格式"></a>IPv4分组的格式</h5><p>一个IP分组由首部和数据部分组成。首部前一部分的长度固定，共20B，是所有IP分组必须具有的。在首部固定部分的后面是一些可选字段，其长度可变，用来提供错误检测及安全等机制。IP数据报的格式如图4.3所示。</p>
<details><summary>图4.3 IP数据报的格式</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004122131.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004122131.png';" /></details>

<p>IP首部的部分重要字段含义如下:</p>
<p>1）版本。指IP的版本，目前广泛使用的版本号为4。</p>
<p>2）首部长度。占4位，可以表示的最大十进制数是15。以32位为单位，最大值为60B(15x4B)。最常用的首部长度是20B，此时不使用任何选项（即可选字段)。</p>
<p>3）总长度。占16位。指首部和数据之和的长度，单位为字节，因此数据报的最大长度为 $2^{16}$ -1=65535B。以太网帧的最大传送单元(MTU)为1500B，因此当一个IP数据报封装成帧时，数据报的总长度（首部加数据）一定不能超过下面的数据链路层的MTU值。</p>
<p>4）标识。占16位。它是一个计数器，每产生一个数据报就加1，并赋值给标识字段。但它并不是“序号”(因为IP是无连接服务)。当一个数据报的长度超过网络的MTU时，必须分片，此时每个数据报片都复制一次标识号，以便能正确重装成原来的数据报。</p>
<p>5）标志。占3位。标志字段的最低位为MF，MF = 1表示后面还有分片，MF =0表示最后一个分片。标志字段中间的一位是DF，只有当DF=0时才允许分片。【格式为0、DF、MF】</p>
<p>6）片偏移。占13位。它指出较长的分组在分片后，某片在原分组中的相对位置。片偏移以8个字节为偏移单位，即每个分片的长度一定是8B(64位）的整数倍。</p>
<p>7）生存时间(TTL)。占8位。数据报在网络中可通过的路由器数的最大值，标识分组在网络中的寿命，以确保分组不会永远在网络中循环。路由器在转发分组前，先把TTL减1。若TTL被减为0，则该分组必须丢弃。</p>
<p>8）协议。占8位。指出此分组携带的数据使用何种协议，即分组的数据部分应交给哪个传输层协议，如TCP、UDP等。其中值为6表示TCP，值为17表示UDP。</p>
<p>9）首部校验和。占16位。IP数据报的首部校验和只校验分组的首部，而不校验数据部分。10）源地址字段。占4B，标识发送方的IP地址。</p>
<p>11）目的地址字段。占4B，标识接收方的IP地址。</p>
<blockquote>
<p>注意:在IP数据报首部中有三个关于长度的标记，一个是首部长度、一个是总长度、一个是片偏移，基本单位分别为4B、1B、8B(这个一定要记住)。题目中经常会出现这几个长度之间的加减运算。另外，读者要熟悉IP数据报首部的各个字段的意义和功能，但不需要记忆IP数据报的首部，正常情况下如果需要参考首部，题目都会直接给出。第5章学到的TCP、UDP的首部也是一样的。</p>
</blockquote>
<h5 id="数据报分片"><a href="#数据报分片" class="headerlink" title="数据报分片"></a>数据报分片</h5><p>一个数据链路层数据报能承载的最大数据量称为最大传送单元(MTU)。因为IP数据报被封装在数据链路层数据报中，因此数据链路层的 MTU严格地限制着IP数据报的长度，而且在IP数据报的源与目的地路径上的各段链路可能使用不同的数据链路层协议，有不同的MTU。例如，以太网的MTU为1500B，而许多广域网的MTU不超过576B。当IP数据报的总长度大于链路MTU时，就需要将IP数据报中的数据分装在两个或多个较小的IP数据报中，这些较小的数据报称为 $\color{green}{\text{片}}$ 。</p>
<p>片在目的地的网络层被重新组装。目的主机使用IP 首部中的标识、标志和片偏移字段来完成对片的重组。创建一个IP数据报时，源主机为该数据报加上一个标识号。当一个路由器需要将一个数据报分片时，形成的每个数据报（即片）都具有原始数据报的标识号。当目的主机收到来自同一发送主机的一批数据报时，它可以通过检查数据报的标识号来确定哪些数据报属于同一个原始数据报的片。P首部中的标志位有3比特，但只有后2比特有意义，分别是MF位.(MoreFragment）和DF位(Don’t Fragment)。只有当DF =0时，该IP数据报才可以被分片。MF则用来告知目的主机该IP数据报是否为原始数据报的最后一个片。当MF= 1时，表示相应的原始数据报还有后续的片;当MF= 0时，表示该数据报是相应原始数据报的最后一个片。 $\color{red}{\text{目的主机}}$ 在 $\color{green}{\text{对片进行重组}}$ 时，使用片偏移字段来确定片应放在原始数据报的哪个位置。</p>
<p>IP分片涉及一定的计算。例如，一个长4000B的IP数据报（首部20B，数据部分3980B)到达一个路由器，需要转发到一条MTU为1500B 的链路上。这意味着原始数据报中的3980B数据必须被分配到3个独立的片中（每片也是一个IP数据报)。假定原始数据报的标识号为777，那么分成的3片如图4.4所示。可以看出，由于偏移值的单位是8B，所以除最后一个片外，其他所有片中的有效数据载荷都是8的倍数。</p>
<details><summary>图4.4 IP分片的例子</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004122635.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004122635.png';" /></details>

<h5 id="网络层转发分组的流程"><a href="#网络层转发分组的流程" class="headerlink" title="网络层转发分组的流程"></a>网络层转发分组的流程</h5><p>网络层的路由器执行的分组转发算法如下:</p>
<p>1）从数据报的首部提取目的主机的IP地址D，得出目的网络地址N。</p>
<p>2）若网络N与此路由器直接相连，则把数据报直接交付给目的主机 D，这称为路由器的直接交付;否则是间接交付，执行步骤3）。</p>
<p>3）若路由表中有目的地址为D的特定主机路由（对特定的目的主机指明一个特定的路由，通常是为了控制或测试网络，或出于安全考虑才采用的)，则把数据报传送给路由表中所指明的下一跳路由器;否则，执行步骤4）。</p>
<p>4）若路由表中有到达网络N的路由，则把数据报传送给路由表指明的下一跳路由器;否则，执行步骤5）。</p>
<p>5）若路由表中有一个默认路由，则把数据报传送给路由表中所指明的默认路由器;否则,执行步骤6)。</p>
<p>6）报告转发分组出错。</p>
<blockquote>
<p>注意:得到下一跳路由器的IP地址后并不是直接将该地址填入待发送的数据报，而是将该IP地址转换成MAC地址(通过ARP)，将其放到MAC帧首部中，然后根据这个MAC地址找到下一跳路由器。在不同网络中传送时，MAC帧中的源地址和目的地址要发生变化,但是网桥在转发帧时，不改变帧的源地址，请注意区分。</p>
</blockquote>
<h4 id="IPv4地址与NAT"><a href="#IPv4地址与NAT" class="headerlink" title="IPv4地址与NAT"></a>IPv4地址与NAT</h4><h5 id="IPv4地址"><a href="#IPv4地址" class="headerlink" title="IPv4地址"></a>IPv4地址</h5><p>连接到因特网上的每台主机(或路由器）都分配一个32比特的全球唯一标识符，即IP地址。传统的IP地址是分类的地址，分为A、B、C、D、E五类。</p>
<p>无论哪类IP地址，都由网络号和主机号两部分组成。即IP地址：：={&lt;网络号&gt;,&lt;主机号&gt;}。其中网络号标志主机（或路由器）所连接到的网络。一个网络号在整个因特网范围内必须是唯一的。主机号标志该主机（或路由器）。一台主机号在它前面的网络号所指明的网络范围内必须是唯一的。由此可见，一个IP地址在整个因特网范围内是唯一的。</p>
<p>分类的IP地址如图4.5所示。</p>
<details><summary>图4.5分类的IP地址</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004123057.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004123057.png';" /></details>

<p>在各类IP地址中，有些IP地址具有特殊用途，不用做主机的IP地址:</p>
<ul>
<li><p>主机号全为0表示本 $\color{green}{\text{网络本身}}$ ，如202.98.174.0。</p>
</li>
<li><p>主机号全为1表示本网络的 $\color{green}{\text{广播地址}}$ ，又称 $\color{green}{\text{直接广播地址}}$ ，如202.98.174.255。</p>
</li>
<li><p>127.0.0.0保留为 $\color{green}{\text{环回自检(Loopback Test）地址}}$ ，此地址表示任意主机本身，目的地址为环回地址的IP数据报永远不会出现在任何网络上。</p>
</li>
<li><p>32位全为0，即0.0.0.0表示本网络上的本主机。</p>
</li>
<li><p>32位全为1，即255.255.255.255表示整个TCP/IP网络的广播地址，又称 $\color{green}{\text{受限广播地址}}$ 。</p>
</li>
<li><p>127.0.0.0到127.255.255.255都是保留地址，<a target="_blank" rel="noopener" href="https://zhidao.baidu.com/question/346570479.html">参考文献</a></p>
</li>
<li><details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211008222112.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211008222112.png';" /></details></li>
</ul>
<p>实际使用时，由于路由器对广播域的隔离，255.255.255.255等效为 $\color{green}{\text{本网络的广播地址}}$ 。</p>
<details><summary>表4.1 常用的三种类别IP地址的使用范围</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004123412.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004123412.png';" /></details>

<p>在表4.1中，A类地址可用的网络数为 $2^7$ -2，减2的原因是:第一，网络号字段全为0的IP地址是保留地址，意思是“本网络”;第二，网络号为127的IP地址是环回自检地址。B类地址的可用网络数为 $2^{14}$ -1，减1的原因是128.0这个网络号是 $\color{green}{\text{不可指派}}$ 的。C类地址的可用网络数为 $2^{21}$ -1，减1的原因是网络号为192.0.0的网络是 $\color{green}{\text{不可指派}}$ 的。</p>
<p>IP地址有以下重要特点:</p>
<p>1）每个IP地址都由网络号和主机号两部分组成，因此IP地址是一种分等级的地址结构。分等级的好处是:①IP地址管理机构在分配IP地址时只分配网络号（第一级），而主机号（第二级）则由得到该网络的单位自行分配，方便了IP地址的管理;②路由器仅根据目的主机所连接的网络号来转发分组（而不考虑目标主机号），从而减小了路由表所占的存储空间。</p>
<p>2）IP地址是标志一台主机（或路由器）和一条链路的接口。当一台主机同时连接到两个网络时，该主机就必须同时具有两个相应的IP地址，每个IP地址的网络号必须与所在网络的网络号相同，且这两个IP地址的网络号是不同的。因此IP网络上的一个路由器必然至少应具有两个IP地址（路由器每个端口必须至少分配一个IP地址）。</p>
<p>3）用转发器或桥接器（网桥等）连接的若干LAN仍然是同一个网络（同一个广播域），因此该LAN中所有主机的IP地址的网络号必须相同，但主机号必须不同。</p>
<p>4）在IP地址中，所有分配到网络号的网络（无论是LAN还是WAN）都是平等的。</p>
<p>5）在同一个局域网上的主机或路由器的IP地址中的网络号必须是一样的。路由器总是具有 $\color{green}{\text{两个}}$ 或 $\color{green}{\text{两个以上}}$ 的IP地址，路由器的每个端口都有一个不同网络号的IP地址。</p>
<h5 id="网络地址转换"><a href="#网络地址转换" class="headerlink" title="网络地址转换"></a>网络地址转换</h5><p>网络地址转换(NAT)是指通过将专用网络地址(如 Intranet）转换为公用地址(如Internet),从而对外隐藏内部管理的IP地址。它使得整个专用网只需要一个全球IP地址就可以与因特网连通，由于专用网本地IP地址是可重用的，所以NAT大大节省了IP地址的消耗。同时，它隐藏了内部网络结构，从而降低了内部网络受到攻击的风险。</p>
<p>此外，为了网络安全，划出了部分IP地址为私有IP地址。私有IP地址只用于LAN，不用于WAN连接(因此私有IP地址不能直接用于Internet，必须通过网关利用NAT 把私有IP地址转换为Internet中合法的全球IP地址后才能用于Internet)，并且允许私有IP地址被LAN重复使用。这有效地解决了IP地址不足的问题。私有IP地址网段如下:</p>
<p>A类:1个A类网段，即 $\color{green}{\text{10}}$ .0.0.0～ $\color{green}{\text{10}}$ .255.255.255。</p>
<p>B类:16个B类网段，即 $\color{green}{\text{172.16}}$ .0.0～ $\color{green}{\text{172.31}}$ .255.255。</p>
<p>C类:256个C类网段，即 $\color{green}{\text{192.168.0}}$ .0～ $\color{green}{\text{192.168.255}}$ .255。</p>
<p>在因特网中的所有路由器，对目的地址是私有地址的数据报一律不进行转发。这种采用私有IP地址的互联网络称为 $\color{green}{\text{专用互联网}}$ 或 $\color{green}{\text{本地互联网}}$ 。私有IP地址也称 $\color{green}{\text{可重用地址}}$ 。</p>
<p>下面以宿舍共享宽带上网为例进行说明。假设某个宿舍办理了2Mbit/s的电信宽带，那么这个宿舍就获得了一个全球IP地址(如138.76.29.7),而宿舍内4台主机使用私有地址(如192.168.0.0网段)。宿舍的网关路由器应该开启NAT功能，并且某时刻路由器上的NAT 转换表见表4.2。那么,当路由器从LAN端口收到源IP及源端口号为192.168.0.2:2233的数据报时，就将其映射成138.76.29.7: 5001，然后从WAN端口发送到因特网上。当路由器从WAN端口收到目的IP及目的端口号为138.76.29.7:5060的数据报时，就将其映射成192.168.0.3: 1234，然后从LAN端口发送给相应的本地主机。这样，只需要一个全球地址，就可以让多台主机同时访问因特网。</p>
<blockquote>
<p>注意:普通路由器在转发IP数据报时，不改变其源IP地址和目的IP地址。而NAT路由器在转发IP数据报时，一定要更换其IP地址（转换源IP地址或目的IP地址)。普通路由器仅工作在网络层，而NAT路由器转发数据报时需要查看和转换传输层的端口号。</p>
</blockquote>
<details><summary>表4.2一个典型的NAT转换表</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004142050.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004142050.png';" /></details>




<h4 id="子网划分与子网掩码、CIDR"><a href="#子网划分与子网掩码、CIDR" class="headerlink" title="子网划分与子网掩码、CIDR"></a>子网划分与子网掩码、CIDR</h4><h5 id="子网划分"><a href="#子网划分" class="headerlink" title="子网划分"></a>子网划分</h5><p>两级IP地址的缺点:IP地址空间的利用率有时很低;给每个物理网络分配一个网络号会使路由表变得太大而使网络性能变坏;两级的IP地址不够灵活。</p>
<p>从1985年起,在IP地址中又增加了一个“子网号字段”，使两级IP地址变成了三级IP地址。这种做法称为 $\color{green}{\text{子网划分}}$ 。子网划分已成为因特网的正式标准协议。</p>
<p>子网划分的基本思路如下:</p>
<ul>
<li>子网划分纯属一个单位内部的事情。单位对外仍然表现为没有划分子网的网络。</li>
<li>从主机号借用若干比特作为子网号，当然主机号也就相应减少了相同的比特。三级IP地址的结构如下:IP地址={&lt;网络号&gt;,&lt;子网号&gt;,&lt;主机号&gt;}。</li>
<li>凡是从其他网络发送给本单位某台主机的IP数据报,仍然是根据IP数据报的目的网络号，先找到连接到本单位网络上的路由器。然后该路由器在收到IP数据报后，按目的网络号和子网号找到目的子网。最后把IP数据报直接交付给目的主机。</li>
</ul>
<p>注意:</p>
<p>1）划分子网只是把IP地址的主机号这部分进行再划分，而不改变IP地址原来的网络号。因此，从一个IP地址本身或IP数据报的首部，无法判断源主机或目的主机所连接的网络是否进行了子网划分。</p>
<p>2） RFC 950规定，对分类的IPv4地址进行子网划分时，子网号不能为全1或全0。但随着CIDR的广泛使用，现在全1和全0的子网号也可使用，但一定要谨慎使用，要弄清你的路由器所用的路由选择软件是否支持 $\color{green}{\text{全0}}$ 或 $\color{green}{\text{全1}}$ 的子网号。</p>
<p>3）不论是分类的IPv4地址还是CIDR，其子网中的主机号为全0或全1的地址都 $\color{green}{\text{不能被指派}}$ 。子网中主机号全0的地址为子网的网络号，主机号全1的地址为子网的广播地址。</p>
<h5 id="子网掩码"><a href="#子网掩码" class="headerlink" title="子网掩码"></a>子网掩码</h5><p>为了告诉主机或路由器对一个A类、B类、C类网络进行了子网划分，使用子网掩码来表达对原网络中主机号的借位。</p>
<p>子网掩码是一个与IP地址相对应的、长32bit的二进制串，它由一串1和跟随的一串0组成。其中，1对应于IP地址中的网络号及子网号，而0对应于主机号。计算机只需将IP地址和其对应的子网掩码逐位“与”(逻辑AND运算)，就可得出相应子网的网络地址。</p>
<p>现在的因特网标准规定:所有的网络都必须使用子网掩码。如果一个网络未划分子网，那么就采用默认子网掩码。A、B、C类地址的默认子网掩码分别为255.0.0.0、255.255.0.0、255.255.255.0。例如，某主机的IP地址192.168.5.56，子网掩码为255.255.255.0，进行逐位“与”运算后，得出该主机所在子网的网络号为192.168.5.0。</p>
<p>由于子网掩码是一个网络或一个子网的重要属性，所以路由器在相互之间交换路由信息时，必须把自己所在网络（或子网）的子网掩码告诉对方。路由表中的每个条目，除要给出目的网络地址和下跳地址外，还要同时给出该目的网络的子网掩码。</p>
<p>在使用子网掩码的情况下:</p>
<p>1）一台主机在设置IP地址信息的同时，必须设置子网掩码。<br>2）同属于一个子网的所有主机及路由器的相应端口，必须设置相同的子网掩码。<br>3）路由器的路由表中，所包含信息的主要内容必须有目的网络地址、子网掩码、下一跳地址。</p>
<p>使用子网掩码时路由器的分组转发算法如下:</p>
<p>1）从收到的分组的首部提取目的IP地址，记为D。</p>
<p>2）先判断是否为直接交付。对路由器直接相连的网络逐个进行检查:用各网络的子网掩码和D逐位相“与”，看结果是否和相应的网络地址匹配。若匹配，则将分组直接交付，否则间接交付，执行步骤3)。</p>
<p>3）若路由表中有目的地址为D的特定主机路由，则将分组传送给路由表中所指明的下一跳路由器;否则，执行4)。</p>
<p>4）对路由表中的每一行（目的网络地址、子网掩码、下一跳地址）中的子网掩码和D逐位相“与”，其结果为N。若N与该行的目的网络地址匹配，则将分组传送给该行指明的下一跳路由器;否则，执行步骤5)。</p>
<p>5）若路由表中有一个默认路由，则将分组传送给路由表中所指明的默认路由器;否则，执行步骤6)。</p>
<p>6）报告转发分组出错。</p>
<h5 id="无分类域间路由选择-CIDR"><a href="#无分类域间路由选择-CIDR" class="headerlink" title="无分类域间路由选择(CIDR)"></a>无分类域间路由选择(CIDR)</h5><p>pp.161</p>
<p>无分类域间路由选择是在变长子网掩码的基础上提出的一种消除传统A、B、C类网络划分，并且可以在软件的支持下实现超网构造的一种IP地址的划分方法。</p>
<p>例如，如果一个单位需要2000个地址，那么就给它分配一个2048地址的块（8个连续的C类网络)，而不是一个完全的B类地址。这样可以大幅度提高IP地址空间的利用率，减小路由器的路由表大小，提高路由转发能力。CIDR的主要特点如下:</p>
<p>1）消除了传统A、B、C类地址及划分子网的概念，因而可以更有效地分配IPv4的地址空间。CIDR使用“网络前缀”的概念代替子网络的概念。因此，IP地址的无分类两级编址为:IP:={&lt;网络前缀&gt;，&lt;主机号&gt;}。</p>
<p>CIDR还使用“斜线记法”(或称CIDR记法)，即 IP地址/网络前缀所占比特数。其中，网络前缀所占比特数对应于网络号的部分，等效于子网掩码中连续1的部分。例如，对于128.14.32.5/20这个地址，它的掩码是20个连续的1和后续12个连续的0，通过逐位相“ $\color{green}{\text{与}}$ ”的方法可以得到该地址的网络前缀（或直接截取前20位):</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005093747.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005093747.png';" /></details>

<p>CIDR虽然不使用子网，但仍然使用“掩码”一词。”CIDR不使用子网”是指CIDR并没有在32位地址中指明若干位作为子网字段。但分配到一个CIDR地址块的组织，仍可以在本组织内根据需要划分出一些子网。例如，某组织分配到地址块/20，就可以再继续划分为8个子网（从主机号中借用3位来划分子网)，这时每个子网的网络前缀就变成了23位。全0和全1的主机号地址一般不使用。</p>
<p>2）将网络前缀都相同的连续IP地址组成“CIDR地址块”。一个CIDR地址块可以表示很多地址，这种地址的聚合称为 $\color{green}{\text{路由聚合}}$ ，或称 $\color{green}{\text{构成超网}}$ 。路由聚合使得路由表中的一个项目可以表示多个原来传统分类地址的路由，有利于减少路由器之间的路由选择信息的交换，从而提高网络性能。</p>
<p>例如，在如图4.6所示的网络中，如果不使用路由聚合，那么R1的路由表中需要分别有到网络1和网络2的路由表项。不难发现，网络1和网络2的网络前缀在二进制表示的情况下，前16位都是相同的，第17位分别是0和1，并且从R1到网络1和网络2的路由的下一跳皆为R2。若使用路由聚合，在R1看来，网络1和网络2可以构成一个更大的地址块206.1.0.0/16，到网络1和网络2的两条路由就可以聚合成一条到206.1.0.0/16的路由。</p>
<details><summary>图4.6路由聚合的例子
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005094050.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005094050.png';" /></details>

<p>CIDR地址块中的地址数一定是2的整数次幂，实际可指派的地址数通常为 $2^N$ -2,N表示主机号的位数，主机号全0代表网络号，主机号全1为广播地址。网络前缀越短，其地址块所包含的地址数就越多。而在三级结构的IP地址中，划分子网使网络前缀变长。</p>
<p>CIDR的优点在于网络前缀长度的灵活性。由于上层网络的前缀长度较短，因此相应的路由表的项目较少。而内部又可采用延长网络前缀的方法来灵活地划分子网。</p>
<p>最长前缀匹配（最佳匹配):使用CIDR时，路由表中的每个项目由“网络前缀”和“下一跳地址”组成。在查找路由表时可能会得到不止一个匹配结果。此时，应当从匹配结果中选择具有最长网络前缀的路由，因为网络前缀越长，其地址块就越小，因而路由就越具体。</p>
<p>CIDR查找路由表的方法:为了更加有效地查找最长前缀匹配，通常将无分类编址的路由表存放在一种层次式数据结构中，然后自上而下地按层次进行查找。这里最常用的数据结构就是 $\color{green}{\text{二叉线索}}$ 。</p>
<h4 id="ARP、DHCP-与ICMP"><a href="#ARP、DHCP-与ICMP" class="headerlink" title="ARP、DHCP 与ICMP"></a>ARP、DHCP 与ICMP</h4><h5 id="IP地址与硬件地址"><a href="#IP地址与硬件地址" class="headerlink" title="IP地址与硬件地址"></a>IP地址与硬件地址</h5><p>IP地址是网络层使用的地址，它是分层次等级的。硬件地址是数据链路层使用的地址（如MAC地址)，它是平面式的。在网络层及网络层之上使用IP地址，IP地址放在IP数据报的首部.而MAC地址放在MAC帧的首部。通过数据封装，把IP数据报分组封装为MAC帧后，数据链路层看不见数据报分组中的IP地址。</p>
<p>由于路由器的隔离，IP网络中无法通过广播方式依靠MAC地址来完成跨网络的寻址，因此在IP网络的网络层只使用IP地址来完成寻址。寻址时，每个路由器依据其路由表（依靠静态路由或动态路由协议生成）选择到目标网络（即主机号全为0的网络地址）需要转发到的下一跳（路由器的物理端口号或下一网络地址)，而IP分组通过多次路由转发到达目标网络后，改为在目标LAN中通过数据链路层的 MAC地址以广播方式寻址。这样可以提高路由选择的效率。</p>
<p>1）在IP层抽象的互联网上只能看到IP数据报。</p>
<p>2）虽然在IP数据报首部中有完整的源IP地址和目的IP地址，但路由器只根据目的地址的网络号进行路由选择。</p>
<p>3)在局域网的数据链路层，只能看见 MAC帧。而通过路由器转发IP分组时，此IP分组在每个网络中都被路由器解封装和重新封装。因此IP数据报在被路由器转发时，其数据链路层封装所使用的MAC地址是不断改变的。这也决定了无法使用MAC地址跨网络通信。</p>
<p>4)尽管互联在一起的网络的硬件地址体系各不相同，但IP层抽象的互联网却屏蔽了下层这些复杂的细节。只要我们在网络层上讨论问题，就能够使用统一的、抽象的IP地址研究主机与主机或路由器之间的通信。</p>
<h5 id="地址解析协议-ARP"><a href="#地址解析协议-ARP" class="headerlink" title="地址解析协议(ARP)"></a>地址解析协议(ARP)</h5><p>无论网络层使用什么协议，在实际网络的链路上传送数据帧时，最终必须使用硬件地址。所以需要一种方法来完成IP地址到 MAC地址的映射，这就是 $\color{green}{\text{地址解析协议}}$ （Address ResolutionProtocol，ARP)。每台主机都设有一个ARP 高速缓存，用来存放本局域网上各主机和路由器的IP地址到MAC地址的映射表，称ARP表。使用ARP来动态维护此ARP表。</p>
<p>ARP 工作在网络层，其工作原理如下:主机A欲向本局域网上的某台主机B发送IP数据报时，先在其ARP高速缓存中查看有无主机B的IP地址。如有，就可查出其对应的硬件地址，再将此硬件地址写入MAC 帧，然后通过局域网将该MAC帧发往此硬件地址。如果没有，那么就通过使用目的MAC地址为 $\color{green}{\text{FF-FF-FF-FF-FF-FF}}$  的帧来封装并 $\color{green}{\text{广播ARP请求分组}}$ ，使同一个局域网里的所有主机收到ARP请求。主机B收到该ARP请求后，向主机A发出响应ARP分组，分组中包含主机B的IP与MAC地址的映射关系，主机A在收到后将此映射写入ARP缓存，然后按查询到的硬件地址发送MAC帧。 $\color{red}{\text{ARP}}$ 由于“看到了”IP地址，所以它工作在 $\color{green}{\text{网络层}}$ ，而NAT路由器由于“看到了” $\color{red}{\text{端口}}$ ，所以它工作在 $\color{green}{\text{传输层}}$ 。对于某个协议工作在哪个层次，读者应该能通过协议的工作原理进行猜测。</p>
<blockquote>
<p>注意:ARP用于解决 $\color{green}{\text{同一个局域网上}}$ 的主机或路由器的IP地址和硬件地址的映射问题。如果所要找的主机和源主机不在同一个局域网上,那么就要通过ARP找到一个位于本局域网上的某个路由器的硬件地址，然后把分组发送给这个路由器，让这个路由器把分组转发给下一个网络。剩下的工作就由下一个网络来做，尽管ARP请求分组是广播发送的，但ARP响应分组是普通的单播，即从一个源地址发送到一个目的地址。</p>
</blockquote>
<p>ARP的4种典型情况总结如下:</p>
<p>1)发送方是主机时，要把IP数据报发送到本网络上的另一台主机。这时用ARP找到目的主机的硬件地址。</p>
<p>2)发送方是主机时，要把IP数据报发送到另一个网络上的一台主机。这时用ARP找到本网络上的一个路由器的硬件地址，剩下的工作由这个路由器来完成。</p>
<p>3)发送方是路由器时，要把IP数据报转发到本网络上的一台主机。这时用ARP找到目的主机的硬件地址。</p>
<p>4)发送方是路由器时，要把IP数据报转发到另一个网络上的一台主机。这时用ARP找到本网络上的一个路由器的硬件地址，剩下的工作由找到的这个路由器完成。</p>
<p>从IP 地址到硬件地址的解析是自动进行的，主机的用户并不知道这种地址解析过程。只要主机或路由器和本网络上的另一个已知IP地址的主机或路由器进行通信，ARP 就会自动地将这个IP地址解析为数据链路层所需要的硬件地址。</p>
<h5 id="动态主机配置协议-DHCP"><a href="#动态主机配置协议-DHCP" class="headerlink" title="动态主机配置协议(DHCP)"></a>动态主机配置协议(DHCP)</h5><p>动态主机配置协议（Dynamic Host Configuration Protocol，DHCP）常用于给主机动态地分配IP地址，它提供了即插即用的联网机制，这种机制允许一台计算机加入新的网络和获取IP地址而不用手工参与。DHCP是 $\color{green}{\text{应用层}}$ 协议，它是基于 $\color{green}{\text{UDP}}$ 的。</p>
<p>DHCP 的工作原理如下:使用客户/服务器方式。需要IP地址的主机在启动时就向 DHCP服务器广播发送发现报文，这时该主机就成为DHCP客户。本地网络上所有主机都能收到此广播报文，但只有DHCP服务器才回答此广播报文。DHCP服务器先在其数据库中查找该计算机的配置信息。若找到，则返回找到的信息。若找不到，则从服务器的IP地址池中取一个地址分配给该计算机。DHCP服务器的回答报文称为 $\color{green}{\text{提供报文}}$ 。</p>
<p>DHCP服务器聚合DHCP客户端的交换过程如下:</p>
<p>1)DHCP客户机广播“DHCP发现”消息，试图找到网络中的DHCP 服务器，以便从 DHCP服务器获得一个IP地址。</p>
<p>2)DHCP服务器收到“DHCP发现”消息后，向网络中广播“DHCP提供”消息，其中包括提供 DHCP客户机的IP地址和相关配置信息。</p>
<p>3)DHCP客户机收到“DHCP提供”消息，如果接收DHCP服务器所提供的相关参数，那么通过广播“DHCP请求”消息向DHCP服务器请求提供IP地址。</p>
<p>4)DHCP服务器广播“DHCP确认”消息，将IP地址分配给DHCP客户机。</p>
<p>DHCP 允许网络上配置多台DHCP服务器，当DHCP客户机发出 DHCP请求时，有可能收到多个应答消息。这时，DHCP客户机只会挑选其中的一个，通常挑选最先到达的。</p>
<p>DHCP服务器分配给DHCP客户的地址是临时的，因此 DHCP客户只能在一段有限的时间内使用这个分配到的IP地址。DHCP称这段时间为租用期。租用期的数值应由 DHCP服务器自己决定，DHCP客户也可在自己发送的报文中提出对租用期的要求。</p>
<p>DHCP是应用层协议，因为它是通过客户/服务器方式工作的，DHCP客户端向DHCP服务器请求服务。读者在后面的学习中会了解到，应用层协议有两种工作方式:客户/服务器方式和P2P方式，而其他层次的协议是没有这两种工作方式的。</p>
<p>DHCP的客户端和服务器端需要通过广播方式来进行交互，原因是在DHCP执行初期，客户端不知道服务器端的IP地址，而在执行中间，客户端并未被分配IP地址，从而导致两者之间的通信必须采用广播的方式。采用UDP而不采用TCP的原因也很明显:TCP需要建立连接，如果连对方的IP地址都不知道，那么更不可能通过双方的套接字建立连接。</p>
<h5 id="网际控制报文协议-ICMP"><a href="#网际控制报文协议-ICMP" class="headerlink" title="网际控制报文协议(ICMP)"></a>网际控制报文协议(ICMP)</h5><p>为了提高IP数据报交付成功的机会，在网络层使用了网际控制报文协议(Internet ControlMessage Protocol，ICMP）来让主机或路由器报告差错和异常情况。ICMP报文作为IP层数据报的数据，加上数据报的首部，组成IP数据报发送出去。ICMP是 $\color{green}{\text{IP层协议}}$ 。</p>
<p>ICMP报文的种类有两种，即 $\color{green}{\text{ICMP差错报告报文}}$ 和 $\color{green}{\text{ICMP询问报文}}$ 。</p>
<p>ICMP差错报告报文用于目标主机或到目标主机路径上的路由器向源主机报告差错和异常情况。共有以下5种类型:</p>
<p>1） $\color{green}{\text{终点不可达}}$ 。当路由器或主机不能交付数据报时，就向源点发送终点不可达报文。</p>
<p>2） $\color{green}{\text{源点抑制}}$ 。当路由器或主机由于 $\color{green}{\text{拥塞}}$ 而丢弃数据报时，就向源点发送源点抑制报文，使源点知道应当把数据报的发送速率放慢。</p>
<p>3） $\color{green}{\text{时间超过}}$ 。当路由器收到生存时间（TTL)为零的数据报时，除丢弃该数据报外，还要向源点发送时间超过报文。当终点在预先规定的时间内不能收到一个数据报的全部数据报片时，就把已收到的数据报片都丢弃，并向源点发送时间超过报文。</p>
<p>4） $\color{green}{\text{参数问题}}$ 。当路由器或目的主机收到的数据报的首部中有的字段的值不正确时，就丢弃该数据报，并向源点发送参数问题报文。</p>
<p>5） $\color{green}{\text{改变路由}}$ (重定向)。路由器把改变路由报文发送给主机，让主机知道下次应将数据报发送给另外的路由器（可通过更好的路由)。</p>
<p>不应发送ICMP 差错报告报文的几种情况如下:</p>
<p>1）对ICMP差错报告报文不再发送ICMP差错报告报文。</p>
<p>2）对第一个分片的数据报片的所有后续数据报片都不发送ICMP差错报告报文。</p>
<p>3）对具有组播地址的数据报都不发送ICMP差错报告报文。</p>
<p>4）对具有特殊地址（如127.0.0.0或0.0.0.0）的数据报不发送ICMP差错报告报文。</p>
<p>ICMP询问报文有4种类型:回送请求和回答报文、时间戳请求和回答报文、掩码地址请求和回答报文、路由器询问和通告报文，最常用的是前两类。</p>
<p>ICMP的两个常见应用是分组网间探测 PING(用来测试两台主机之间的连通性)和 Traceroute(UNIX中的名字，在 Windows 中是 Tracert，可以用来跟踪分组经过的路由)。其中 PING使用ICMP回送请求和回答报文，Traceroute ( Tracert）使用了ICMP时间超过报文。</p>
<blockquote>
<p>注意:PING工作在应用层，它直接使用网络层的ICMP，而未使用传输层的TCP或UDP.Traceroute/Tracert 工作在网络层。</p>
</blockquote>
<h3 id="IPv6"><a href="#IPv6" class="headerlink" title="IPv6"></a>IPv6</h3><h4 id="IPv6的主要特点"><a href="#IPv6的主要特点" class="headerlink" title="IPv6的主要特点"></a>IPv6的主要特点</h4><p>解决“IP地址耗尽”问题的措施有以下三种:①采用无类别编址CIDR，使IP地址的分配更加合理;②采用网络地址转换(NAT)方法以节省全球IP地址;③采用具有更大地址空间的新版本的 IPv6。其中前两种方法只是延长了IPv4地址分配完毕的时间，只有第三种方法从根本上解决了IP地址的耗尽问题。</p>
<p>IPv6的主要特点如下:</p>
<p>1）更大的地址空间。IPv6将地址从IPv4的32位增大到了128位。IPv6的字节数(16B）是 IPv4字节数（4B）的平方。</p>
<p>2）扩展的地址层次结构。</p>
<p>3）灵活的首部格式。</p>
<p>4）改进的选项。</p>
<p>5）允许协议继续扩充。</p>
<p>6）支持即插即用（即自动配置)。</p>
<p>7）支持资源的预分配。</p>
<p>8）IPv6只有在包的源结点才能分片，是端到端的，传输路径中的路由器不能分片，所以从一般意义上说，IPv6 $\color{green}{\text{不允许分片}}$ （不允许类似IPv4的路由分片)。</p>
<p>9）IPv6首部长度必须是8B的整数倍，而IPv4首部是4B的整数倍。</p>
<p>10）增大了安全性。身份验证和保密功能是IPv6的关键特征。</p>
<p>虽然IPv6与IPv4不兼容，但总体而言它与所有其他的因特网协议兼容，包括TCP、UDP、ICMP、IGMP、OSPF、BGP 和 DNS，只是在少数地方做了必要的修改（大部分是为了处理长的地址)。IPv6相当好地满足了预定的目标，主要体现在:</p>
<p>1）首先也是最重要的，IPv6有比IPv4长得多的地址。IPv6的地址用16个字节表示，地址空间是IPv4的 $2^{128-32}$  = $2^{96}$ 倍，从长远来看，这些地址是绝对够用的。</p>
<p>2）简化了IP分组头，它包含8个域(IPv4是12个域)。这一改变使得路由器能够更快地处理分组，从而可以改善吞吐率。</p>
<p>3）更好地支持选项。这一改变对新的分组首部很重要，因为一些从前必要的段现在变成了可选段。此外，表示选项的方式的改变还能加快分组的处理速度。</p>
<ul>
<li>IPv6的首部长度是 $\color{green}{\text{固定的}}$ </li>
<li>IPv6没有校验和字段</li>
<li>使用零压缩法时，双冒号“::”在一个地址中只能出现一次。也就是说，当有多处不相邻的0时，只能用“::”代表其中的一处。</li>
</ul>
<p><a target="_blank" rel="noopener" href="https://baike.baidu.com/item/IPv6/172297?fr=aladdin">参考文献</a></p>
<details><summary>IPv6的头部</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211009174527.webp" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211009174527.png';" /></details>

<h4 id="IPv6地址"><a href="#IPv6地址" class="headerlink" title="IPv6地址"></a>IPv6地址</h4><p>IPv6数据报的目的地址可以是以下三种基本类型地址之一:</p>
<p>1） $\color{green}{\text{单播}}$ 。单播就是传统的点对点通信。</p>
<p>2） $\color{green}{\text{多播}}$ 。多播是一点对多点的通信，分组被交付到一组计算机的每台计算机。</p>
<p>3） $\color{green}{\text{任播}}$ 。这是IPv6增加的一种类型。任播的目的站是一组计算机，但数据报在交付时只交付其中的一台计算机，通常是距离最近的一台计算机。</p>
<p>IPv4地址通常使用点分十进制表示法。如果IPv6也使用这种表示法，那么地址书写起来将会相当长。在IPv6标准中指定了一种比较紧凑的表示法，即把地址中的每4位用一个十六进制数表示，并用冒号分隔每16位，如4BF5:AA12:0216:FEBC:BA5F:039A:BE9A:2170。</p>
<p>通常可以把Pv6地址缩写成更紧凑的形式。当16位域的开头有一些0时，可以采用一种缩写表示法，但在域中必须至少有一个数字。例如，可以把地址4BF5:0000:0000:0000:BA 5F:039A:000A:2176缩写为4BF5:0:0:0:BA5F:39A:A:2176。</p>
<p>IPv6扩展了IPv4地址的分级概念，它使用以下3个等级: $\color{green}{\text{第一级}}$ （顶级）指明全球都知道的公共拓扑; $\color{green}{\text{第二级}}$ (场点级）指明单个场点; $\color{green}{\text{第三级}}$ 指明单个网络接口。IPv6地址采用多级体系主要是为了使路由器能够更快地查找路由。</p>
<p>IPv4向IPv6过渡只能采用逐步演进的办法,同时还必须使新安装的IPv6系统能够向后兼容。IPv6系统必须能够接收和转发IPv4分组，并且能够为IPv4分组选择路由。</p>
<p>IPv4向IPv6过渡可以采用 $\color{green}{\text{双协议栈}}$ 和 $\color{green}{\text{隧道技术}}$ 两种策略:双协议栈技术是指在一台设备上同时装有IPv4和 IPv6协议栈，那么这台设备既能和IPv4网络通信，又能和IPv6网络通信。如果这台设备是一个路由器，那么在路由器的不同接口上分别配置了IPv4地址和IPv6地址，并很可能分别连接了IPv4网络和IPv6网络;如果这台设备是一台计算机，那么它将同时拥有IPv4地址和IPv6地址，并具备同时处理这两个协议地址的功能。隧道技术是将整个IPv6数据报封装到IPv4数据报的数据部分，使得IPv6数据报可以在 IPv4网络的隧道中传输。</p>
<h3 id="路由协议"><a href="#路由协议" class="headerlink" title="路由协议"></a>路由协议</h3><h4 id="自治系统"><a href="#自治系统" class="headerlink" title="自治系统"></a>自治系统</h4><p>自治系统(Autonomous System，AS):单-一技术管理下的一组路由器，这些路由器使用一种AS内部的路由选择协议和共同的度量来确定分组在该AS内的路由，同时还使用一种AS之间的路由选择协议来确定分组在AS之间的路由。</p>
<p>一个自治系统内的所有网络都由一个行政单位（如一家公司、一所大学、一个政府部门等)管辖，一个自治系统的所有路由器在本自治系统内都必须是连通的。</p>
<h4 id="域内路由与域间路由"><a href="#域内路由与域间路由" class="headerlink" title="域内路由与域间路由"></a>域内路由与域间路由</h4><p>自治系统内部的路由选择称为 $\color{green}{\text{域内路由选择}}$ ，自治系统之间的路由选择称为 $\color{green}{\text{域间路由选择}}$ 。因特网有两大类路由选择协议。</p>
<h5 id="内部网关协议-Interior-Gateway-Protocol，IGP"><a href="#内部网关协议-Interior-Gateway-Protocol，IGP" class="headerlink" title="内部网关协议(Interior Gateway Protocol，IGP)"></a>内部网关协议(Interior Gateway Protocol，IGP)</h5><p>内部网关协议即在一个自治系统内部使用的路由选择协议，它与互联网中其他自治系统选用什么路由选择协议无关。目前这类路由选择协议使用得最多，如RIP和OSPF。</p>
<h5 id="外部网关协议-External-Gateway-Protocol，EGP"><a href="#外部网关协议-External-Gateway-Protocol，EGP" class="headerlink" title="外部网关协议(External Gateway Protocol，EGP)"></a>外部网关协议(External Gateway Protocol，EGP)</h5><p>若源站和目的站处在不同的自治系统中，当数据报传到一个自治系统的边界时(两个自治系统可能使用不同的IGP)，就需要使用一种协议将路由选择信息传递到另一个自治系统中。这样的协议就是外部网关协议（EGP)。目前使用最多的外部网关协议是BGP-4。</p>
<p>图4.7是两个自治系统互联的示意图。每个自治系统自己决定在本自治系统内部运行哪个内部路由选择协议（例如，可以是RIP，也可以是OSPF)，但每个自治系统都有一个或多个路由器(图中的路由器R1和R2)。除运行本系统的内部路由选择协议外，还要运行自治系统间的路由选择协议（如 BGP-4)。</p>
<details><summary>图4.7自治系统和内部网关协议、外部网关协议</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005101400.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005101400.png';" /></details>

<h4 id="路由信息协议（RIP"><a href="#路由信息协议（RIP" class="headerlink" title="路由信息协议（RIP)"></a>路由信息协议（RIP)</h4><p>路由信息协议(Routing Information Protocol，RIP）是 $\color{green}{\text{内部网关协议}}$ (IGP）中最先得到广泛应用的协议。RIP是一种分布式的基于 $\color{green}{\text{距离向量}}$ 的路由选择协议，其最大优点就是简单。</p>
<h5 id="RIP规定"><a href="#RIP规定" class="headerlink" title="RIP规定"></a>RIP规定</h5><p>1）网络中的每个路由器都要维护从它自身到其他每个目的网络的距离记录（因此这是一组距离，称为距离向量)。</p>
<p>2）距离也称跳数（Hop Count)，规定从一个路由器到直接连接网络的距离（跳数）为1。而每经过一个路由器，距离（跳数）加1。</p>
<p>3）RIP认为好的路由就是它通过的路由器的数目少，即优先选择跳数少的路径。</p>
<p>4）RIP允许一条路径最多只能包含15个路由器（即最多允许15跳)。因此距离等于 $\color{green}{\text{16}}$ 时，它表示网络不可达。可见 RIP只适用于小型互联网。距离向量路由可能会出现环路的情况，规定路径上的最高跳数的目的是为了防止数据报不断循环在环路上，减少网络拥塞的可能性。</p>
<p>5）RIP默认在任意两个使用RIP的路由器之间每30秒广播一次RIP路由更新信息，以便自动建立并维护路由表（动态维护)。</p>
<p>6）在RIP中不支持子网掩码的RIP广播，所以RIP中每个网络的子网掩码必须相同。但在新的RIP2中，支持变长子网掩码和CIDR。</p>
<h5 id="RIP的特点（注意与OSPF的特点比较"><a href="#RIP的特点（注意与OSPF的特点比较" class="headerlink" title="RIP的特点（注意与OSPF的特点比较)"></a>RIP的特点（注意与OSPF的特点比较)</h5><p>1）仅和相邻路由器交换信息。</p>
<p>2）路由器交换的信息是当前路由器所知道的全部信息，即自己的路由表。</p>
<p>3）按固定的时间间隔交换路由信息，如每隔30秒。</p>
<p>RIP通过距离向量算法来完成路由表的更新。最初，每个路由器只知道与自己直接相连的网络。通过每30秒的RIP广播，相邻两个路由器相互将自己的路由表发给对方。于是经过第一-次RIP广播，每个路由器就知道了与自己相邻的路由器的路由表(即知道了距离自己跳数为1的网络的路由)。同理,经过第二次RIP广播,每个路由器就知道了距离自己跳数为2的网络的路由……因此，经过若干RIP广播后，所有路由器都最终知道了整个IP网络的路由表，成为RIP最终是收敛的。通过RIP收敛后，每个路由器到每个目标网络的路由都是距离最短的（即跳数最少，最短路由)，哪怕还存在另一条高速（低时延）但路由器较多的路由。</p>
<h5 id="距离向量算法"><a href="#距离向量算法" class="headerlink" title="距离向量算法"></a>距离向量算法</h5><p>每个路由表项目都有三个关键数据:&lt;目的网络N，距离d，下一跳路由器地址X&gt;。对于每个相邻路由器发送过来的RIP报文，执行如下步骤:</p>
<p>1）对地址为X的相邻路由器发来的RIP报文，先修改此报文中的所有项目:把“下一跳”字段中的地址都改为X，并把所有“距离”字段的值加1。</p>
<p>2）对修改后的RIP报文中的每个项目，执行如下步骤:</p>
<p>2）对修改后的RIP报文中的每个项目，执行如下步骤:</p>
<p>①当原来的路由表中没有目的网络N时，把该项目添加到路由表中。</p>
<p>${\textstyle\unicode{x2461}}$  当原来的路由表中有目的网络N，且下一跳路由器的地址是X时，用收到的项目替换原路由表中的项目。</p>
<p>${\textstyle\unicode{x2462}}$ 当原来的路由表中有目的网络N，且下一跳路由器的地址不是X时，如果收到的项目中的距离d小于路由表中的距离，那么就用收到的项目替换原路由表中的项目;否则什么也不做。</p>
<p>3）如果180秒(RIP默认超时时间为 $\color{green}{\text{180秒}}$ )还没有收到相邻路由器的更新路由表，那么把此相邻路由器记为不可达路由器，即把距离设置为16（距离为16表示不可达)。</p>
<p>4）返回。</p>
<p>RIP最大的优点是实现简单、开销小、收敛过程较快。RIP 的缺点如下:</p>
<p>RIP最大的优点是实现简单、开销小、收敛过程较快。RI的缺点如下:1）RIP限制了网络的规模，它能使用的最大距离为15（16表示不可达)。</p>
<p>2）路由器之间交换的是路由器中的完整路由表，因此网络规模越大，开销也越大。</p>
<p>3）网络出现故障时，会出现慢收敛现象（即需要较长时间才能将此信息传送到所有路由器)，俗称“坏消息传得慢”，使更新过程的收敛时间长。</p>
<p>RIP是应用层协议，它使用UDP传送数据（ $\color{green}{\text{端口520}}$ )。RIP选择的路径不一定是时间最短的，但一定是具有最少路由器的路径。因为它是根据最少跳数进行路径选择的。</p>
<h4 id="开放最短路径优先（OSPF-协议"><a href="#开放最短路径优先（OSPF-协议" class="headerlink" title="开放最短路径优先（OSPF)协议"></a>开放最短路径优先（OSPF)协议</h4><h5 id="OSPF协议的基本特点"><a href="#OSPF协议的基本特点" class="headerlink" title="OSPF协议的基本特点"></a>OSPF协议的基本特点</h5><p>开放最短路径优先（OSPF）协议是使用分布式 $\color{green}{\text{链路状态路由算法}}$ 的典型代表，也是 $\color{green}{\text{内部网关协议}}$ （IGP）的一种。OSPF与RIP相比有以下4点主要区别:</p>
<p>1）OSPF向本自治系统中的所有路由器发送信息，这里使用的方法是 $\color{green}{\text{洪泛法}}$ 。而RIP仅向自己相邻的几个路由器发送信息。</p>
<p>2）发送的信息是与本路由器相邻的所有路由器的链路状态，但这只是路由器所知道的部分信息。“链路状态”说明本路由器和哪些路由器相邻及该链路的“度量”(或代价)。而在RIP中，发送的信息是本路由器所知道的全部信息，即整个路由表。</p>
<p>3）只有当链路状态发生变化时，路由器才用洪泛法向所有路由器发送此信息，并且更新过程收敛得快，不会出现RIP“坏消息传得慢”的问题。而在 RIP中，不管网络拓扑是否发生变化，路由器之间都会定期交换路由表的信息。</p>
<p>4）OSPF是 $\color{green}{\text{网络层协议}}$ ，它不使用UDP或TCP，而直接用IP数据报传送（其IP数据报首部的协议字段为 $\color{green}{\text{89}}$ )。而RIP是 $\color{green}{\text{应用层协议}}$ ，它在传输层使用UDP。</p>
<p>除以上区别外，OSPF 还有以下特点:</p>
<p>1)OSPF对不同的链路可根据IP分组的不同服务类型（TOS）而设置成不同的代价。因此OSPF对于不同类型的业务可计算出不同的路由，十分灵活。</p>
<p>2）如果到同一个目的网络有多条相同代价的路径，那么可以将通信量分配给这几条路径。这称为多路径间的 $\color{green}{\text{负载平衡}}$ 。</p>
<p>3）所有在OSPF 路由器之间交换的分组都具有鉴别功能，因而保证了仅在可信赖的路由器之间交换链路状态信息。</p>
<p>4）支持可变长度的子网划分和无分类编址CIDR。</p>
<p>5）每个链路状态都带上一个32位的序号，序号越大，状态就越新。</p>
<h5 id="OSPF的基本工作原理"><a href="#OSPF的基本工作原理" class="headerlink" title="OSPF的基本工作原理"></a>OSPF的基本工作原理</h5><p>由于各路由器之间频繁地交换链路状态信息，因此所有路由器最终都能建立一个链路状态数据库。这个数据库实际上就是全网的拓扑结构图，它在全网范围内是一致的(称为链路状态数据库的同步)。然后，每个路由器根据这个全网拓扑结构图，使用Dijkstra最短路径算法计算从自己到各目的网络的最优路径，以此构造自己的路由表。此后，当链路状态发生变化时，每个路由器重新计算到各目的网络的最优路径，构造新的路由表。</p>
<blockquote>
<p>注意:虽然使用Dijkstra算法能计算出完整的最优路径，但路由表中不会存储完整路径，而只存储“下一跳”(只有到了下一跳路由器，才能知道再下一跳应当怎样走)。</p>
</blockquote>
<p>为使OSPF 能够用于规模很大的网络，OSPF 将一个自治系统再划分为若干更小的范围，称为 $\color{green}{\text{区域}}$ 。划分区域的好处是，将利用洪泛法交换链路状态信息的范围局限于每个区域而非整个自治系统，减少了整个网络上的通信量。在一个区域内部的路由器只知道本区域的完整网络拓扑，而不知道其他区域的网络拓扑情况。这些区域也有层次之分。处在上层的域称为主干区域，负责连通其他下层的区域，并且还连接其他自治域。</p>
<h5 id="OSPF的五种分组类型"><a href="#OSPF的五种分组类型" class="headerlink" title="OSPF的五种分组类型"></a>OSPF的五种分组类型</h5><p>OSPF 共有以下五种分组类型:</p>
<p>1） $\color{green}{\text{问候分组}}$ ，用来发现和维持邻站的可达性。</p>
<p>2） $\color{green}{\text{数据库描述分组}}$ ，向邻站给出自己的链路状态数据库中的所有链路状态项目的摘要信息。</p>
<p>3） $\color{green}{\text{链路状态请求分组}}$ ，向对方请求发送某些链路状态项目的详细信息。</p>
<p>4） $\color{green}{\text{链路状态更新分组}}$ ，用洪泛法对全网更新链路状态。</p>
<p>5） $\color{green}{\text{链路状态确认分组}}$ ，对链路更新分组的确认。</p>
<p>通常每隔10秒，每两个相邻路由器要交换一次问候分组，以便知道哪些站可达。在路由器刚开始工作时，OSPF让每个路由器使用数据库描述分组和相邻路由器交换本数据库中已有的链路状态摘要信息。然后，路由器使用链路状态请求分组，向对方请求发送自己所缺少的某些链路状态项目的详细信息。经过一系列的这种分组交换，就建立了全网同步的链路数据库。图4.8给出了OSPF的基本操作，说明了两个路由器需要交换的各种类型的分组。</p>
<details><summary>图4.8 OSPF的基本操作
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005102404.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005102404.png';" /></details>

<p>在网络运行的过程中，只要一个路由器的链路状态发生变化，该路由器就要使用链路状态更新分组，用洪泛法向全网更新链路状态。其他路由器在更新后，发送链路状态确认分组对更新分组进行确认。</p>
<p>为了确保链路状态数据库与全网的状态保持一致，OSPF还规定每隔一段时间（如30分钟)就刷新一次数据库中的链路状态。由于一个路由器的链路状态只涉及与相邻路由器的连通状态，因而与整个互联网的规模并无直接关系。因此，当互联网规模很大时，OSPF 要比RIP好得多,而且 OSPF协议没有“坏消息传播得慢”的问题。</p>
<blockquote>
<p>注意:教材上说OSPF 协议不使用UDP数据报传送，而是直接使用IP数据报传送，在此解释一下什么称为用UDP传送，什么称为用IP数据报传送。用UDP传送是指将该信息作为UDP报文的数据部分，而直接使用IP 数据报传送是指将该信息直接作为IP数据报的数据部分。RIP报文是作为UDP 数据报的数据部分。</p>
</blockquote>
<h4 id="边界网关协议（BGP"><a href="#边界网关协议（BGP" class="headerlink" title="边界网关协议（BGP)"></a>边界网关协议（BGP)</h4><p>边界网关协议(Border Gateway Protocol，BGP)是不同自治系统的路由器之间交换路由信息的协议，是一种外部网关协议。边界网关协议常用于互联网的网关之间。路由表包含已知路由器的列表、路由器能够达到的地址及到达每个路由器的路径的跳数。</p>
<p>内部网关协议主要设法使数据报在一个AS 中尽可能有效地从源站传送到目的站。在一个 AS内部不需要考虑其他方面的策略。然而BGP使用的环境却不同，主要原因如下:</p>
<p>1）因特网的规模太大，使得自治系统之间路由选择非常困难。</p>
<p>2）对于自治系统之间的路由选择，要寻找最佳路由是很不现实的。</p>
<p>3）自治系统之间的路由选择必须考虑有关策略。</p>
<p>边界网关协议（BGP）只能力求寻找一条能够到达目的网络且比较好的路由（不能兜圈子),而并非寻找一条最佳路由。BGP采用的是路径向量路由选择协议，它与距离向量协议和链路状态协议有很大的区别。BGP是 $\color{green}{\text{应用层}}$ 协议，它是 $\color{green}{\text{基于TCP}}$ 的。</p>
<p>BGP的工作原理如下:每个自治系统的管理员要选择至少一个路由器(可以有多个)作为该自治系统的“BGP发言人”。一个BGP发言人与其他自治系统中的BGP发言人要交换路由信息，就要先建立TCP连接(可见BGP报文是通过TCP传送的，也就是说BGP报文是TCP报文的数据部分)，然后在此连接上交换BGP报文以建立BGP会话，再利用BGP会话交换路由信息。当所有BGP发言人都相互交换网络可达性的信息后，各BGP发言人就可找出到达各个自治系统的较好路由。</p>
<p>每个BGP发言人除必须运行BGP 外，还必须运行该AS所用的内部网关协议，如 OSPF或RIP。BGP所交换的网络可达性信息就是要到达某个网络（用网络前缀表示）所要经过的一系列AS。图4.9给出了一个BGP发言人交换路径向量的例子。</p>
<ol>
<li>BGP交换路由信息的结点数量级是自治系统的数量级，要比这些自治系统中的网络数少很多。</li>
</ol>
<p>2）每个自治系统中BGP发言人(或边界路由器）的数目是很少的。这样就使得自治系统之间的路由选择不致过分复杂。</p>
<details><summary>图4.9 主干网与自治系统间路径向量的交换</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005102621.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005102621.png';" /></details>

<p>3）BGP支持CIDR，因此 BGP 的路由表也就应当包括目的网络前缀、下一跳路由器，以及到达该目的网络所要经过的各个自治系统序列。</p>
<p>4）在BGP刚运行时，BGP 的邻站交换整个BGP路由表，但以后只需在发生变化时更新有变化的部分。这样做对节省网络带宽和减少路由器的处理开销都有好处。</p>
<p>BGP-4共使用4种报文:</p>
<p>1）打开（Open）报文。用来与相邻的另一个BGP发言人建立关系。</p>
<p>2）更新（Update）报文。用来发送某一路由的信息，以及列出要撤销的多条路由。</p>
<p>3）保活(Keepalive）报文。用来确认打开报文并周期性地证实邻站关系。</p>
<p>4）通知(Notification）报文。用来发送检测到的差错。</p>
<p>RIP、OSPF与 BGP的比较如表4.3所示。</p>
<details><summary>表4.3三种路由协议的比较
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005102717.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005102717.png';" /></details>

<h3 id="IP组播"><a href="#IP组播" class="headerlink" title="IP组播"></a>IP组播</h3><h4 id="组播的概念"><a href="#组播的概念" class="headerlink" title="组播的概念"></a>组播的概念</h4><p>为了能够支持像视频点播和视频会议这样的多媒体应用，网络必须实施某种有效的组播机制。使用多个单播传送来仿真组播总是可能的，但这会引起主机上大量的处理开销和网络上太多的交通量。人们所需要的组播机制是让源计算机一次发送的单个分组可以抵达用一个组地址标识的若干目标主机，并被它们正确接收。</p>
<p>组播一定仅应用于 $\color{green}{\text{UDP}}$ ，它对将报文同时送往多个接收者的应用来说非常重要。而TCP是一个面向连接的协议，它意味着分别运行于两台主机（由IP地址来确定）内的两个进程（由端号来确定）之间存在一条连接，因此会一对一地发送。</p>
<p>使用组播的缘由是，有的应用程序要把一个分组发送给多个目的地主机。不是让源主机给每个目的地主机都发送一个单独的分组，而是让源主机把单个分组发送给一个组播地址，该组播地址标识一组地址。网络（如因特网）把这个分组的副本投递给该组中的每台主机。主机可以选择加入或离开一个组，因此一台主机可以同时属于多个组。</p>
<p>因特网中的IP组播也使用组播组的概念，每个组都有一个特别分配的地址，要给该组发送的计算机将使用这个地址作为分组的目标地址。在IPv4中，这些地址在D类地址空间中分配,而IPv6也有一部分地址空间保留给组播组。</p>
<p>主机使用一个称为 $\color{green}{\text{IGMP}}$ (因特网组管理协议）的协议加入组播组。它们使用该协议通知本地网络上的路由器关于要接收发送给某个组播组的分组的愿望。通过扩展路由器的路由选择和转发功能，可以在许多路由器互联的支持硬件组播的网络上面实现因特网组播。</p>
<p>需要注意的是，主机组播时仅发送一份数据，只有数据在传送路径出现分岔时才将分组复制后继续转发。因此，对发送者而言，数据只需发送一次就可发送到所有接收者，大大减轻了网络的负载和发送者的负担。组播需要路由器的支持才能实现，能够运行组播协议的路由器称为 $\color{green}{\text{组播路由器}}$ 。单播与组播的比较如图4.10所示。</p>
<details><summary>图4.10单播与组播的比较
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005102825.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005102825.png';" /></details>


<h4 id="IP组播地址"><a href="#IP组播地址" class="headerlink" title="IP组播地址"></a>IP组播地址</h4><p>IP组播使用D类地址格式。D类地址的前四位是1110，因此D类地址范围是 224.0.0.0～239.255.255.255。每个D类IP地址标志一个组播组。</p>
<p>组播数据报和一般的IP数据报的区别是，前者使用D类IP地址作为目的地址，并且首部中的协议字段值是 $\color{green}{\text{2}}$ ，表明使用IGMP。需要注意的是:</p>
<p>1）组播数据报也是“尽最大努力交付”，不提供可靠交付。</p>
<p>2）组播地址只能用于目的地址，而不能用于源地址。</p>
<p>3）对组播数据报不产生ICMP差错报文。因此，若在PNG 命令后面键入组播地址，将永远不会收到响应。</p>
<p>4）并非所有的D类地址都可作为组播地址。</p>
<p>IP组播可以分为两种:一种只在本局域网上进行硬件组播;另一种则在因特网的范围内进行组播。在因特网上进行组播的最后阶段，还是要把组播数据报在局域网上用硬件组播交付给组播组的所有成员〔见图4.10(b)]。下面讨论这种硬件组播。</p>
<p>IANA拥有的以太网组播地址的范围是从01-00-5E-00-00-00到01-00-5E-7F-FF-FF。不难看出,在每个地址中，只有23位可用作组播。这只能和D类IP地址中的23位有一一对应关系。D类IP地址可供分配的有28位，可见在这28位中，前5位不能用来构成以太网的硬件地址，如图4.11所示。</p>
<details><summary>图4.11D类IP地址与以太网组播地址的映射关系
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005102938.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005102938.png';" /></details>

<p>例如，IP组播地址224.128.64.32(即 E0-80-40-20）和另一个IP组播地址224.0.64.32（即EO-00-40-20）转换成以太网的硬件组播地址都是01-00-5E-00-40-20。由于组播IP地址与以太硬件地址的映射关系不是唯一的，因此收到组播数据报的主机，还要在IP层利用软件进行过滤把不是本主机要接收的数据报丢弃。</p>
<h4 id="IGMP与组播路由算法"><a href="#IGMP与组播路由算法" class="headerlink" title="IGMP与组播路由算法"></a>IGMP与组播路由算法</h4><p>要使路由器知道组播组成员的信息，需要利用因特网组管理协议(Internet Group ManagemenProtocol，IGMP)。连接到局域网上的组播路由器还必须和因特网上的其他组播路由器协同工作,以便把组播数据报用最小代价传送给所有组成员，这就需要使用组播路由选择协议。</p>
<p>IGMP并不是在因特网范围内对所有组播组成员进行管理的协议。IGMP不知道IP组播组包含的成员数，也不知道这些成员分布在哪些网络上。IGMP让连接到本地局域网上的组播路由器知道本局域网上是否有主机参加或退出了某个组播组。</p>
<p>IGMP应视为TCP/IP的一部分，其工作可分为两个阶段。</p>
<p>$\color{green}{\text{第一阶段}}$ :当某台主机加入新的组播组时，该主机应向组播组的组播地址发送一个IGMP报文，声明自己要成为该组的成员。本地的组播路由器收到IGMP报文后，将组成员关系转发给因特网上的其他组播路由器。</p>
<p>$\color{green}{\text{第二阶段}}$ :因为组成员关系是动态的，本地组播路由器要周期性地探询本地局域网上的主机,以便知道这些主机是否仍继续是组的成员。只要对某个组有一台主机响应，那么组播路由器就认为这个组是活跃的。但一个组在经过几次的探询后仍然没有一台主机响应时，则不再将该组的成员关系转发给其他的组播路由器。</p>
<p>组播路由选择实际上就是要找出以源主机为根结点的 $\color{green}{\text{组播转发树}}$ ，其中每个分组在每条链路上只传送一次（即在组播转发树上的路由器不会收到重复的组播数据报)。不同的多播组对应于不同的多播转发树;同一个多播组，对不同的源点也会有不同的多播转发树。</p>
<p>在许多由路由器互联的支持硬件多点传送的网络上实现因特网组播时，主要有三种路由算法:第一种是基于链路状态的路由选择;第二种是基于距离-向量的路由选择;第三种可以建立在任何路由器协议之上，因此称为 $\color{green}{\text{协议无关的组播}}$ （PIM,Protocol Independent Multicast)。</p>
<h3 id="移动IP"><a href="#移动IP" class="headerlink" title="移动IP"></a>移动IP</h3><h4 id="移动IP的概念"><a href="#移动IP的概念" class="headerlink" title="移动IP的概念"></a>移动IP的概念</h4><p>支持移动性的因特网体系结构与协议共称为移动IP，它是为了满足移动结点（计算机、服务器、网段等）在移动中保持其连接性而设计的。更确切地说，移动IP技术是指移动结点以固定的网络IP地址实现跨越不同网段的漫游功能，并保证基于网络IP的网络权限在漫游过程中不发生任何改变。移动IP的目标是把分组自动地投递给移动结点。一个移动结点是把其连接点从一个网络或子网改变到另一个网络或子网的主机。使用移动IP，一个移动结点可以在不改变其IP地址的情况下改变其驻留位置。</p>
<p>基于IPv4的移动IP定义三种功能实体: $\color{green}{\text{移动结点}}$ 、 $\color{green}{\text{归属代理}}$ （也称本地代理）和 $\color{green}{\text{外埠代理}}$ (也称外部代理)。归属代理和外埠代理又统称为移动代理。</p>
<p>1） $\color{green}{\text{移动结点}}$ 。具有永久IP地址的移动结点。</p>
<p>2） $\color{green}{\text{本地代理}}$ 。在一个网络环境中，一个移动结点的永久“居所”被称为归属网络，在归属网络中代表移动结点执行移动管理功能的实体称为 $\color{green}{\text{归属代理}}$ （本地代理)，它根据移动用户的转交地址，采用隧道技术转交移动结点的数据包。</p>
<p>3） $\color{green}{\text{外部代理}}$ 。在外部网络中帮助移动结点完成移动管理功能的实体称为外部代理。</p>
<p>移动IP和移动自组网络并不相同，移动技术使漫游的主机可以用多种方式连接到因特网,移动IP的核心网络功能仍然是基于固定互联网中一直使用的各种路由选择协议，移动自组网络是将移动性扩展到无线领域中的自治系统，它具有自己独特的路由选择协议，并且可以不和因特网相连。</p>
<p>移动IP与动态IP是两个完全不同的概念，动态IP指的是局域网中的计算机可以通过网络中的 DHCP 服务器动态地获得一个IP地址，而不需要用户在计算机的网络设置中指定IP地址，动态IP和DHCP经常会应用在我们的实际工作环境中。</p>
<h4 id="移动IP通信过程"><a href="#移动IP通信过程" class="headerlink" title="移动IP通信过程"></a>移动IP通信过程</h4><p>在移动IP中，每个移动结点都有一个唯一的本地地址，当移动结点移动时，它的本地地址是不变的，在本地网络链路上每个本地结点还必须有一个本地代理来为它维护当前的位置信息，这就需要引入 $\color{green}{\text{转交地址}}$ 。当移动结点连接到外地网络链路上时，转交地址就用来标识移动结点现在所处的位置，以便进行路由选择。移动结点的本地地址与当前转交地址的联合称为移动绑定或简称绑定。当移动结点得到一个新的转交地址时，通过绑定向本地代理进行注册，以便让本地代理即时了解移动结点的当前位置。</p>
<p>移动IP技术的基本通信流程如下:</p>
<p>1）移动结点在本地网时，按传统的TCP/IP方式进行通信（在本地网中有固有的地址)。</p>
<p>2）移动结点漫游到一个外地网络时，仍然使用固定的IP地址进行通信。为了能够收到通信对端发给它的IP分组，移动结点需要向本地代理注册当前的位置地址，这个位置地址就是转交地址（它可以是外部代理的地址或动态配置的一个地址)。</p>
<p>3）本地代理接收来自转交地址的注册后，会构建一条通向转交地址的隧道，将截获的发给移动结点的IP分组通过隧道送到转交地址处。</p>
<p>4)在转交地址处解除隧道封装，恢复原始的IP分组，最后送到移动结点，这样移动结点在外网就能够收到这些发送给它的IP分组。</p>
<p>5）移动结点在外网通过外网的路由器或外部代理向通信对端发送IP数据包。</p>
<p>6）移动结点来到另一个外网时，只需向本地代理更新注册的转交地址，就可继续通信。</p>
<p>7）移动结点回到本地网时，移动结点向本地代理注销转交地址，这时移动结点又将使用传统的TCP/IP方式进行通信。</p>
<p>移动IP为移动主机设置了两个IP地址，即 $\color{green}{\text{主地址}}$ 和 $\color{green}{\text{辅地址}}$ (转交地址)。移动主机在本地网时，使用的是主地址。当移动到另一个网络时，需要获得一个临时的辅地址，但此时主地址仍然不变。从外网移回本地网时，辅地址改变或撤销，而主地址仍然保持不变。</p>
<h3 id="网络层设备"><a href="#网络层设备" class="headerlink" title="网络层设备"></a>网络层设备</h3><h4 id="路由器的组成和功能"><a href="#路由器的组成和功能" class="headerlink" title="路由器的组成和功能"></a>路由器的组成和功能</h4><p>路由器是一种具有多个输入/输出端口的专用计算机，其任务是连接不同的网络(连接异构网络）并完成路由转发。在多个逻辑网络（即多个广播域）互联时必须使用路由器。</p>
<p>当源主机要向目标主机发送数据报时，路由器先检查源主机与目标主机是否连接在同一个网络上。如果源主机和目标主机在同一个网络上，那么直接交付而无须通过路由器。如果源主机和目标主机不在同一个网络上，那么路由器按照转发表（路由表）指出的路由将数据报转发给下一个路由器，这称为间接交付。可见，在同一个网络中传递数据无须路由器的参与，而跨网络通信必须通过路由器进行转发。例如，路由器可以连接不同的LAN，连接不同的VLAN，连接不同的WAN，或者把LAN和WAN 互联起来。路由器隔离了广播域。</p>
<p>从结构上看,路由器由路由选择和分组转发两部分构成，如图4.12所示。而从模型的角度看，路由器是网络层设备，它实现了网络模型的下三层，即物理层、数据链路层和网络层。</p>
<details><summary>图4.12路由器体系结构
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005103611.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005103611.png';" /></details>

<blockquote>
<p>注意:如果一个存储转发设备实现了某个层次的功能，那么它就可以互联两个在该层次上使用不同协议的网段（网络)。如网桥实现了物理层和数据链路层，那么网桥可以互联两个物理层和数据链路层不同的网段;但中继器实现了物理层后，却不能互联两个物理层不同的网段，这是因为中继器不是存储转发设备，它属于直通式设备。</p>
</blockquote>
<p>路由选择部分也称 $\color{green}{\text{控制部分}}$ ，其核心构件是路由选择处理机。路由选择处理机的任务是根据所选定的路由选择协议构造出路由表，同时经常或定期地和相邻路由器交换路由信息而不断更新和维护路由表。</p>
<p>分组转发部分由三部分组成: $\color{green}{\text{交换结构}}$ 、 $\color{green}{\text{一组输入端口}}$ 和 $\color{green}{\text{一组输出端口}}$ 。输入端口在从物理层接收到的比特流中提取出数据链路层帧，进而从帧中提取出网络层数据报，输出端口则执行恰好相反的操作。交换结构是路由器的关键部件，它根据转发表对分组进行处理，将某个输入端口进入的分组从一个合适的输出端口转发出去。有三种常用的交换方法: $\color{green}{\text{通过存储器进行交换}}$ 、 $\color{green}{\text{通过总线进行交换}}$ 和 $\color{green}{\text{通过互联网络进行交换}}$ 。交换结构本身就是一个网络。</p>
<p>路由器主要完成两个功能:一是分组转发，二是路由计算。前者处理通过路由器的数据流.关键操作是转发表查询、转发及相关的队列管理和任务调度等;后者通过和其他路由器进行基于路由协议的交互，完成路由表的计算。</p>
<p>路由器和网桥的重要区别是:网桥与高层协议无关，而路由器是面向协议的，它依据网络地址进行操作，并进行路径选择、分段、帧格式转换、对数据报的生存时间和流量进行控制等。现今的路由器一般都提供多种协议的支持，包括OSI、TCP/IP、IPX等。</p>
<h4 id="路由表与路由转发"><a href="#路由表与路由转发" class="headerlink" title="路由表与路由转发"></a>路由表与路由转发</h4><p>路由表是根据路由选择算法得出的，主要用途是路由选择。从历年统考真题可以看出，标准的路由表有4个项目:目的网络IP地址、子网掩码、下一跳IP地址、接口。在如图4.13所示的网络拓扑中，R1的路由表见表4.4，该路由表包含到互联网的默认路由。</p>
<details><summary>图4.13一个简单的网络拓扑</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005103732.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005103732.png';" /></details>

<details><summary>表4.4R1的路由表</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005103753.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005103753.png';" /></details>


<p>转发表是从路由表得出的，其表项和路由表项有直接的对应关系。但转发表的格式和路由表的格式不同，其结构应使查找过程最优化（而路由表则需对网络拓扑变化的计算最优化)。转发表中含有一个分组将要发往的目的地址，以及分组的下一跳（即下一步接收者的目的地址，实际为MAC地址)。为了减少转发表的重复项目，可以使用一个默认路由代替所有具有相同“下一跳”的项目，并将默认路由设置得比其他项目的优先级低，如图4.14所示。 $\color{red}{\text{路由表}}$ 总是用 $\color{green}{\text{软件}}$ 来实现的; $\color{red}{\text{转发表}}$ 可以用 $\color{green}{\text{软件}}$ 来实现，甚至也可以用特殊的 $\color{green}{\text{硬件}}$ 来实现。</p>
<details><summary>图4.14未使用默认路由的转发表和使用了默认路由的转发表的对比</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005103828.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005103828.png';" /></details>

<p>注意转发和路由选择的区别:“ $\color{red}{\text{转发}}$ ”是路由器根据转发表把收到的IP数据报从合适的端转发出去，它仅涉及 $\color{green}{\text{一个路由器}}$ 。而“路由选择”则涉及很多路由器， $\color{red}{\text{路由表}}$ 是 $\color{green}{\text{许多路由器}}$ 协同工作的结果。这些路由器按照复杂的路由算法，根据从各相邻路由器得到的关于网络拓扑的变化情况，动态地改变所选择的路由，并由此构造出整个路由表。</p>
<p>注意，在讨论路由选择的原理时，往往不去区分转发表和路由表的区别，但要注意路由表不等于转发表。分组的实际转发是靠直接查找转发表，而不是直接查找路由表。</p>
<h3 id="本章小结及疑难点"><a href="#本章小结及疑难点" class="headerlink" title="本章小结及疑难点"></a>本章小结及疑难点</h3><h4 id="“尽最大努力交付”有哪些含义"><a href="#“尽最大努力交付”有哪些含义" class="headerlink" title="“尽最大努力交付”有哪些含义?"></a>“尽最大努力交付”有哪些含义?</h4><p>1）不保证源主机发送的IP数据报一定无差错地交付到目的主机。</p>
<p>2）不保证源主机发送的IP数据报都在某一规定的时间内交付到目的主机。</p>
<p>3）不保证源主机发送的IP数据报一定按发送时的顺序交付到目的主机。</p>
<p>4）不保证源主机发送的IP数据报不会重复交付给目的主机。</p>
<p>5）不故意丢弃IP数据报。丢弃IP数据报的情况是:路由器检测出首部校验和有错误;或由于网络中通信量过大，路由器或目的主机中的缓存已无空闲空间。</p>
<p>但要注意，IP数据报的首部中有一个“首部校验和”。当它检验出IP数据报的首部出现了差错时，就丢弃该数据报。因此，凡交付给目的主机的IP数据报都是IP首部没有差错的或没有检测出差错的。也就是说，在传输过程中，出现差错的IP数据报都被丢弃了。</p>
<p>现在因特网上绝大多数的通信量都属于“尽最大努力交付”。如果数据必须可靠地交付给目的地,那么使用IP的高层软件必须负责解决这一问题。</p>
<h4 id="“IP网关”和“IP路由器”是否为同义语-“互连网”和“互联网”有没有区别"><a href="#“IP网关”和“IP路由器”是否为同义语-“互连网”和“互联网”有没有区别" class="headerlink" title="“IP网关”和“IP路由器”是否为同义语?“互连网”和“互联网”有没有区别?"></a>“IP网关”和“IP路由器”是否为同义语?“互连网”和“互联网”有没有区别?</h4><p>当初发明TCP/IP的研究人员使用IP Gateway作为网际互联的设备，可以认为“IP网关”和“IP路由器”是同义词。</p>
<p>“互连网”和“互联网”都是推荐名，都可以使用，不过建议优先使用“互联网”。</p>
<h4 id="在一个互联网中，能否用一个很大的交换机-switch-来代替互联网中很多的路由器"><a href="#在一个互联网中，能否用一个很大的交换机-switch-来代替互联网中很多的路由器" class="headerlink" title="在一个互联网中，能否用一个很大的交换机( switch)来代替互联网中很多的路由器?"></a>在一个互联网中，能否用一个很大的交换机( switch)来代替互联网中很多的路由器?</h4><p>不行。交换机和路由器的功能是不相同的。</p>
<p>交换机可在单个网络中与若干计算机相连，并且可以将一台计算机发送过来的帧转发给另一台计算机。从这一点上看，交换机具有集线器的转发帧的功能，但交换机比集线器的功能强很多。在同一时间，集线器只允许一台计算机发送数据。</p>
<p>路由器连接两个或多个同构的或异构的网络，在网络之间转发分组（即IP数据报)。</p>
<p>因此，如果许多相同类型的网络互联时，那么用一个很大的交换机（如果能够找其他计算机进行通信，交换机允许找得到）代替原来的一些路由器是可行的。但若这些互联的网络是异构的网络，那么就必须使用路由器来进行互联。</p>
<h4 id="网络前缀是指网络号字段（-net-id）中前面的几个类别位还是指整个的网络号字段"><a href="#网络前缀是指网络号字段（-net-id）中前面的几个类别位还是指整个的网络号字段" class="headerlink" title="网络前缀是指网络号字段（ net-id）中前面的几个类别位还是指整个的网络号字段?"></a>网络前缀是指网络号字段（ net-id）中前面的几个类别位还是指整个的网络号字段?</h4><p>是指整个的网络号字段，包括最前面的几个类别位在内。网络前缀常常简称为前缀。例如一个B类地址10100000 00000000 00000000 00010000，其类别位就是最前面的两位:10，而网络前缀就是前16位:10100000 00000000。</p>
<h4 id="IP有分片的功能，但广域网中的分组则不必分片，这是为什么"><a href="#IP有分片的功能，但广域网中的分组则不必分片，这是为什么" class="headerlink" title="IP有分片的功能，但广域网中的分组则不必分片，这是为什么?"></a>IP有分片的功能，但广域网中的分组则不必分片，这是为什么?</h4><p>IP数据报可能要经过许多个网络，而源结点事先并不知道数据报后面要经过的这些网络所能通过的分组的最大长度是多少。等到IP数据报转发到某个网络时，中间结点可能才发现数据报太长了，因此在这时就必须进行分片。</p>
<p>但广域网能够通过的分组的最大长度是该广域网中所有结点都事先知道的，源结点不可能发送网络不支持的过长分组。因此广域网没有必要将已经发送出的分组再进行分片。</p>
<h4 id="数据链路层广播和IP广播有何区别"><a href="#数据链路层广播和IP广播有何区别" class="headerlink" title="数据链路层广播和IP广播有何区别?"></a>数据链路层广播和IP广播有何区别?</h4><p>数据链路层广播是用数据链路层协议（第二层)在一个以太网上实现的对该局域网上的所有主机进行广播MAC帧，而P广播则是用P通过因特网实现的对一个网络（即目的网络）上的所有主机进行广播IP数据报。</p>
<h4 id="主机在接收一个广播帧或组播帧时，其CPU所要做的事情有何区别"><a href="#主机在接收一个广播帧或组播帧时，其CPU所要做的事情有何区别" class="headerlink" title="主机在接收一个广播帧或组播帧时，其CPU所要做的事情有何区别?"></a>主机在接收一个广播帧或组播帧时，其CPU所要做的事情有何区别?</h4><p>在接收广播帧时，主机通过其适配器〔即网络接口卡(NIC)]接收每个广播帧，然后将其传递给操作系统。CPU执行协议软件，并界定是否接收和处理该帧。在接收组播帧时，CPU要对适配器进行配置，而适配器根据特定的组播地址表来接收帧。凡与此组播地址表不匹配的帧都将被NIC 丢弃。因此在组播的情况下，是适配器NIC而不是CPU决定是否接收一个帧。</p>
<h4 id="假定在一个局域网中计算机A发送ARP请求分组，希望找出计算机B的硬件地址。这时局域网上的所有计算机都能收到这个广播发送的ARP请求分组。试问这时由哪个计算机使用ARP响应分组将计算机B的硬件地址告诉计算机A"><a href="#假定在一个局域网中计算机A发送ARP请求分组，希望找出计算机B的硬件地址。这时局域网上的所有计算机都能收到这个广播发送的ARP请求分组。试问这时由哪个计算机使用ARP响应分组将计算机B的硬件地址告诉计算机A" class="headerlink" title="假定在一个局域网中计算机A发送ARP请求分组，希望找出计算机B的硬件地址。这时局域网上的所有计算机都能收到这个广播发送的ARP请求分组。试问这时由哪个计算机使用ARP响应分组将计算机B的硬件地址告诉计算机A?"></a>假定在一个局域网中计算机A发送ARP请求分组，希望找出计算机B的硬件地址。这时局域网上的所有计算机都能收到这个广播发送的ARP请求分组。试问这时由哪个计算机使用ARP响应分组将计算机B的硬件地址告诉计算机A?</h4><p>这要区分两种情况。第一，如果计算机B和计算机A都连接在同一个局域网上，那么就是计算机B发送ARP响应分组。第二，如果计算机B和计算机A不连接在同一个局域网上，那么就必须由一个连接计算机A所在局域网的路由器来转发ARP请求分组。这时，该路由器向计算机A发送ARP回答分组，给出自己的硬件地址。</p>
<h4 id="路由器实现了物理层、数据链路层、网络层，这句话的含义是什么"><a href="#路由器实现了物理层、数据链路层、网络层，这句话的含义是什么" class="headerlink" title="路由器实现了物理层、数据链路层、网络层，这句话的含义是什么?"></a>路由器实现了物理层、数据链路层、网络层，这句话的含义是什么?</h4><p>第1章中提到了网络中的两个通信结点利用协议栈进行通信的过程。发送方一层一层地把数据“包装”，接收方一层一层地把“包装”拆开，最后上交给用户。路由器实现了物理层，数据链路层和网络层的含义是指路由器有能力对这三层协议的控制信息进行识别、分析以及转换，直观的理解是路由器有能力对数据“包装”这三层协议或者“拆开”这三层协议。自然，路由器就有能力互联这三层协议不同的两个网络。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://ednow.github.io/2021/07/24/%E7%8E%8B%E9%81%93-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-ch3-%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ednow">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ednow">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/24/%E7%8E%8B%E9%81%93-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-ch3-%E6%95%B0%E6%8D%AE%E9%93%BE%E8%B7%AF%E5%B1%82/" class="post-title-link" itemprop="url">王道-计算机网络-ch3-数据链路层</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-07-24 09:09:18" itemprop="dateCreated datePublished" datetime="2021-07-24T09:09:18+08:00">2021-07-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-10-07 21:19:54" itemprop="dateModified" datetime="2021-10-07T21:19:54+08:00">2021-10-07</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%80%83%E7%A0%94/" itemprop="url" rel="index"><span itemprop="name">考研</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="王道"><a href="#王道" class="headerlink" title="王道"></a>王道</h1><h2 id="数据链路层"><a href="#数据链路层" class="headerlink" title="数据链路层"></a>数据链路层</h2><p>【考纲内容】<br>(一）数据链路层的功能</p>
<p>(二）组帧</p>
<p>(三）差错控制</p>
<p>检错编码;纠错编码</p>
<p>(四）流量控制与可靠传输机制</p>
<p>流量控制、可靠传输与滑动窗口机制;停止-等待协议</p>
<p>后退N帧协议(GBN);选择重传协议(SR)</p>
<p>(五）介质访问控制</p>
<p>1．信道划分</p>
<p>频分多路复用、时分多路复用、波分多路复用、码分多路复用的概念和基本原理</p>
<p>2．随机访问</p>
<p>ALOHA协议;CSMA协议;CSMA/CD协议;CSMA/CA协议</p>
<p>3．轮询访问:令牌传递协议</p>
<p>(六）局域网<br>局域网的基本概念与体系结构;以太网与IEEE 802.3</p>
<p>IEEE 802.11;<del>令牌环网的基本原理（2022考纲已删除）</del>  $\color{red}{\text{无线局域网（2022新增）}}$ </p>
<p> $\color{red}{\text{VLAN基本概念与原理（2022新增）}}$ </p>
<p>(七）广域网</p>
<p>广域网的基本概念;PPP协议;<del>HDLC协议（2022考纲已删除）</del></p>
<p>(八）数据链路层设备</p>
<p><del>网桥的概念及其基本原理（2022考纲已删除）</del>;局域网交换机及其工作原理</p>
<p>【复习提示】</p>
<p>本章是历年考查的重点。要求在了解数据链路层基本概念和功能的基础上，重点掌握滑动窗口机制、三种可靠传输协议、各种 MAC协议、HDLC协议和PPP协议，特别是CSMA/CD协议和以太网帧格式，以及局域网的争用期和最小帧长的概念、二进制指数退避算法。此外，中继器、网卡、集线器、网桥和局域网交换机的原理及区别也要重点掌握。</p>
<h3 id="数据链路层的功能"><a href="#数据链路层的功能" class="headerlink" title="数据链路层的功能"></a>数据链路层的功能</h3><p>数据链路层在物理层提供服务的基础上向网络层提供服务，其主要作用是加强物理层传输原始比特流的功能，将物理层提供的可能出错的物理连接改造为 $\color{green}{\text{逻辑上无差错}}$ 的数据链路，使之对网络层表现为一条无差错的链路。下面具体介绍数据链路层的功能。</p>
<h4 id="为网络层提供服务"><a href="#为网络层提供服务" class="headerlink" title="为网络层提供服务"></a>为网络层提供服务</h4><p>对网络层而言，数据链路层的基本任务是将源机器中来自网络层的数据传输到目标机器的网络层。数据链路层通常可为网络层提供如下服务:</p>
<p>1） $\color{green}{\text{无确认的无连接服务}}$ 。源机器发送数据帧时不需先建立链路连接，目的机器收到数据帧时不需发回确认。对丢失的帧，数据链路层不负责重发而交给上层处理。适用于实时通信或误码率较低的通信信道，如以太网。</p>
<p>2） $\color{green}{\text{有确认的无连接服务}}$ 。源机器发送数据帧时不需先建立链路连接，但目的机器收到数据帧时必须发回确认。源机器在所规定的时间内未收到确定信号时，就重传丢失的帧，以提高传输的可靠性。该服务适用于误码率较高的通信信道，如无线通信。</p>
<p>3） $\color{green}{\text{有确认的面向连接服务}}$ 。帧传输过程分为三个阶段:建立数据链路、传输帧、释放数据链路。目的机器对收到的每一帧都要给出确认，源机器收到确认后才能发送下一-帧，因而该服务的可靠性最高。该服务适用于通信要求（可靠性、实时性）较高的场合。注意:有连接就一定要有确认，即不存在无确认的面向连接的服务。</p>
<h4 id="链路管理"><a href="#链路管理" class="headerlink" title="链路管理"></a>链路管理</h4><p>数据链路层连接的 $\color{green}{\text{建立}}$ 、 $\color{green}{\text{维持}}$ 和 $\color{green}{\text{释放}}$ 过程称为 $\color{red}{\text{链路管理}}$ ，它主要用于面向连接的服务。链路两端的结点要进行通信，必须首先确认对方已处于就绪状态，并交换一些必要的信息以对帧序号初始化，然后才能建立连接，在传输过程中则要能维持连接，而在传输完毕后要释放该连接。在多个站点共享同一物理信道的情况下（如在局域网中）如何在要求通信的站点间分配和管理信道也属于数据链路层管理的范畴。</p>
<h4 id="帧定界、帧同步与透明传输"><a href="#帧定界、帧同步与透明传输" class="headerlink" title="帧定界、帧同步与透明传输"></a>帧定界、帧同步与透明传输</h4><p>两台主机之间传输信息时，必须将网络层的分组封装成帧，以帧的格式进行传送。将一段数据的前后分别添加 $\color{green}{\text{首部}}$ 和 $\color{green}{\text{尾部}}$ ，就构成了 $\color{red}{\text{帧}}$ 。因此，帧长等于数据部分的长度加上首部和尾部的长度。首部和尾部中含有很多控制信息，它们的一个重要作用是确定帧的界限，即 $\color{green}{\text{帧定界}}$ 。而帧同步指的是接收方应能从接收到的二进制比特流中区分出帧的起始与终止。如在HDLC协议中，用标识位F （01111110）来标识帧的开始和结束。通信过程中，检测到帧标识位F即认为是帧的开始，然后一旦检测到帧标识位F即表示帧的结束。HDLC标准帧格式如图3.1所示。为了提高帧的传输效率，应当使帧的数据部分的长度尽可能地大于首部和尾部的长度，但每种数据链路层协议都规定了帧的数据部分的长度上限– $\color{green}{\text{最大传送单元}}$ （MTU)。</p>
<details><summary>图3.1 HDLC标准帧格式</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004082646.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004082646.png';" /></details>

<p>如果在数据中恰好出现与帧定界符相同的比特组合（会误认为“传输结束”而丢弃后面的数据)，那么就要采取有效的措施解决这个问题，即透明传输。更确切地说， $\color{red}{\text{透明传输}}$ 就是不管所传数据是什么样的比特组合，都应当能在链路上传送。</p>
<h4 id="流量控制"><a href="#流量控制" class="headerlink" title="流量控制"></a>流量控制</h4><p>由于收发双方各自的工作速率和缓存空间的差异，可能出现发送方的发送能力大于接收方的接收能力的现象，如若此时不适当限制发送方的发送速率（即链路上的信息流量)，前面来不及接收的帧将会被后面不断发送来的帧“淹没”，造成帧的丢失而出错。因此，流量控制实际上就是限制发送方的数据流量，使其发送速率不超过接收方的接收能力。</p>
<p>这个过程需要通过某种反馈机制使发送方能够知道接收方是否能跟上自己，即需要有一些规则使得发送方知道在什么情况下可以接着发送下一帧，而在什么情况下必须暂停发送，以等待收到某种反馈信息后继续发送。</p>
<p>流量控制（见图3.2）并不是数据链路层特有的功能，许多高层协议中也提供此功能，只不过控制的对象不同而已。对于数据链路层来说，控制的是 $\color{green}{\text{相邻两结点}}$ 之间数据链路上的流量，而对于 $\color{red}{\text{运输层}}$ 来说，控制的则是从 $\color{green}{\text{源端}}$ 到 $\color{green}{\text{目的端}}$ 之间的流量。</p>
<details><summary>图3.2数据链路层的流量控制</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004082806.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004082806.png';" /></details>

<h4 id="差错控制"><a href="#差错控制" class="headerlink" title="差错控制"></a>差错控制</h4><p>由于信道噪声等各种原因，帧在传输过程中可能会出现错误。用以使发送方确定接收方是否正确收到由其发送的数据的方法称为 $\color{green}{\text{差错控制}}$ 。通常，这些错误可分为位错和帧错。</p>
<p>位错指帧中某些位出现了差错。通常采用循环冗余校验（CRC)方式发现位错，通过 $\color{green}{\text{自动重传请求}}$ (Automatic Repeat reQuest，ARQ）方式来重传出错的帧。具体做法是:让发送方将要发送的数据帧附加一定的CRC冗余检错码一并发送，接收方则根据检错码对数据帧进行错误检测,若发现错误则丢弃，发送方超时重传该数据帧。这种差错控制方法称为ARQ法。ARQ法只需返回很少的控制信息就可有效地确认所发数据帧是否被正确接收。</p>
<p>帧错指帧的 $\color{green}{\text{丢失}}$ 、 $\color{green}{\text{重复}}$ 或 $\color{green}{\text{失序}}$ 等错误。在数据链路层引入定时器和编号机制，能保证每一帧最终都能有且仅有一次正确地交付给目的结点。</p>
<h3 id="组帧"><a href="#组帧" class="headerlink" title="组帧"></a>组帧</h3><p>数据链路层之所以要把比特组合成帧为单位传输，是为了在出错时只重发出错的帧，而不必重发全部数据，从而提高效率。为了使接收方能正确地接收并检查所传输的帧，发送方必须依据一定的规则把网络层递交的分组封装成帧（称为组帧)。组帧主要解决帧定界、帧同步、透明传输等问题。通常有以下4种方法实现组帧。</p>
<blockquote>
<p>注意:组帧时既要加首部，又要加尾部。原因是，在网络中信息是以帧为最小单位进行传输的，所以接收端要正确地接收帧，必须要清楚该帧在一串比特流中从哪里开始到哪里结束（因为接收端收到的是一串比特流，没有首部和尾部是不能正确区分帧的)。而分组（即IP数据报）仅是包含在帧中的数据部分（后面将详细讲解)，所以不需要加尾部来定界。</p>
</blockquote>
<h4 id="字符计数法"><a href="#字符计数法" class="headerlink" title="字符计数法"></a>字符计数法</h4><p>如图3.3所示，字符计数法是指在 $\color{red}{\text{帧头部}}$ 使用一个 $\color{green}{\text{计数字段}}$ 来标明 $\color{green}{\text{帧内字符数}}$ 。目的结点的数据链路层收到字节计数值时，就知道后面跟随的字节数，从而可以确定帧结束的位置（计数字段提供的字节数包含自身所占用的一个字节)。</p>
<details><summary>图3.3字符计数法</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004082948.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004082948.png';" /></details>

<p>这种方法最大的问题在于如果计数字段出错，即失去了帧边界划分的依据，那么接收方就无法判断所传输帧的结束位和下一帧的开始位，收发双方将失去同步，从而造成灾难性后果。</p>
<h4 id="字符填充的首尾定界符法"><a href="#字符填充的首尾定界符法" class="headerlink" title="字符填充的首尾定界符法"></a>字符填充的首尾定界符法</h4><p>字符填充法使用特定字符来定界一帧的开始与结束，在图3.4的例子中，控制字符SOH放在帧的最前面，表示帧的首部开始，控制字符EOT表示帧的结束。为了使信息位中出现的特殊字符不被误判为帧的首尾定界符，可在特殊字符前面填充一个转义字符（ESC)来加以区分（注意，转义字符是ASCII码中的控制字符，是一个字符，而非“E””S”“C”三个字符的组合)，以实现数据的透明传输。接收方收到转义字符后，就知道其后面紧跟的是数据信息，而不是控制信息。</p>
<p>如图3.4(a)所示的字符帧，帧的数据段中出现EOT或SOH字符，发送方在每个EOT或SOH字符前再插入一个ESC字符[ 见图3.4(b) ]，接收方收到数据后会自己删除这个插入的ESC字符，结果仍得到原来的数据[ 见图3.4(c) ]。这也正是字符填充法名称的 $\color{red}{\text{由来}}$ 。如果转义字符ESC $\color{green}{\text{也}}$ 出现在数据中，那么解决方法仍是在转义字符前 $\color{green}{\text{插入}}$ 一个转义字符。</p>
<details><summary>图3.4字符填充法</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004083032.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004083032.png';" /></details>

<h4 id="零比特填充的首尾标志法"><a href="#零比特填充的首尾标志法" class="headerlink" title="零比特填充的首尾标志法"></a>零比特填充的首尾标志法</h4><p>如图3.5所示，零比特填充法允许数据帧包含任意个数的比特，也允许每个字符的编码包含任意个数的比特。它使用一个特定的比特模式，即 01111110来标志一帧的开始和结束。为了不使信息位中出现的比特流01111110被误判为帧的首尾标志，发送方的数据链路层在信息位中遇到5个连续的“1”时，将自动在其后插入一个“0”，而接收方做该过程的逆操作，即每收到5个连续的“1”时，自动删除后面紧跟的“0”，以恢复原信息。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004083059.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004083059.png';" /></details>

<p>零比特填充法很容易由硬件来实现，性能优于字符填充法。</p>
<ul>
<li> $\color{red}{\text{Q}}$ ：怎么知道现在连续的1是在信息位中，还是结尾字符呢</li>
</ul>
<h4 id="违规编码法"><a href="#违规编码法" class="headerlink" title="违规编码法"></a>违规编码法</h4><p>在物理层进行比特编码时，通常采用违规编码法。例如，曼彻斯特编码方法将数据比特编码成“高-低”电平对，将数据比特“0”编码成“低-高”电平对，而“高-高”电平对和“低-低”电平对在数据比特中是违规的（即没有采用)。可以借用这些违规编码序列来定界帧的起始和终止。局域网IEEE 802标准就采用了这种方法。</p>
<p>违规编码法不需要采用任何填充技术，便能实现数据传输的透明性，但它只适用于采用 $\color{green}{\text{冗余编码}}$ 的特殊编码环境。</p>
<p>由于字节计数法中计数字段的脆弱性和字符填充法实现上的复杂性与不兼容性，目前较常用的组帧方法是比特填充法和违规编码法。</p>
<h3 id="差错控制-1"><a href="#差错控制-1" class="headerlink" title="差错控制"></a>差错控制</h3><p>实际通信链路都不是理想的，比特在传输过程中可能会产生差错，1可能会变成0，0也可能会变成1，这就是比特差错。比特差错是传输差错中的一种，本节仅讨论比特差错。</p>
<p>通常利用编码技术进行差错控制,主要有两类: $\color{green}{\text{自动重传请求ARQ}}$ 和 $\color{green}{\text{前向纠错FEC}}$ 。在ARQ方式中，接收端检测出差错时，就设法通知发送端重发，直到接收到正确的码字为止。在FEC方式中，接收端不但能发现差错，而且能确定比特串的错误位置，从而加以纠正。因此，差错控制又可分为检错编码和纠错编码。</p>
<h4 id="检错编码"><a href="#检错编码" class="headerlink" title="检错编码"></a>检错编码</h4><p>检错编码都采用冗余编码技术，其核心思想是在有效数据（信息位）被发送前，先按某种关系附加一定的冗余位，构成一个符合某一规则的码字后再发送。当要发送的有效数据变化时，相应的冗余位也随之变化，使得码字遵从不变的规则。接收端根据收到的码字是否仍符合原规则来判断是否出错。常见的检错编码有奇偶校验码和循环冗余码。</p>
<blockquote>
<p>注意:建议结合《计算机组成原理考研复习指导》第2章有关校验码的内容对比复习。</p>
</blockquote>
<h5 id="奇偶校验码"><a href="#奇偶校验码" class="headerlink" title="奇偶校验码"></a>奇偶校验码</h5><p>奇偶校验码是奇校验码和偶校验码的统称，是一种最基本的检错码。它由 $n$ -1位信息元和1位校验元组成，如果是奇校验码，那么在附加一个校验元后，码长为 $n$ 的码字中“1”的个数为奇数;如果是偶校验码，那么在附加一个校验元以后，码长为 $n$ 的码字中“1”的个数为偶数。它只能检测奇数位的出错情况，但并不知道哪些位错了，也不能发现偶数位的出错情况。</p>
<h5 id="循环冗余码"><a href="#循环冗余码" class="headerlink" title="循环冗余码"></a>循环冗余码</h5><details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004083438.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004083438.png';" /></details>

<p>通过循环冗余码（CRC)的检错技术，数据链路层做到了对帧的无差错接收。也就是说，凡是接收端数据链路层接收的帧，我们都认为这些帧在传输过程中没有产生差错;而接收端丢弃的帧虽然也收到了，但最终因为有差错而被丢弃，即未被接受。</p>
<blockquote>
<p>注意:循环冗余码(CRC)是具有 $\color{green}{\text{纠错功能}}$ 的，只是 $\color{green}{\text{数据链路层}}$ 仅使用了它的 $\color{green}{\text{检错功能}}$ ，检测到帧出错则直接丢弃，因此本节将CRC放在检错编码中介绍。</p>
</blockquote>
<ul>
<li>重点在不 $\color{green}{\text{借位}}$ ， $\color{green}{\text{异或}}$ </li>
</ul>
<h4 id="纠错编码"><a href="#纠错编码" class="headerlink" title="纠错编码"></a>纠错编码</h4><p>在数据通信的过程中，解决差错问题的一种方法是在每个要发送的数据块上附加足够的冗余信息，使接收方能够推导出发送方实际送出的应该是什么样的比特串。最常见的纠错编码是 $\color{green}{\text{海明码}}$ ，其实现原理是在有效信息位中加入几个校验位形成海明码，并把海明码的每个二进制位分配到几个奇偶校验组中。当某一位出错后，就会引起有关的几个校验位的值发生变化，这不但可以发现错位，而且能指出错位的位置，为自动纠错提供依据。</p>
<p>现以数据码1010为例讲述海明码的编码原理和过程。</p>
<details><summary>海明码的编码原理和过程</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004083637.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004083637.png';" /></details>

<h3 id="流量控制与可靠传输机制"><a href="#流量控制与可靠传输机制" class="headerlink" title="流量控制与可靠传输机制"></a>流量控制与可靠传输机制</h3><h4 id="流量控制、可靠传输与滑动窗口机制"><a href="#流量控制、可靠传输与滑动窗口机制" class="headerlink" title="流量控制、可靠传输与滑动窗口机制"></a>流量控制、可靠传输与滑动窗口机制</h4><p>流量控制涉及对链路上的帧的发送速率的控制，以使接收方有足够的缓冲空间来接收每个帧。例如，在面向帧的自动重传请求系统中，当待确认帧的数量增加时，有可能超出缓冲存储空间而造成过载。流量控制的基本方法是由接收方控制发送方发送数据的速率，常见的方式有两种:停止-等待协议和滑动窗口协议。</p>
<h5 id="停止-等待流量控制基本原理"><a href="#停止-等待流量控制基本原理" class="headerlink" title="停止-等待流量控制基本原理"></a>停止-等待流量控制基本原理</h5><p>发送方每发送一帧，都要等待接收方的应答信号，之后才能发送下一帧;接收方每接收一帧，都要反馈一个应答信号，表示可接收下一帧，如果接收方不反馈应答信号，那么发送方必须一直等待。每次只允许发送一帧，然后就陷入等待接收方确认信息的过程中，因而传输效率很低。</p>
<h5 id="滑动窗口流量控制基本原理"><a href="#滑动窗口流量控制基本原理" class="headerlink" title="滑动窗口流量控制基本原理"></a>滑动窗口流量控制基本原理</h5><p>在任意时刻，发送方都维持一组连续的允许发送的帧的序号，称为 $\color{green}{\text{发送窗口}}$ ﹔同时接收方也维持一组连续的允许接收帧的序号，称为 $\color{green}{\text{接收窗口}}$ 。发送窗口用来对发送方进行流量控制，而发送窗口的大小W代表在还未收到对方确认信息的情况下发送方最多还可以发送多少个数据帧。同理，在接收端设置接收窗口是为了控制可以接收哪些数据帧和不可以接收哪些帧。在接收方，只有收到的数据帧的序号落入接收窗口内时，才允许将该数据帧收下。若接收到的数据帧落在接收窗口之 $\color{green}{\text{外}}$ ，则 $\color{green}{\text{一律}}$ 将其 $\color{green}{\text{丢弃}}$ 。</p>
<p>图3.7给出了发送窗口的工作原理，图3.8给出了接收窗口的工作原理。</p>
<details><summary>图3.7发送窗口的工作原理</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004085724.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004085724.png';" /></details>

<details><summary>图3.8接收窗口的工作原理</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004085755.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004085755.png';" /></details>

<p>发送端每收到一个确认帧，发送窗口就向前滑动一个帧的位置，当发送窗口内没有可以发送的帧（即窗口内的帧全部是已发送但未收到确认的帧)时，发送方就会停止发送，直到收到接收方发送的确认帧使窗口移动，窗口内有可以发送的帧后，才开始继续发送。</p>
<p>接收端收到数据帧后，将窗口向前移一个位置，并发回确认帧，若收到的数据帧落在接收窗口之外，则一律丢弃。</p>
<p>滑动窗口有以下重要特性:</p>
<p>1）只有接收窗口向前滑动（同时接收方发送了确认帧）时，发送窗口才有可能（只有发送方收到确认帧后才一定）向前 $\color{green}{\text{滑动}}$ 。</p>
<p>2）从滑动窗口的概念看，停止-等待协议、后退N帧协议和选择重传协议只在发送窗口大小与接收窗口大小上有所差别:</p>
<p>$\color{green}{\text{停止-等待协议}}$ :发送窗口大小= 1，接收窗口大小=1。</p>
<p>$\color{green}{\text{后退N帧协议}}$ :发送窗口大小&gt;1，接收窗口大小=1。</p>
<p>$\color{green}{\text{选择重传协议}}$ :发送窗口大小&gt;1，接收窗口大小&gt;1。</p>
<p>3）接收窗口的大小为1时，可保证帧的 $\color{green}{\text{有序}}$ 接收。</p>
<p>4）数据链路层的滑动窗口协议中，窗口的大小在传输过程中是 $\color{green}{\text{固定的}}$ （注意与 $\color{red}{\text{第5章}}$ 传输层的滑动窗口协议的区别)。</p>
<h5 id="可靠传输机制"><a href="#可靠传输机制" class="headerlink" title="可靠传输机制"></a>可靠传输机制</h5><p>数据链路层的可靠传输通常使用 $\color{green}{\text{确认}}$ 和 $\color{green}{\text{超时重传}}$  两种机制来完成。确认是一种无数据的控制帧，这种控制帧使得接收方可以让发送方知道哪些内容被正确接收。有些情况下为了提高传输效率，将确认捎带在一个回复帧中，称为 $\color{green}{\text{捎带确认}}$ 。超时重传是指发送方在发送某个数据帧后就开启一个计时器，在一定时间内如果没有得到发送的数据帧的确认帧，那么就重新发送该数据帧,直到发送成功为止。</p>
<p>自动重传请求（Auto Repeat reQuest，ARQ）通过接收方请求发送方重传出错的数据帧来恢复出错的帧，是通信中用于处理信道所带来差错的方法之一。传统 $\color{red}{\text{自动重传请求}}$ 分为三种，即 $\color{green}{\text{停止-等待}}$ (Stop-and-Wait）ARQ、 $\color{green}{\text{后退N帧}}$ (Go-Back-N)ARQ和 $\color{green}{\text{选择性重传}}$ (Selective Repeat)ARQ。后两种协议是滑动窗口技术与请求重发技术的结合，由于窗口尺寸开到足够大时，帧在线路上可以连续地流动，因此又称其为 $\color{green}{\text{连续ARQ协议}}$ 。注意，在数据链路层中 $\color{green}{\text{流量控制机制}}$ 和 $\color{green}{\text{可靠传输机制}}$ 是 $\color{red}{\text{交织在一起}}$ 的。</p>
<blockquote>
<p>注意:现有的实际有线网络的数据链路层很少采用可靠传输(不同于OSI参考模型的思路),因此大多数教材把这部分内容放在第5章运输层中讨论，本书按照408考纲，不做变动。</p>
</blockquote>
<h4 id="单帧滑动窗口与停止-等待协议"><a href="#单帧滑动窗口与停止-等待协议" class="headerlink" title="单帧滑动窗口与停止-等待协议"></a>单帧滑动窗口与停止-等待协议</h4><p>在停止-等待协议中，源站发送单个帧后必须等待确认，在目的站的回答到达源站之前，源站不能发送其他的数据帧。从滑动窗口机制的角度看，停止-等待协议相当于发送窗口和接收窗口大小均为1的滑动窗口协议。</p>
<p>在停止-等待协议中，除数据 $\color{green}{\text{帧丢失}}$ 外，还可能出现以下两种差错。</p>
<p>到达目的站的 $\color{green}{\text{帧}}$ 可能已遭 $\color{green}{\text{破坏}}$ ，接收站利用前面讨论过的差错检测技术检出后，简单地将该帧丢弃。为了对付这种可能发生的情况，源站装备了计时器。在一个帧发送之后，源站等待确认,如果在计时器计满时仍未收到确认，那么再次发送相同的帧。如此重复，直到该数据帧无错误地到达为止。</p>
<p>另一种可能的差错是数据帧正确而 $\color{green}{\text{确认帧被破坏}}$ ，此时接收方已收到正确的数据帧，但发送方收不到确认帧，因此发送方会重传已被接收的数据帧，接收方收到同样的数据帧时会丢弃该帧,并重传一个该帧对应的确认帧。发送的帧交替地用0和1来标识,肯定确认分别用ACK0和 ACK1来表示，收到的确认有误时，重传已发送的帧。对于停止-等待协议，由于每发送一个数据帧就停止并等待,因此用1bit 来编号就已足够。在停止-等待协议中，若连续出现相同发送序号的数据帧,表明发送端进行了超时重传。连续出现相同序号的确认帧时，表明接收端收到了重复帧。</p>
<p>此外，为了超时重发和判定重复帧的需要，发送方和接收方都须设置一个帧缓冲区。发送端在发送完数据帧时，必须在其发送缓存中保留此数据帧的副本，这样才能在出差错时进行重传。只有在收到对方发来的确认帧ACK时，方可清除此副本。</p>
<p>由图3.9可知，停止-等待协议通信信道的利用率很低。为了克服这一缺点，就产生了另外两种协议，即后退N帧协议和选择重传协议。</p>
<details><summary>图3.9 停止-等待协议中数据帧和确认帧的发送时间关系</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004090226.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004090226.png';" /></details>


<h4 id="多帧滑动窗口与后退N帧协议（GBN"><a href="#多帧滑动窗口与后退N帧协议（GBN" class="headerlink" title="多帧滑动窗口与后退N帧协议（GBN)"></a>多帧滑动窗口与后退N帧协议（GBN)</h4><p>在后退N帧式ARQ中，发送方无须在收到上一个帧的ACK后才能开始发送下一帧，而是可以连续发送帧。当接收方检测出失序的信息帧后，要求发送方重发最后一个正确接收的信息帧之后的所有未被确认的帧;或者当发送方发送了N个帧后，若发现该N个帧的前一个帧在计时器超时后仍未返回其确认信息，则该帧被判为出错或丢失，此时发送方就不得不重传该出错帧及随后的N个帧。换句话说，接收方只允许 $\color{green}{\text{按顺序接收帧}}$ 。</p>
<p>如图3.10 所示，源站向目的站发送数据帧。当源站发完0号帧后，可以继续发送后续的1号帧、2号帧等。源站每发送完一帧就要为该帧设置超时计时器。由于连续发送了许多帧，所以确认帧必须要指明是对哪一帧进行确认。为了减少开销，GBN协议还规定接收端不一定每收到一个正确的数据帧就必须立即发回一个确认帧，而可以在连续收到好几个正确的数据帧后，才对最后一个数据帧发确认信息，或者可在自己有数据要发送时才将对以前正确收到的帧加以 $\color{green}{\text{捎带确认}}$ 。这就是说，<strong>对某一数据帧的确认就表明该数据帧和 $\color{green}{\text{此前所有}}$ 的数据帧均已 $\color{green}{\text{正确无误}}$ 地收到</strong>。在图3.10中，ACK $n$ 表示对第 $n$ 号帧的确认，表示接收方已正确收到第 $n$ 号帧及以前的所有帧，下一次期望收到第 $n+1$ 号帧（也可能是第0号帧)。接收端只按序接收数据帧。虽然在有差错的2号帧之后接着又收到了正确的6个数据帧，但接收端都必须将这些帧丢弃。接收端虽然丢弃了这些不按序的无差错帧，但应 $\color{green}{\text{重复发送}}$ 已发送的 $\color{green}{\text{最后一个确认帧}}$ ACK1（这是为了防止已发送的确认帧ACK1丢失)。</p>
<p>后退N帧协议的接收窗口为1，可以保证按序接收数据帧。若采用 $n$ 比特对帧编号，则其 $\color{red}{\text{发送窗口的尺寸}}$  $W_T$ 应满足 $1 \leq W_T \leq 2^n-1$ 。若 $\color{red}{\text{发送窗口的尺寸}}$ 大于 $2^n-1$ ，则会造成接收方无法分辨新帧和旧帧（请参考本章疑难点3)。</p>
<p>从图3.10不难看出，后退N帧协议一方面因连续发送数据帧而提高了信道的利用率，另一方面在重传时又必须把原来已传送正确的数据帧进行重传（仅因这些数据帧的前面有一个数据帧出了错)，这种做法又使传送效率降低。由此可见，若信道的传输质量很差导致误码率较大时，后退N帧协议不一定优于停止-等待协议。</p>
<details><summary>图3.10 GBN协议的工作原理:对出错数据帧的处理
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211005165256.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211005165256.png';" /></details>

<h4 id="多帧滑动窗口与选择重传协议-SR"><a href="#多帧滑动窗口与选择重传协议-SR" class="headerlink" title="多帧滑动窗口与选择重传协议(SR)"></a>多帧滑动窗口与选择重传协议(SR)</h4><p>为进一步提高信道的利用率，可设法只重传出现差错的数据帧或计时器超时的数据帧，但此时必须加大接收窗口，以便先收下发送序号不连续但仍处在接收窗口中的那些数据帧。等到所缺序号的数据帧收到后再一并送交主机。这就是选择重传ARQ协议。</p>
<p>在选择重传协议中，每个发送缓冲区对应一个计时器，当计时器超时时，缓冲区的帧就会重传。另外，该协议使用了比上述其他协议更有效的差错处理策略，即一旦接收方怀疑帧出错，就会发一个否定帧NAK给发送方，要求发送方对NAK中指定的帧进行重传，如图3.11所示。</p>
<p>选择重传协议的接收窗口尺寸 $W_R$ 和发送窗口尺寸 $W_T$ 都大于1,一次可以发送或接收多个帧。若采用 $n$ 比特对帧编号，为了保证接收方向前移动窗口后，新窗口序号与旧窗口序号没有重叠部分，需要满足条件:接收窗口 $W_R$ +发送窗口 $W_T \leq 2^n$ 。假定仍然采用 $\color{green}{\text{累计确认}}$ 的方法，并且接收窗口 $W_R$ 显然不应超过发送窗口 $W_T$ (否则无意义)，那么接收窗口尺寸不应超过序号范围的一半，即 $W_R \leq 2^{n-1}$ 。接收窗口为最大值时， $W_{Tmax}$ =  $W_{Rmax}=2^{(n-1)}$  。需要提醒读者的是，一般情况下，在SR协议中，接收窗口的大小和发送窗口的大小是相同的。</p>
<details><summary>图3.11选择重传协议</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004091736.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004091736.png';" /></details>

<p>选择重传协议可以避免重复传送那些本已正确到达接收端的数据帧，但在接收端要设置具有相当容量的缓冲区来暂存那些未按序正确收到的帧。接收端不能接收窗口下界以下或窗口上界以上的序号的帧，因此所需缓冲区的数目等于窗口的大小，而不是序号数目。</p>
<p>在往年统考真题中曾经出现过对“信道效率”“信道的吞吐率”等概念的考查，有些读者未接触过“通信原理”等相关的课程，可能对这些概念不太熟悉，在这里给读者补充一下。</p>
<p>信道的效率，也称信道利用率。可从不同的角度来定义 $\color{green}{\text{信道的效率}}$ ，这里给出一种从时间角度的定义:信道效率是对 $\color{green}{\text{发送方}}$ 而言的，是指发送方在一个 $\color{green}{\text{发送周期}}$ 的时间内， $\color{green}{\text{有效地}}$ 发送数据所需要的时间占整个发送周期的比率。</p>
<p>例如，发送方从开始发送数据到收到第一个确认帧为止，称为一个发送周期，设为T，发送方在这个周期内共发送L比特的数据，发送方的数据传输速率为C，则发送方用于发送有效数据的时间为L/C，在这种情况下，信道的利用率为(L/C)/T。</p>
<p>从上面的讨论可以发现，求信道的利用率主要是求周期时间T和有效数据发送时间L/C，在题目中，这两个量一般不会直接给出，需要读者根据题意自行计算。</p>
<p style='text-align:center'> $\color{green}{\text{信道吞吐率}}$ =信道利用率x发送方的发送速率。</p>

<p>本节习题有不少是关于信道利用率和信道吞吐率的，以帮助读者理解和记忆这两个概念。流量控制的三种滑动窗口协议的信道利用率是一个关键知识点，希望读者能结合习题，自行推导。</p>
<h3 id="介质访问控制"><a href="#介质访问控制" class="headerlink" title="介质访问控制"></a>介质访问控制</h3><p>介质访问控制所要完成的主要任务是，为使用介质的每个结点 $\color{green}{\text{隔离}}$ 来自同一信道上其他结点所传送的信号，以协调活动结点的传输。用来决定广播信道中信道分配的协议属于数据链路层的一个子层，称为 $\color{green}{\text{介质访问控制}}$ (Medium Access Control，MAC）子层。</p>
<p>图3.12是广播信道的通信方式，结点A、B、C、D、E共享广播信道，假设A要与C发生通信，B要与D发生通信，由于它们共用一条信道，如果不加控制，那么两对结点间的通信可能会因为互相干扰而失败。介质访问控制的内容是，采取一定的措施，使得两对结点之间的通信不会发生互相干扰的情况。</p>
<p>常见的 $\color{red}{\text{介质访问控制方法}}$ 有 $\color{green}{\text{信道划分介质访问控制}}$ 、 $\color{green}{\text{随机访问介质访问控制}}$ 和 $\color{green}{\text{轮询访问介质访问控制}}$ 。其中前者是静态划分信道的方法，而后两者是动态分配信道的方法。</p>
<details><summary>图3.12广播信道的通信方式</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004091942.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004091942.png';" /></details>

<h4 id="信道划分介质访问控制"><a href="#信道划分介质访问控制" class="headerlink" title="信道划分介质访问控制"></a>信道划分介质访问控制</h4><p>信道划分介质访问控制将使用介质的每个设备与来自同一通信信道上的其他设备的通信隔离开来，把时域和频域资源合理地分配给网络上的设备。</p>
<p>下面介绍多路复用技术的概念。当传输介质的带宽超过传输单个信号所需的带宽时人们就通过在一条介质上同时携带多个传输信号的方法来提高传输系统的利用率，这就是所谓的多路复用，也是实现信道划分介质访问控制的途径。多路复用技术把多个信号组合在一条物理信道上进行传输，使多个计算机或终端设备共享信道资源，提高了信道的利用率。</p>
<p>采用多路复用技术可把多个输入通道的信息整合到一个复用通道中，在接收端把收到的信息分离出来并传送到对应的输出通道，如图3.13所示。</p>
<details><summary>图3.13多路复用原理示意图</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004092047.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004092047.png';" /></details>

<p>信道划分的实质就是通过分时、分频、分码等方法把原来的一条广播信道，逻辑上分为几条用于两个结点之间通信的互不干扰的子信道，实际上就是把广播信道转变为点对点信道。</p>
<p>信道划分介质访问控制分为以下4种。</p>
<h5 id="color-green-text-频分多路复用-FDM"><a href="#color-green-text-频分多路复用-FDM" class="headerlink" title="$\color{green}{\text{频分多路复用}}$ (FDM)"></a>$\color{green}{\text{频分多路复用}}$ (FDM)</h5><p>频分多路复用是一种将多路基带信号调制到不同频率载波上，再叠加形成一个复合信号的多路复用技术。在物理信道的可用带宽超过单个原始信号所需带宽的情况下，可将该物理信道的总带宽分割成若干与传输单个信号带宽相同（或略宽）的子信道，每个子信道传输一种信号，这就是频分多路复用，如图3.14所示。</p>
<p>每个子信道分配的带宽可不相同，但它们的总和必须不超过信道的总带宽。在实际应用中，为了防止子信道之间的干扰，相邻信道之间需要加入“保护频带”。</p>
<p>频分多路复用的优点在于充分利用了传输介质的带宽，系统效率较高;由于技术比较成熟，实现也较容易。</p>
<details><summary>图3.14 频分多路复用原理示意图
</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004092217.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004092217.png';" /></details>

<h5 id="时分多路复用-TDM"><a href="#时分多路复用-TDM" class="headerlink" title="时分多路复用(TDM)"></a>时分多路复用(TDM)</h5><ul>
<li>time-division multiplexing, TDM</li>
</ul>
<p>时分多路复用是将一条物理信道按时间分成若干时间片，轮流地分配给多个信号使用。每个时间片由复用的一个信号占用，而不像 FDM那样，同一时间同时发送多路信号。这样，利用每个信号在时间上的交叉，就可以在一条物理信道上传输多个信号，如图3.15所示。</p>
<p>就某个时刻来看，时分多路复用信道上传送的仅是某一对设备之间的信号;就某段时间而言，传送的是按时间分割的多路复用信号。但由于计算机数据的突发性，一个用户对已经分配到的子信道的利用率一般不高。统计时分多路复用(STDM，又称异步时分多路复用）是TDM的一种改进，它采用STDM帧，STDM帧并不固定分配时隙，而按需动态地分配时隙，当终端有数据要传送时，才会分配到时间片，因此可以提高线路的利用率。例如，线路传输速率为8000b/s，4个用户的平均速率都为2000b/s，当采用TDM方式时，每个用户的最高速率为2000b/s，而在STDM方式下，每个用户的最高速率可达8000b/s。</p>
<details><summary>图3.15时分多路复用原理示意图</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004092236.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004092236.png';" /></details>

<h5 id="波分多路复用-WDM"><a href="#波分多路复用-WDM" class="headerlink" title="波分多路复用(WDM)"></a>波分多路复用(WDM)</h5><p>波分多路复用即光的频分多路复用，它在一根光纤中传输多种不同波长（频率）的光信号，由于波长（频率）不同，各路光信号互不干扰，最后再用波长分解复用器将各路波长分解出来。由于光波处于频谱的高频段，有很高的带宽，因而可以实现多路的波分复用，如图3.16所示。</p>
<details><summary>图3.16 波分多路复用原理示意图</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004092423.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004092423.png';" /></details>

<h5 id="码分多路复用-CDM"><a href="#码分多路复用-CDM" class="headerlink" title="码分多路复用(CDM)"></a>码分多路复用(CDM)</h5><p>码分多路复用是采用不同的编码来区分各路原始信号的一种复用方式。与FDM和TDM不同，它既共享信道的频率，又共享时间。下面举一个直观的例子来理解码分复用。</p>
<p>假设A站要向C站运输黄豆，B站要向C站运输绿豆，A与C、B与C之间有一条公共的道路，可以类比为广播信道，如图3.17所示。在频分复用方式下，公共道路被划分为两个牛道，分别提供给A到C的车和B到C的车行走，两类车可以同时行走，但只分到了公共车道的一半，因此频分复用(波分复用也一样）共享时间而不共享空间。在时分复用万式下，先让A到C的牛走一趟，再让B到C的车走一趟，两类车交替地占用公共车道。公共牛道没有划分，内此网千共享了空间，但不共享时间。码分复用与另外两种信道划分方式大为不同，在码分复用情况下，黄豆与绿豆放在同一辆车上运送，到达C后，由C站负责把车上的黄豆和绿豆分开。因此，黄豆和绿豆的运送，在码分复用的情况下，既共享了空间，也共享了时间。</p>
<details><summary>图3.17共享信道的传输</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004092447.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004092447.png';" /></details>

<p>实际上，更常用的名词是 $\color{green}{\text{码分多址}}$ (Code Division Multiple Access，CDMA)，其原理是每个比特时间再划分成m个短的时间槽，称为码片(Chip)，通常m的值是64或128，下例中为简单起见，设m为8。每个站点被指派一个唯一的m位码片序列。发送1时，站点发送它的码片序列;发送0时，站点发送该码片序列的反码。当两个或多个站点同时发送时，各路数据在信道中线性相加。为从信道中分离出各路信号，要求各个站点的码片序列相互正交。</p>
<p>简单理解就是，A站向C站发出的信号用一个向量来表示，B站向C站发出的信号用另一个向量来表示，两个向量要求相互正交。向量中的分量，就是所谓的码片。</p>
<p>下面举例说明<a target="_blank" rel="noopener" href="https://blog.csdn.net/dog250/article/details/24552307">CDMA的原理</a>。</p>
<p>假如站点A的码片序列被指派为00011011，则A站发送00011011就表示发送比特1，发送11100100就表示发送比特0。为了方便，按惯例将码片中的0写为-1，将1写为+1，因此A站的码片序列是-1-1-1+1+1-1+1+1。</p>
<p>令向量S表示A站的码片向量，令T表示B站的码片向量。两个不同站的码片序列正交，即向量S和T的规格化内积（Inner Product）为0:</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004092751.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004092751.png';" /></details>

<p>任何一个码片向量和该码片向量自身的规格化内积都是1，任何一个码片向量和该码片反码的向量的规格化内积是-1，如</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004092821.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004092821.png';" /></details>

<p>令向量T为(-1-1+1-1+1+1+1-1)。</p>
<p>当A站向C站发送数据1时，就发送了向量(-1-1-1+1 +1-1+1 +1)。</p>
<p>当B站向C站发送数据0时，就发送了向量(+1+1-1 +1-1-1-1 +1)。两个向量到了公共信道上就进行叠加，实际上就是线性相加，得到</p>
<p style='text-align:center'> $\mathbf{S}$ - $\mathbf{T}$  =(0 0 -2 2 0 -2 0 2)</p>

<p>到达C站后，进行数据分离，如果要得到来自A站的数据，C站就必须知道A站的码片序列，让S与S–T进行规格化内积。根据叠加原理，其他站点的信号都在内积的结果中被过滤掉了，内积的相关项都是0，而只剩下A站发送的信号。得到</p>
<p style='text-align:center'>S $\cdot$ (S-T)= 1</p>

<p>所以A站发出的数据是1。同理，如果要得到来自B站的数据，那么</p>
<p style='text-align:center'>T $\cdot$ (S-T) = -1</p>

<p>因此从B站发送过来的信号向量是一个反码向量，代表0。</p>
<p>规格化内积是线性代数中的内容，它是在得到两个向量的内积后再除以向量的分量的个数。</p>
<p>码分多路复用技术具有频谱利用率高、抗干扰能力强、保密性强、语音质量好等优点，还可以减少投资和降低运行成本，主要用于无线通信系统，特别是移动通信系统。</p>
<h4 id="随机访问介质访问控制"><a href="#随机访问介质访问控制" class="headerlink" title="随机访问介质访问控制"></a>随机访问介质访问控制</h4><p>在随机访问协议中，不采用集中控制方式解决发送信息的次序问题，所有用户能根据自己的意愿随机地发送信息，占用信道全部速率。在总线形网络中，当有两个或多个用户同时发送信息时，就会产生帧的冲突（碰撞，即前面所说的相互干扰)，导致所有冲突用户的发送均以失败告终。为了解决随机接入发生的碰撞，每个用户需要按照一定的规则反复地重传它的帧，直到该帧无碰撞地通过。这些规则就是随机访问介质访问控制协议，常用的协议有ALOHA协议、CSMA协议、CSMA/CD协议和 CSMA/CA 协议等，它们的核心思想都是:胜利者通过争用获得信道，从而获得信息的发送权。因此，随机访问介质访问控制协议又称 $\color{green}{\text{争用型协议}}$ 。</p>
<p>读者会发现，如果介质访问控制采用信道划分机制，那么结点之间的通信要么共享空间，要么共享时间，要么两者都共享;而如果采用随机访问控制机制，那么各结点之间的通信就可既不共享时间，也不共享空间。所以随机介质访问控制实质上是一种将 $\color{green}{\text{广播信道}}$ 转化为 $\color{green}{\text{点到点信道}}$ 的行为，如图3.18所示。</p>
<details><summary>图3.18共享信道举例</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004093506.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004093506.png';" /></details>

<h5 id="ALOHA协议"><a href="#ALOHA协议" class="headerlink" title="ALOHA协议"></a>ALOHA协议</h5><p>夏威夷大学早期研制的随机接入系统称为ALOHA,它是Additive Link On-line HAwaii system的缩写。ALOHA协议分为纯ALOHA协议和时隙ALOHA协议两种。</p>
<p>（1） $\color{green}{\text{纯ALOHA协议}}$ </p>
<p>纯ALOHA协议的基本思想是，当网络中的任何一个站点需要发送数据时，可以不进行任何检测就发送数据。如果在一段时间内未收到确认，那么该站点就认为传输过程中发生了冲突。发送站点需要等待一段时间后再发送数据，直至发送成功。图3.19所示的模型不仅可代表总线形网络的情况，而且可以代表无线信道的情况。</p>
<details><summary>图3.19ALOHA 协议的一般模型</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004093604.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004093604.png';" /></details>


<p>图3.20表示一个纯ALOHA协议的工作原理。每个站均自由地发送数据帧。为简化问题，不考虑由信道不良而产生的误码，并假定所有站发送的帧都是定长的，帧的长度不用比特而用发送这个帧所需的时间来表示，在图3.20中用 $T_0$ 表示这段时间。</p>
<p>在图3.20的例子中，当站1发送帧1时，其他站都未发送数据，所以站1的发送必定是成功的。但随后站2和站N-1发送的帧2和帧3在时间上重叠了一些(即发生了碰撞)。碰撞的结果是，碰撞双方(有时也可能是多方）所发送的数据出现了差错，因而都须进行重传。但是发生碰撞的各站并不能马上进行重传,因为这样做必然会继续发生碰撞。纯ALOHA系统采用的重传策略是让各站等待一段随机的时间,然后再进行重传。若再次发生碰撞，则需要再等待一段随机的时间，直到重传成功为止。图中其余一些帧的发送情况是帧4发送成功，而帧5和帧6发生碰撞。</p>
<details><summary>图3.20纯ALOHA 协议的工作原理</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004093836.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004093836.png';" /></details>

<p>假设网络负载（ $T_0$ 时间内所有站点发送成功的和未成功而重传的帧数）为G，则纯ALOHA网络的吞吐量（ $T_0$ )时间内成功发送的平均帧数）为 $S=Ge^{-2G}$ 。当G=0.5时，S=0.5 $e^{-1}$  $\approx$ 0.184,这是吞吐量S可能达到的极大值。可见，纯ALOHA网络的吞吐量很低。为了克服这一缺点，人们在原始的纯ALOHA 协议的基础上进行改进，产生了时隙ALOHA 协议。</p>
<p>(2） $\color{green}{\text{时隙ALOHA 协议}}$ </p>
<p>时隙ALOHA协议把所有各站在时间上同步起来,并将时间划分为一段段等长的 $\color{green}{\text{时隙}}$ (Slot)，规定只能在每个时隙开始时才能发送一个帧。从而避免了用户发送数据的随意性，减少了数据产生冲突的可能性，提高了信道的利用率。</p>
<p>图3.21为两个站的时隙ALOHA 协议的工作原理示意图。时隙的长度 $T_0$ 。使得每个帧正好在一个时隙内发送完毕。每个帧在到达后，一般都要在缓存中等待一段小于 $T_0$ 的时间，然后才能发送出去。在一个时隙内有两个或两个以上的帧到达时，在下一个时隙将产生碰撞。碰撞后重传的策略与纯ALOHA的情况是相似的。</p>
<details><summary>图3.21 时隙ALOHA 协议的工作原理</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004094127.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004094127.png';" /></details>

<p>时隙ALOHA网络的吞吐量S与网络负载G的关系是 $S=Ge^{-G}$ 。当G=1时, $S=e^{-1}$  $\approx$ 0.368。这是吞吐量S可能达到的极大值。可见，时隙ALOHA网络比纯ALOHA网络的吞吐量大了1倍。</p>
<h5 id="CSMA协议"><a href="#CSMA协议" class="headerlink" title="CSMA协议"></a>CSMA协议</h5><p>时隙ALOHA系统的效率虽然是纯ALOHA系统的两倍，但每个站点都是随心所欲地发送数据的，即使其他站点正在发送也照发不误，因此发送碰撞的概率很大。</p>
<p>若每个站点在发送前都先侦听一下共用信道，发现信道空闲后再发送，则就会大大降低冲突的可能，从而提高信道的利用率，载波侦听多路访问(Carrier Sense Multiple Access，CSMA）协议依据的正是这一思想。CSMA协议是在ALOHA协议基础上提出的一种改进协议,它与ALOHA协议的主要区别是多了一个载波侦听装置。</p>
<p>根据侦听方式和侦听到信道忙后的处理方式不同，CSMA 协议分为三种。</p>
<h6 id="color-green-text-1-坚持CSMA"><a href="#color-green-text-1-坚持CSMA" class="headerlink" title="$\color{green}{\text{1-坚持CSMA}}$"></a>$\color{green}{\text{1-坚持CSMA}}$</h6><p>1-坚持CSMA ( 1-persistent CSMA）的基本思想是:一个结点要发送数据时，首先侦听信道;如果信道空闲，那么立即发送数据;如果信道忙，那么等待，同时继续侦听直至信道空闲;如果发生冲突，那么随机等待一段时间后，再重新开始侦听信道。</p>
<p>“1-坚持”的含义是:侦听到信道忙后，继续坚持侦听信道;侦听到信道空闲后， $\color{green}{\text{发送帧的概率}}$ 为 $\color{green}{\text{1}}$ ，即立刻发送数据。</p>
<p>传播延迟对1-坚持CSMA 协议的性能影响较大。结点A开始发送数据时，结点B也正好有数据要发送，但这时结点A发出数据的信号还未到达结点B，结点B侦听到信道空闲，于是立即发送数据，结果必然导致冲突。即使不考虑延迟，1-坚持CSMA协议也可能产生冲突。例如，结点A正在发送数据时，结点B和C也准备发送数据，侦听到信道忙，于是坚持侦听，结果当结点A一发送完毕，结点B和C就会立即发送数据，同样导致冲突。</p>
<h6 id="color-green-text-非坚持CSMA"><a href="#color-green-text-非坚持CSMA" class="headerlink" title="$\color{green}{\text{非坚持CSMA}}$"></a>$\color{green}{\text{非坚持CSMA}}$</h6><p>非坚持CSMA (Non-persistent CSMA）的基本思想是:一个结点要发送数据时，首先侦听信道;如果信道空闲，那么立即发送数据;如果信道忙，那么放弃侦听，等待一个随机的时间后再重复上述过程。</p>
<p>非坚持CSMA协议在侦听到信道忙后就放弃侦听,因此降低了多个结点等待信道空闲后同时发送数据导致冲突的概率，但也会增加数据在网络中的平均延迟。可见，信道利用率的提高是以增加数据在网络中的延迟时间为代价的。</p>
<h6 id="color-green-text-p-坚持CSMA"><a href="#color-green-text-p-坚持CSMA" class="headerlink" title="$\color{green}{\text{p-坚持CSMA}}$"></a>$\color{green}{\text{p-坚持CSMA}}$</h6><p>p-坚持CSMA (P-persistent CSMA）用于时分信道，其基本思想是:一个结点要发送数据时,首先帧听信道;如果信道忙，就持续侦听，直至信道空闲;如果信道空闲，那么以概率p 发送数据，以概率1-p推迟到下一个时隙;如果在下一个时隙信道仍然空闲，那么仍以概率p 发送数据，以概率1-p推迟到下一个时隙;这个过程一直持续到数据发送成功或因其他结点发送数据而检测到信道忙为止，若是后者，则等待下一个时隙再重新开始帧听。</p>
<p>p-坚持CSMA 在检测到信道空闲后，以概率p发送数据，以概率1-p推迟到下一个时隙，其目的是降低1-坚持CSMA协议中多个结点检测到信道空闲后同时发送数据的冲突概率;采用坚持“帧听”的目的是，试图克服非坚持CSMA协议中由于随机等待而造成的延迟时间较长的缺点。因此，p-坚持CSMA协议是非坚持CSMA协议和1-坚持CSMA协议的折中方案。</p>
<p>三种不同类型的CSMA协议比较如表3.1所示。</p>
<details><summary>表3.1三种不同类型的CSMA 协议比较</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004095030.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004095030.png';" /></details>

<h5 id="CSMA-CD-协议"><a href="#CSMA-CD-协议" class="headerlink" title="CSMA/CD 协议"></a>CSMA/CD 协议</h5><p>载波侦听多路访问/碰撞检测(Carrier Sense Multiple Access with Collision Detection, CSMA/CD)协议是CSMA协议的改进方案，适用于总线形网络或半双工网络环境。“载波帧听”就是发送前先侦听，即每个站在发送数据之前先要检测一下总线上是否有其他站点正在发送数据，若有则暂时不发送数据，等待信道变为空闲时再发送。“碰撞检测”就是边发送边侦听，即适配器边发送数据边检测信道上信号电压的变化情况，以便判断自己在发送数据时其他站点是否也在发送数据。</p>
<p>CSMA/CD的工作流程可简单概括为“ $\color{green}{\text{先听后发}}$ ， $\color{green}{\text{边听边发}}$ ， $\color{green}{\text{冲突停发}}$ ， $\color{green}{\text{随机重发}}$ ”。</p>
<p>1）适配器从网络层获得一个分组，封装成以太网帧，放入适配器的缓存，准备发送。</p>
<p>2）如果适配器侦听到信道空闲，那么它开始发送该帧。如果适配器侦听到信道忙，那么它持续侦听直至信道上没有信号能量，然后开始发送该帧。</p>
<p>3）在发送过程中，适配器持续检测信道。若一直未检测到碰撞，则顺利地把这个帧发送完毕。若检测到碰撞，则中止数据的发送，并发送一个拥塞信号，以让所有用户都知道。</p>
<p>4）在中止发送后，适配器就执行 $\color{green}{\text{指数退避}}$ 算法，等待一段随机时间后返回到步骤2）。</p>
<p>电磁波在总线上的传播速率总是有限的，因此，当某个时刻发送站检测到信道空闲时，此时信道并不一定是空闲的。如图3.22所示，设 $\tau$ 为单程传播时延。在t=0时,A发送数据。在t=  $\tau - \sigma$ 时，A发送的数据还未到达B，由于B检测到信道空闲而发送数据。经过时间 $\sigma/2$ 后，即在t =  $\tau-\sigma$ /2时，A发送的数据和B发送的数据发生碰撞，但这时A和B都不知道。在t= $\tau$ 时，B检测到碰撞,于是停止发送数据。在 $t =2\tau-\sigma$ 时，A检测到碰撞，也停止发送数据。显然，CSMA/CD 中的站不可能同时进行发送和接收，因此采用CSMA/CD协议的 $\color{green}{\text{以太网}}$ 只能进行 $\color{green}{\text{半双工通信}}$ 。</p>
<p>由图3.22可知，站A在发送帧后至多经过时间2 $\tau$ (端到端传播时延的2倍）就能知道所发送的帧有没有发生碰撞（当 $\sigma \to 0$ 时)。因此把以太网端到端往返时间 $2\tau$ 称为 $\color{green}{\text{争用期}}$ (又称 $\color{green}{\text{冲突窗口}}$ 或 $\color{green}{\text{碰撞窗口}}$ )。每个站在自己发送数据之后的一小段时间内，存在发生碰撞的可能性，只有经过争用期这段时间还未检测到碰撞时，才能确定这次发送不会发生碰撞。</p>
<details><summary>图3.22 传播时延对载波帧听的影响</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004100829.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004100829.png';" /></details>

<p>为了确保发送站在发送数据的同时能检测到可能存在的碰撞，需要在发送完帧之前就能收到自己发送出去的数据，即帧的传输时延至少要两倍于信号在总线中的传播时延，所以CSMA/CD总线网中的所有数据帧都必须要大于一个 $\color{green}{\text{最小帧长}}$ 。任何站点收到帧长小于最小帧长的帧时，就把它当作无效帧立即丢弃。最小帧长的计算公式为</p>
<p style='text-align:center'>最小帧长=总线传播时延×数据传输速率×2</p>


<p>例如,以太网规定取51.2 $\mu$ s为争用期的长度。对于10Mb/s的以太网,在争用期内可发送512bit,即64B。在以太网发送数据时，如果前64B未发生冲突，那么后续数据也就不会发生冲突(表示已成功抢占信道)。换句话说，如果发生冲突，那么就一定在前64B。由于一旦检测到冲突就立即停止发送，因此这时发送出去的数据一定小于64B。因此，以太网规定最短帧长为64B，凡长度小于64B的帧都是由于冲突而异常中止的无效帧，收到这种无效帧时应立即 $\color{green}{\text{丢弃}}$ 。</p>
<p>如果只发送小于64B的帧，如40B的帧，那么需要在MAC子层中于数据字段的后面加入一个整数字节的填充字段，以保证以太网的MAC帧的长度不小于64B。</p>
<p>除检测冲突外，CSMA/CD还能从冲突中恢复。一旦发生了冲突，参与冲突的两个站点紧接着再次发送是没有意义的，如果它们这样做，那么将会导致无休止的冲突。CSMA/CD 采用 $\color{green}{\text{二进制指数退避算法}}$ 来解决碰撞问题。算法精髓如下:</p>
<p>1）确定基本退避时间，一般取两倍的总线端到端传播时延2 $\tau$ (即争用期)。</p>
<p>2）定义参数k，它等于重传次数，但k不超过10，即 k= min[重传次数,10]。当重传次数不超过10时，k等于重传次数;当重传次数大于10时，k就不再增大而 $\color{green}{\text{一直等于10}}$ (这个条件往往容易忽略，请读者注意)。</p>
<p>3)从离散的整数集合[0,1,…,  $2^k$ -1]中随机取出一个数r，重传所需要退避的时间就是r倍的基本退避时间，即2r $\tau$ 。</p>
<p>4)当重传达 $\color{green}{\text{16次}}$ 仍不能成功时，说明网络太拥挤，认为此帧永远无法正确发出抛弃此帧并向高层报告出错（这个条件也容易忽略，请读者注意)。</p>
<p>现在来看一个例子。假设一个适配器首次试图传输一帧，当传输时，它检测到碰撞。第1次重传时，k= 1，随机数r从整数{0，1}中选择，因此适配器可选的重传推迟时间是0或2 $\tau$ 。若再次发送碰撞，则在第2次重传时，随机数r从整数{0,1,2,3}中选择，因此重传推迟时间是在0,2$\tau$,4$\tau$,6$\tau$这4个时间中随机地选取一个。以此类推。</p>
<p>使用二进制指数退避算法可使重传需要推迟的平均时间随重传次数的增大而增大(这也称 $\color{green}{\text{动态退避}}$ )，因而能降低发生碰撞的概率，有利于整个系统的稳定。</p>
<h5 id="CSMA-CA协议"><a href="#CSMA-CA协议" class="headerlink" title="CSMA/CA协议"></a>CSMA/CA协议</h5><p>CSMA/CD 协议已成功应用于使用有线连接的局域网，但在无线局域网环境下，却不能简单地搬用CSMA/CD协议，特别是碰撞检测部分。主要有两个原因:</p>
<p>1）接收信号的强度往往会远小于发送信号的强度，且在无线介质上信号强度的动态变化范围很大，因此若要实现碰撞检测，则硬件上的花费就会过大。</p>
<p>2）在无线通信中，并非所有的站点都能够听见对方，即存在“ $\color{green}{\text{隐蔽站}}$ ”问题。</p>
<p>为此，802.11标准定义了广泛应用于无线局域网的CSMA/CA协议，它对CSMA/CD协议进行了修改，把碰撞检测改为碰撞避免（Collision Avoidance，CA)。“碰撞避免”并不是指协议可以完全避免碰撞，而是指协议的设计要尽量降低碰撞发生的概率。由于802.11无线局域网不使用碰撞检测，一旦站点开始发送一个帧，就会完全地发送该帧，但碰撞存在时仍然发送整个数据帧(尤其是长数据帧）会严重降低网络的效率，因此要采用碰撞避免技术降低碰撞的可能性。</p>
<p>由于无线信道的通信质量远不如有线信道，802.11使用链路层 $\color{green}{\text{确认}}$ / $\color{green}{\text{重传}}$ （ARQ）方案，即站点每通过无线局域网发送完一帧，就要在收到对方的确认帧后才能继续发送下一帧。</p>
<p>为了尽量避免碰撞，802.11规定，所有的站完成发送后，必须再等待一段很短的时间（继续监听）才能发送下一帧。这段时间称为 $\color{green}{\text{帧间间隔}}$ （InterFrame Space，IFS)。帧间间隔的长短取决于该站要发送的帧的类型。802.11使用了3种IFS:</p>
<p>1)SIFS(短 IFS):最短的IFS，用来分隔属于一次对话的各帧，使用SIFS的帧类型有ACK帧、CTS帧、分片后的数据帧，以及所有回答AP探询的帧等。</p>
<p>2）PIFS （点协调IFS):中等长度的IFS，在PCF操作中使用。</p>
<p>3）DIFS(分布式协调IFS):最长的IFS，用于异步帧竞争访问的时延。</p>
<p>CSMA/CA 的退避算法和CSMA/CD的稍有不同(见教材)。信道从忙态变为空闲态时，任何一个站要发送数据帧，不仅都要等待一个时间间隔，而且要进入争用窗口，计算随机退避时间以便再次试图接入信道，因此降低了碰撞发生的概率。当且仅当检测到信道空闲且这个数据帧是要发送的第一个数据帧时，才不使用退避算法。其他所有情况都必须使用退避算法，具体为:①在发送第一个帧前检测到信道忙;②每次重传;③每次成功发送后要发送下一帧。</p>
<p>CSMA/CA算法归纳如下:</p>
<p>1）若站点最初有数据要发送（而不是发送不成功再进行重传)，且检测到信道空闲，在等待时间DIFS后，就发送整个数据帧。</p>
<p>2）否则，站点执行CSMA/CA退避算法，选取一个随机回退值。一旦检测到信道忙， $\color{green}{\text{退避计时器}}$ 就保持不变。只要信道空闲，退避计时器就进行倒计时。</p>
<p>3）当退避计时器减到0时（这时信道只可能是空闲的)，站点就发送整个帧并等待确认。</p>
<p>4)发送站若收到确认，就知道已发送的帧被目的站正确接收。这时如果要发送第二帧，就要从步骤2）开始。</p>
<p>若发送站在规定时间内没有收到确认帧ACK（由重传计时器控制)，就必须重传该帧，再次使用CSMA/CA协议争用该信道，直到收到确认，或经过若干次重传失败后放弃发送。</p>
<p>处理隐蔽站问题: $\color{green}{\text{RTS}}$（request to send,rts） 和 $\color{green}{\text{CTS}}$ （clear to send,cts）</p>
<p>在图3.23中，站A和B都在AP的覆盖范围内，但A和B相距较远，彼此都听不见对方。当A和B检测到信道空闲时，都向AP发送数据，导致碰撞的发生，这就是隐蔽站问题。</p>
<p>为了避免该问题，802.11允许发送站对信道进行预约，源站要发送数据帧之前先广播一个所持续的短请求发送 RTS 控制帧，它包括源地址、目的地址和这次通信（含相应的确认帧)时间，该帧能被其范围内包括AP在内的所有站点听到。若信道空闲，则 AP广播一个允许发送CTS 控制帧，它包括这次通信所需的持续时间（从RTS帧复制)，该帧也能被其范围内包括A和B在内的所有站点听到。B和其他站听到CTS后，在CTS帧中指明的时间内将抑制发送，如图3.24所示。CTS帧有两个目的:①给源站明确的发送许可;②指示其他站点在预约期内不要发送。</p>
<p>这种机制实质上是在发送数据帧之前先对信道进行预约。使用RTS和 CTS帧会使网络效率有所下降，但这两种帧都很短，与数据帧相比开销不算大。相反，若不使用这种控制帧，一旦发生碰撞而导致数据帧重发，则浪费的时间更多。因此，协议设有三种情况供用户选择:第一种是使用RTS和 CTS帧;第二种是只有当数据帧的长度超过某一阈值时才使用RTS和CTS帧（显然数据帧本身很短，再使用RTS和CTS帧只能增加开销);第三种是不使用RTS和CTS 帧。</p>
<details><summary>图3.23 A和B同时向AP发送信号，发生碰撞  图3.24 使用RTS和CTS帧的碰撞避免</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004101854.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004101854.png';" /></details>

<p>CSMA/CD与CSMA/CA主要有如下区别:</p>
<p>1）CSMA/CD可以检测冲突，但无法避免;CSMA/CA发送数据的同时不能检测信道上有无冲突，本结点处没有冲突并不意味着在接收结点处就没有冲突，只能尽量避免。</p>
<p>2） $\color{green}{\text{传输介质}}$ 不同。CSMA/CD 用于总线形以太网，CSMA/CA用于无线局域网802.11a/b/g/n等。</p>
<p>3） $\color{green}{\text{检测方式}}$ 不同。CSMA/CD通过电缆中的电压变化来检测;而CSMA/CA采用能量检测、载波检测和能量载波混合检测三种检测信道空闲的方式。</p>
<p>总结:CSMA/CA 协议的基本思想是在发送数据时先广播告知其他结点，让其他结点在某段时间内不要发送数据，以免出现碰撞。CSMA/CD协议的基本思想是发送前侦听，边发送边侦听，一旦出现碰撞马上停止发送。</p>
<h4 id="轮询访问介质访问控制：令牌传递协议"><a href="#轮询访问介质访问控制：令牌传递协议" class="headerlink" title="*轮询访问介质访问控制：令牌传递协议"></a>*轮询访问介质访问控制：令牌传递协议</h4><p>在轮询访问中，用户不能随机地发送信息，而要通过一个集中控制的监控站，以循环方式轮询每个结点，再决定信道的分配。当某结点使用信道时，其他结点都不能使用信道。典型的轮询访问介质访问控制协议是令牌传递协议，它主要用在令牌环局域网中。</p>
<p>在令牌传递协议中，一个令牌在各结点间以某个固定次序交换。令牌是由一组特殊的比特组合而成的帧。当环上的一个站希望传送帧时，必须等待令牌。一旦收到令牌，站点便可启动发送帧。帧中包括目的站的地址，以标识哪个站应接收此帧。帧在环上传送时，不管该帧是否是发给本站点的，所有站点都进行转发，直到该帧回到它的始发站，并由该始发站撤销该帧。帧的目的站除转发帧外，应针对该帧维持一个副本，并通过在帧的尾部设置“响应比特”来指示已收到此副本。站点在发送完一帧后，应释放令牌，以便让其他站使用。</p>
<p>当计算机都不需要发送数据时，令牌就在环形网上游荡，而需要发送数据的计算机只有在拿到该令牌后才能发送数据帧，因此不会发送冲突（因为令牌只有一个)。</p>
<p>在令牌传递网络中，传输介质的物理拓扑不必是一个环，但是为了把对介质访问的许可从一个设备传递到另一个设备，令牌在设备间的传递通路逻辑上必须是一个环。</p>
<p>轮询介质访问控制非常适合负载很高的广播信道。所谓负载很高的信道，是指多个结点在同一时刻发送数据概率很大的信道。可以想象，如果这样的广播信道采用随机介质访问控制，那么发生冲突的概率将会很大，而采用轮询介质访问控制则可以很好地满足各结点间的通信需求。</p>
<p>轮询介质访问控制既不共享时间,也不共享空间，它实际上是在随机介质访问控制的基础上，限定了有权力发送数据的结点只能有一个。</p>
<p>即使是广播信道也可通过介质访问控制机制使广播信道逻辑上变为点对点的信道，所以说数据链路层研究的是“点到点”之间的通信。</p>
<h3 id="局域网"><a href="#局域网" class="headerlink" title="局域网"></a>局域网</h3><h4 id="局域网的基本概念和体系结构"><a href="#局域网的基本概念和体系结构" class="headerlink" title="局域网的基本概念和体系结构"></a>局域网的基本概念和体系结构</h4><p>局域网（Local Area Network，LAN）是指在一个较小的地理范围(如一所学校)内，将各种计算机、外部设备和数据库系统等通过双绞线、同轴电缆等连接介质互相连接起来，组成资源和信息共享的计算机互联网络。主要特点如下:</p>
<p>1)为一个单位所拥有，且地理范围和站点数目均有限。</p>
<p>2）所有站点共享较高的总带宽（即较高的数据传输速率)。</p>
<p>3)较低的时延和较低的误码率。</p>
<p>4)各站为平等关系而非主从关系。</p>
<p>5）能进行广播和组播。</p>
<p>局域网的特性主要由 $\color{red}{\text{三个要素}}$ 决定: $\color{green}{\text{拓扑结构}}$ 、 $\color{green}{\text{传输介质}}$ 、 $\color{green}{\text{介质访问控制方式}}$ ,其中最重要的是介质访问控制方式，它决定着局域网的技术特性。</p>
<p>常见的局域网拓扑结构主要有以下 $\color{red}{\text{4大类}}$ : $\color{green}{\text{①星形结构}}$ ; $\color{green}{\text{②环形结构}}$ ; $\color{green}{\text{③总线形结构}}$ ; $\color{green}{\text{④星形和总线形结合的复合型结构}}$ 。</p>
<p>局域网可以使用 $\color{green}{\text{双绞线}}$ 、 $\color{green}{\text{铜缆}}$ 和 $\color{green}{\text{光纤}}$ 等多种 $\color{red}{\text{传输介质}}$ ，其中双绞线为主流传输介质。</p>
<p>局域网的 $\color{red}{\text{介质访问控制方法}}$ 主要有 $\color{green}{\text{CSMA/CD}}$ 、 $\color{green}{\text{令牌总线}}$ 和 $\color{green}{\text{令牌环}}$ ，其中前两种方法主要用于总线形局域网，令牌环主要用于环形局域网。</p>
<p>三种特殊的局域网拓扑实现如下:</p>
<ul>
<li> $\color{green}{\text{以太网}}$ (目前使用范围最广的局域网)。 $\color{red}{\text{逻辑拓扑}}$ 是 $\color{green}{\text{总线形}}$ 结构， $\color{red}{\text{物理拓扑}}$ 是 $\color{green}{\text{星形}}$ 或拓展星形结构。</li>
<li> $\color{green}{\text{令牌环}}$ （Token Ring，IEEE 802.5)。逻辑拓扑是环形结构，物理拓扑是 $\color{green}{\text{星形结构}}$ 。</li>
<li> $\color{green}{\text{FDDI}}$ (光纤分布数字接口，IEEE 802.8)。逻辑拓扑是环形结构，物理拓扑是 $\color{green}{\text{双环结构}}$ 。</li>
</ul>
<p>IEEE 802标准定义的局域网参考模型只对应于OSI参考模型的 $\color{green}{\text{数据链路层}}$ 和 $\color{green}{\text{物理层}}$ ,并将数据链路层拆分为两个子层: $\color{green}{\text{逻辑链路控制}}$ （LLC)子层和 $\color{green}{\text{媒体接入控制}}$ (MAC)子层。与接入传输媒体有关的内容都放在MAC子层，它向上层屏蔽对物理层访问的各种差异，提供对物理层的统一访问接口，主要功能包括: $\color{green}{\text{组帧}}$ 和 $\color{green}{\text{拆卸帧}}$ 、比特传输 $\color{green}{\text{差错检测}}$ 、 $\color{green}{\text{透明传输}}$ 。LLC子层与传输媒体无关，它向网络层提供无确认无连接、面向连接、带确认无连接、高速传送4种不同的连接服务类型。</p>
<p>由于以太网在局域网市场中取得垄断地位，几乎成为局域网的代名词，而802委员会制定的LLC子层作用已经不大，因此现在许多网卡仅装有MAC协议而没有LLC协议。IEEE 802协议层与OSI参考模型的比较如图3.25所示。</p>
<p>需要提醒读者的是，局域网的各类协议和广域网的各类协议也是统考的重点，容易出选择题，需要大家认真记忆。</p>
<details><summary>图3.25 IEEE 802协议层与OSI模型的比较</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004102806.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004102806.png';" /></details>

<h4 id="以太网与IEEE-802-3"><a href="#以太网与IEEE-802-3" class="headerlink" title="以太网与IEEE 802.3"></a>以太网与IEEE 802.3</h4><p>IEEE 802.3标准是一种基带总线形的局域网标准，它描述物理层和数据链路层的MAC子层的实现方法。随着技术的发展，该标准又有了大量的补充与更新，以支持更多的传输介质和更高的传输速率。</p>
<p>以太网逻辑上采用总线形拓扑结构，以太网中的所有计算机共享同一条总线，信息以广播方式发送。为了保证数据通信的方便性和可靠性，以太网简化了通信流程并使用了CSMA/CD方式对总线进行访问控制。</p>
<p>严格来说，以太网应当是指符合DIX Ethernet V2标准的局域网，但 DIX Ethernet V2标准与IEEE 802.3标准只有很小的差别，因此通常将802.3局域网简称为 $\color{green}{\text{以太网}}$ 。</p>
<p>以太网采用两项措施以简化通信:①采用无连接的工作方式，不对发送的数据帧编号，也不要求接收方发送确认，即以太网尽最大努力交付数据，提供的是不可靠服务，对于差错的纠正则由高层完成;②发送的数据都使用曼彻斯特编码的信号，每个码元的中间出现一次电压转换，接收端利用这种电压转换方便地把位同步信号提取出来。</p>
<h5 id="以太网的传输介质与网卡"><a href="#以太网的传输介质与网卡" class="headerlink" title="以太网的传输介质与网卡"></a>以太网的传输介质与网卡</h5><p>以太网常用的传输介质有4种:粗缆、细缆、双绞线和光纤。各种传输介质的适用情况见表3.2。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004103035.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004103035.png';" /></details>

<blockquote>
<p>注意:10BASE-T非屏蔽双绞线以太网拓扑结构为星形网，星形网中心为集线器，但使用集线器的以太网在逻辑上仍然是一个总线网，属于一个冲突域。上表的内容是常识，例如题目中出现10BASE5时，是不会显式地告诉你它的传输媒体、编码方式、拓扑结构等信息的。</p>
</blockquote>
<p>计算机与外界局域网的连接是通过主机箱内插入的一块网络接口板「又称 $\color{green}{\text{网络适配器(Adapter)}}$ 或 $\color{green}{\text{网络接口卡(Network Interface Card，NIC)}}$ 」实现的。网卡上装有处理器和存储器，是工作在数据链路层的网络组件。网卡和局域网的通信是通过电缆或双绞线以串行方式进行的，而网卡和计算机的通信则是通过计算机主板上的IO总线以并行方式进行的。因此，网卡的重要功能就是进行数据的串并转换。网卡不仅能实现与局域网传输介质之间的物理连接和电信号匹配，还涉及帧的发送与接收、帧的封装与拆封、介质访问控制、数据的编码与解码及数据缓存功能等。</p>
<p>全世界的每块网卡在出厂时都有一个唯一的代码，称为介质访问控制(MAC)地址，这个地址用于控制主机在网络上的数据通信。数据链路层设备(网桥、交换机等）都使用各个网卡的MAC地址。另外，网卡控制着主机对介质的访问，因此网卡也工作在物理层，因为它只关注比特，而不关注任何地址信息和高层协议信息。</p>
<h5 id="以太网的MAC帧"><a href="#以太网的MAC帧" class="headerlink" title="以太网的MAC帧"></a>以太网的MAC帧</h5><p>每块网卡中的MAC地址也称物理地址;MAC地址长6字节，一般用由连字符(或冒号)分隔的12个十六进制数表示，如 02-60-8c-e4-b1-21。高24位为厂商代码，低24位为厂商自行分配的网卡序列号。严格来讲，局域网的“地址”应是每个站的“名字”或标识符。</p>
<p>由于总线上使用的是广播通信，因此网卡从网络上每收到一个MAC帧，首先要用硬件检查MAC帧中的 MAC地址。如果是发往本站的帧，那么就收下，否则丢弃。</p>
<p>以太网MAC帧格式有两种标准:DIX Ethernet V2标准（即以太网V2标准）和IEEE 802.3标准。这里先介绍最常用的以太网V2的 MAC帧格式，如图3.26所示。</p>
<p>$\color{green}{\text{前导码}}$ :使接收端与发送端时钟同步。在帧前面插入的8字节可再分为两个字段:第一个字段共7字节，是前同步码，用来快速实现MAC帧的比特同步;第二个字段是帧开始定界符，表示后面的信息就是MAC帧。</p>
<details><summary>图3.26以太网V2标准的MAC帧格式</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004103430.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004103430.png';" /></details>

<blockquote>
<p>注意:MAC 帧并不需要帧结束符，因为以太网在传送帧时，各帧之间必须有一定的间隙。因此，接收端只要找到帧开始定界符，其后面连续到达的比特流就都属于同一个MAC 帧,所以图3.26只有帧开始定界符。但不要误以为以太网MAC帧不需要尾部，在数据链路层上，帧既要加首部，也要加尾部。</p>
</blockquote>
<p>$\color{green}{\text{地址}}$ :通常使用6字节(48bit）地址(MAC地址)。</p>
<p>$\color{green}{\text{类型}}$ :2字节，指出数据域中携带的数据应交给哪个协议实体处理。</p>
<p>$\color{green}{\text{数据}}$ :46～1500字节，包含高层的协议消息。由于CSMA/CD算法的限制，以太网帧必须满足最小长度要求64字节，数据较少时必须加以填充（0～46字节)。</p>
<blockquote>
<p>注意:46和1500是怎么来的?首先，由CSMA/CD算法可知以太网帧的最短帧长为64B，而MAC帧的首部和尾部的长度为18字节，所以数据字段最短为64-18=46字节。其次，最大的1500字节是规定的，没有为什么。</p>
</blockquote>
<p>$\color{green}{\text{填充}}$ :0～46字节，当帧长太短时填充帧，使之达到64字节的最小长度。</p>
<p>$\color{green}{\text{校验码（FCS）}}$ :4字节，校验范围从目的地址段到数据段的末尾，算法采用32位循环冗余码(CRC)，不但需要检验MAC帧的数据部分，还要检验目的地址、源地址和类型字段，但不校验前导码。</p>
<p>802.3帧格式与DIX以太帧格式的不同之处在于用长度域替代了DIX帧中的类型域，指出数据域的长度。在实践中，前述长度/类型两种机制可以并存，由于IEEE 802.3数据段的最大字节数是1500，所以长度段的最大值是1500，因此从1501到65535的值可用于类型段标识符。</p>
<h5 id="高速以太网"><a href="#高速以太网" class="headerlink" title="高速以太网"></a>高速以太网</h5><p>速率达到或超过100Mb/s的以太网称为高速以太网。</p>
<h6 id="color-green-text-100BASE-T以太网"><a href="#color-green-text-100BASE-T以太网" class="headerlink" title="$\color{green}{\text{100BASE-T以太网}}$"></a>$\color{green}{\text{100BASE-T以太网}}$</h6><p>100BASE-T以太网是在双绞线上传送100Mb/s基带信号的星形拓扑结构以太网，它使用CSMA/CD 协议。这种以太网既支持全双工方式，又支持半双工方式，可在全双工方式下工作而无冲突发生。因此，在全双工方式下不使用CSMA/CD协议。</p>
<p>MAC帧格式仍然是802.3标准规定的。保持最短帧长不变，但将一个网段的最大电缆长度减小到100m。帧间时间间隔从原来的9.6us改为现在的0.96us。</p>
<h6 id="color-green-text-吉比特以太网"><a href="#color-green-text-吉比特以太网" class="headerlink" title="$\color{green}{\text{吉比特以太网}}$"></a>$\color{green}{\text{吉比特以太网}}$</h6><p>吉比特以太网又称千兆以太网，允许在1Gb/s下用全双工和半双工两种方式工作。使用802.3协议规定的帧格式。在半双工方式下使用CSMA/CD协议（全双工方式不需要使用CSMA/CD协议)。与10BASE-T和100BASE-T技术向后兼容。</p>
<h6 id="10吉比特以太网"><a href="#10吉比特以太网" class="headerlink" title="10吉比特以太网"></a>10吉比特以太网</h6><p>10吉比特以太网与10Mb/s、100Mb/s和 1Gb/s以太网的帧格式完全相同。10吉比特以太网还保留了802.3标准规定的以太网最小和最大帧长，便于升级。10吉比特以太网不再使用铜线而只使用光纤作为传输媒体。10吉比特以太网只工作在 $\color{green}{\text{全双工}}$ 方式，因此没有争用问题，也不使用CSMA/CD 协议。</p>
<p>以太网从10Mb/s到10Gb/s的演进证明了以太网是可扩展的(从10Mb/s到10Gb/s)、灵活的(多种传输媒体、全/半双工、共享/交换)，易于安装，稳健性好。</p>
<h4 id="IEEE-802-11"><a href="#IEEE-802-11" class="headerlink" title="IEEE 802.11"></a>IEEE 802.11</h4><p>无线局域网可分为两大类:有固定基础设施无线局域网和无固定基础设施移动自组织网络。所谓“固定基础设施”，是指预先建立的、能覆盖一定地理范围的固定基站。</p>
<h5 id="有固定基础设施无线局域网"><a href="#有固定基础设施无线局域网" class="headerlink" title="有固定基础设施无线局域网"></a>有固定基础设施无线局域网</h5><p>对于有固定基础设施的无线局域网，IEEE制定了无线局域网的802.11系列协议标准，包括802.11a/b/g/n等。802.11使用星形拓扑，其中心称为 $\color{green}{\text{接入点}}$ (Access Point，AP)，在MAC层使用CSMA/CA协议。使用802.11系列协议的局域网又称Wi-Fi。</p>
<p>802.11标准规定无线局域网的最小构件是 $\color{green}{\text{基本服务集BSS}}$  (Basic Service Set，BSS)。一个基本服务集包括一个基站和若干移动站。所有的站在本 BSS内都可以直接通信，但在和本 BSS 外的站通信时都必须通过本BSS的基站。上面提到的AP就是基本服务集中的基站(base station)。安装AP时，必须为该AP分配一个不超过32字节的 $\color{green}{\text{服务集标识符}}$ (Service Set IDentifier，SSID)和一个信道。一个基本服务集覆盖的地理范围称为一个 $\color{green}{\text{基本服务区}}$ （Basic Service Area，BSA),无线局域网的基本服务区的直径一般不超过100m。</p>
<p>一个基本服务集可以是孤立的，也可通过AP连接到一个 $\color{green}{\text{分配系统}}$ （Distribution System，DS),然后连接到另一个基本服务集，构成一个 $\color{green}{\text{扩展的服务集}}$ (Extended Service Set，ESS)，ESS还可以通过一种称为Portal (门桥）的设备为无线用户提供到有线连接的以太网的接入。门桥的作用相当于一个网桥。在图3.27中，移动站A如果要和另一个基本服务集中的移动站B通信，就必须经过两个接入点 $AP_1$ 和  $AP_2$ ，即 A $\to$  $AP_1$   $\to$  $AP_2$  $\to$ B，注意 $AP_1$ 到 $AP_2$ 的通信是使用有线传输的。</p>
<details><summary>图3.27基本服务集和扩展服务集</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004105112.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004105112.png';" /></details>

<p>移动站A从某个基本服务集漫游到另一个基本服务集时(图3.27中的A’)，仍然可保持与另一个移动站B的通信。但A在不同的基本服务集使用的AP改变了。</p>
<h5 id="无固定基础设施移动自组织网络"><a href="#无固定基础设施移动自组织网络" class="headerlink" title="无固定基础设施移动自组织网络"></a>无固定基础设施移动自组织网络</h5><p>另一种无线局域网是无固定基础设施的无线局域网，又称自组网络(ad hoc network )。自组网络没有上述基本服务集中的 AP，而是由一些平等状态的移动站相互通信组成的临时网络（见图3.28)。各结点之间地位平等，中间结点都为转发结点，因此都具有路由器的功能。</p>
<p>自组网络通常是这样构成的:一些可移动设备发现在它们附近还有其他的可移动设备，并且要求和其他移动设备进行通信。自组网络中的每个移动站都要参与网络中其他移动站的路由的发现和维护，同时由移动站构成的网络拓扑可能随时间变化得很快，因此在固定网络中行之有效的一些路由选择协议对移动自组网络已不适用，需引起特别的关注。</p>
<details><summary>图3.28 由一些处于平等状态的便携机构成的自组网络</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004105232.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004105232.png';" /></details>

<p>自组网络和移动IP并不相同。移动IP技术使漫游的主机可以用多种方法连接到因特网，其核心网络功能仍然是基于固定网络中一直使用的各种路由选择协议。而自组网络是把移动性扩展到无线领域中的自治系统，具有自己特定的路由选择协议，并且可以不和因特网相连。</p>
<h4 id="令牌环网的基本原理"><a href="#令牌环网的基本原理" class="headerlink" title="令牌环网的基本原理"></a>令牌环网的基本原理</h4><p>令牌环网的基本原理如图3.29所示。令牌环网的每一站通过电缆与环接口干线耦合器(TCU)相连。TCU的主要作用是，传递所有经过的帧，为接入站发送和接收数据提供接口。与此对应，TCU的状态也有两个:收听状态和发送状态。数据总是在某个特定的方向上从一个TCU到下一个TCU逐比特地依次传送，每个TCU重新产生并重新传输每一比特。</p>
<details><summary>图3.29令牌环网的基本原理</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004105346.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004105346.png';" /></details>

<p>令牌环网的媒体接入控制机制采用的是分布式控制模式的循环方法。在令牌环网中有一个令牌(Token）沿着环形总线在入网结点计算机间依次传递，令牌实际上是一个特殊格式的MAC控制帧，它本身并不包含信息，仅控制信道的使用，确保在同一时刻只有一个结点能够独占信道。站点只有取得令牌后才能发送数据帧，因此令牌环网不会发生碰撞。由于令牌在网环上是按顺序依次传递的，因此对所有入网计算机而言，访问权是公平的。</p>
<p>令牌环网中令牌和数据的传递过程如下:</p>
<p>1）网络空闲时，环路中只有令牌帧在循环传递。</p>
<p>2）令牌传递到有数据要发送的站点处时，该站点就修改令牌中的一个标志位，并在令牌中附加自己需要传输的数据，将令牌变成一个数据帧，然后将这个数据帧发送出去。</p>
<p>3）数据帧沿着环路传输，接收到的站点一边转发数据，一边查看帧的目的地址。如果目的地址和自己的地址相同，那么接收站就复制该数据帧以便进一步处理。</p>
<p>4）数据帧沿着环路传输，直到到达该帧的源站点，源站点接收到自己发出去的数据帧后便不再进行转发。同时，发送方可以通过检验返回的数据帧来查看数据传输过程中是否有错，若有错则重传该帧。</p>
<p>5）源站点传送完数据后，重新产生一个令牌，并将令牌传递给下一个站点，以交出对媒体<br>的访问权限。</p>
<p>令牌环网在物理上采用星形拓扑结构，但逻辑上仍是环形拓扑结构，其标准由IEEE 802.5定义。图3.29中的环并不是由通信线路连接而成的物理环。</p>
<h3 id="广域网"><a href="#广域网" class="headerlink" title="广域网"></a>广域网</h3><h4 id="广域网的基本概念"><a href="#广域网的基本概念" class="headerlink" title="广域网的基本概念"></a>广域网的基本概念</h4><p>广域网通常是指覆盖范围很广（远超一个城市的范围）的长距离网络。广域网是因特网的核心部分，其任务是长距离运送主机所发送的数据。连接广域网各结点交换机的链路都是高速链路，它可以长达几千千米的光缆线路，也可以是长达几万千米的点对点卫星链路。因此广域网首要考虑的问题是通信容量必须足够大，以便支持日益增长的通信量。</p>
<p>广域网不等于互联网，互联网可以连接不同类型的网络（既可以连接 $\color{green}{\text{局域网}}$ ，又可以连接 $\color{green}{\text{广域网}}$ )，通常使用 $\color{green}{\text{路由器}}$ 来连接。图3.30显示了由相距较远的局域网通过路由器与厂域网相连而成的一个覆盖范围很广的互联网。因此，局域网可以通过广域网与另一个相隔很远的局域网通信。</p>
<details><summary>图3.30由局域网和广域网组成的互联网</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004105547.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004105547.png';" /></details>

<p>从层次上考虑，广域网和局域网的区别很大，因为局域网使用的协议主要在数据链路层（还有少量在物理层)，而广域网使用的协议主要在网络层。怎么理解“局域网使用的协议主要在数据链路层，而广域网使用的协议主要在网络层”这句话呢？如果网络中的两个结点要进行数据交换，那么结点除要给出数据外，还要给数据“包装”上一层控制信息，用于实现检错纠错等功能。如果这层控制信息是数据链路层协议的控制信息，那么就称使用了数据链路层协议，如果这层控制信息是网络层的控制信息，那么就称使用了网络层协议。</p>
<p>它们的区别与联系见表3.3。</p>
<details><summary>表3.3广域网和局域网的区别与联系</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004105653.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004105653.png';" /></details>

<p>广域网中的一个重要问题是路由选择和分组转发。路由选择协议负责搜索分组从某个结点到目的结点的最佳传输路由，以便构造路由表，然后从路由表再构造出转发分组的转发表。分组是通过转发表进行转发的。</p>
<p>PPP协议和HDLC 协议是目前最常用的两种广域网数据链路层控制协议。</p>
<h4 id="PPP协议"><a href="#PPP协议" class="headerlink" title="PPP协议"></a>PPP协议</h4><p>PPP (Point-to-Point Protocol)是使用串行线路通信的面向字节的协议，该协议应用在直接连接两个结点的链路上。设计的目的主要是用来通过拨号或专线方式建立点对点连接发送数据，使其成为各种主机、网桥和路由器之间简单连接的一种共同的解决方案。</p>
<p>PPP协议是在 $\color{green}{\text{SLIP}}$  协议的基础上发展而来的，它既可以在异步线路上传输，又可在同步线路上使用;不仅用于Modem链路，也用于租用的路由器到路由器的线路。</p>
<p>背景:SLIP主要完成数据报的传送，但没有寻址、数据检验、分组类型识别和数据压缩等功能，只能传送IP分组。如果上层不是IP协议，那么无法传输，并且此协议对一些高层应用也不支持，但实现比较简单。为了改进SLIP 的缺点，于是制定了点对点协议（PPP)。</p>
<p>PPP协议有三个组成部分:</p>
<p>1） $\color{green}{\text{链路控制协议}}$ (LCP)。一种扩展链路控制协议，用于建立、配置、测试和管理数据链路。</p>
<p>2） $\color{green}{\text{网络控制协议}}$ (NCP)。PPP协议允许同时采用多种网络层协议，每个不同的网络层协议要用一个相应的NCP来配置，为网络层协议建立和配置逻辑连接。</p>
<p>3）一个将IP数据报封装到串行链路的方法。IP数据报在PPP帧中就是其信息部分，这个信息部分的长度受最大传送单元（MTU）的限制。</p>
<p>PPP帧的格式如图3.31所示。PPP帧的前3个字段和最后2个字段与HDLC帧是一样的， $\color{green}{\text{标志字段}}$ (F)仍为7E(01111110)，前后各占1字节，若它出现在信息字段中，就必须做 $\color{green}{\text{字节填充}}$ ，使用的控制转义字节是7D（01111101)。但在PPP中， $\color{green}{\text{地址字段(A)}}$ 占1字节，规定为0xFF， $\color{green}{\text{控制字段(C)}}$ 占1字节，规定为0x03，两者的内容始终是固定不变的。PPP是面向字节的，因而所有PPP帧的长度都是整数个字节。</p>
<details><summary>图3.31 PPP帧的格式</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004105913.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004105913.png';" /></details>

<p>第4个字段是 $\color{green}{\text{协议段}}$ ，占2字节，在HDLC中没有该字段，它是说明信息段中运载的是什么种类的分组。以比特0开始的是诸如IP、IPX和AppleTalk 这样的网络层协议;以比特1开始的被用来协商其他协议，包括LCP及每个支持的网络层协议的一个不同的NCP。</p>
<p>第5段 $\color{green}{\text{信息段}}$ 的长度是可变的，大于等于0且小于等于1500B。为了实现透明传输，当信息段中出现和标志字段一样的比特组合时，必须采用一些措施来改进。</p>
<blockquote>
<p>注意:因为PPP是点对点的，并不是总线形，所以无须采用CSMA/CD协议，自然就没有最短帧，所以信息段占0<del>1500字节，而不是46</del>1500字节。另外，当数据部分出现和标志位一样的比特组合时,就需要采用一些措施来实现透明传输。</p>
</blockquote>
<p>第6个字段是 $\color{green}{\text{帧检验序列}}$ (FCS,Frame Check Sequence)，占2字节，即循环冗余码检验中的冗余码。检验区包括地址字段、控制字段、协议字段和信息字段。</p>
<p>图3.32给出了PPP链路建立、使用、撤销所经历的状态图。当线路处于静止状态时，不存在物理层连接。当线路检测到载波信号时，建立物理连接，线路变为建立状态。此时，LCP开始选项商定，商定成功后就进入身份验证状态。双发身份验证通过后，进入网络状态。这时，采用NCP配置网络层，配置成功后，进入打开状态，然后就可进行数据传输。当数据传输完成后，线路转为终止状态。载波停止后则回到静止状态。</p>
<details><summary>图3.32 PPP协议的状态图</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004110109.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004110109.png';" /></details>

<p>注意:</p>
<ol>
<li>PPP提供差错检测但不提供纠错功能，只保证无差错接收（通过硬件进行CRC校验)它是不可靠的传输协议，因此也不使用序号和确认机制。</li>
</ol>
<p>2）它仅支持 $\color{green}{\text{点对点}}$ 的链路通信，不支持多点线路。</p>
<p>3 ) PPP只支持 $\color{green}{\text{全双工}}$ 链路。</p>
<p>4）PPP的两端可以运行不同的网络层协议，但仍然可使用同一个PPP进行通信。</p>
<p>5）PPP是面向字节的，当信息字段出现和标志字段一致的比特组合时，PPP有两种不同的处理方法:若PPP用在异步线路（默认)，则采用 $\color{green}{\text{字节填充法}}$ ;若 PPP用在SONET/SDH等同步线路，则协议规定采用硬件来完成 $\color{green}{\text{比特填充}}$ （和HDLC的做法一样)。</p>
<h4 id="HDLC协议"><a href="#HDLC协议" class="headerlink" title="*HDLC协议"></a>*HDLC协议</h4><p>高级数据链路控制（High-level Data Link Control，HDLC）协议是ISO制定的面向比特（记住PPP协议是面向字节的）的数据链路层协议。该以以个依簇了HT个T丁通信。有较高的数据可透明传输，用于实现透明传输的“0比特插入法”易于硬件实现;全双工通信，有较高的数据链路传输效率;所有帧采用CRC检验，对信息帧进行顺序编号，可防止漏收或重发，传输可靠性高;传输控制功能与处理功能分离，具有较大的灵活性。</p>
<p>HDLC适用于链路的两种基本配置:非平衡配置和平衡配置。</p>
<p>1）非平衡配置的特点是由一个主站控制整个链路的工作。</p>
<p>2）平衡配置的特点是链路两端的两个站都是复合站，每个复合站都可以平等地发起数据传输，而不需要得到对方复合站的允许。</p>
<h5 id="站"><a href="#站" class="headerlink" title="站"></a>站</h5><p>HDLC有3种站类型:主站、从站和复合站。主站负责控制链路的操作，主站发出的帧称为命令帧。从站受控于主站，按主站的命令进行操作;发出的帧称为响应帧。另外，有些站既具有主站的功能，又具有从站的功能，所以这类站称为复合站，它可以发出命令帧和响应帧。</p>
<h5 id="数据操作方式"><a href="#数据操作方式" class="headerlink" title="数据操作方式"></a>数据操作方式</h5><p>HDLC有3种数据操作方式:</p>
<p>1） $\color{green}{\text{正常响应方式}}$ 。这是一种非平衡结构操作方式，即主站向从站传输数据，从站响应传输但从站只有在收到主站的许可后，才可进行响应。</p>
<p>2） $\color{green}{\text{异步平衡方式}}$ 。这是一种平衡结构操作方式。在这种方式中，每个复合站都可以进行对另一站的数据传输。</p>
<p>3） $\color{green}{\text{异步响应方式}}$ 。这是一种非平衡结构操作方式。在这种方式中，从站即使未受到主站的允许，也可进行传输。</p>
<h5 id="HDLC帧"><a href="#HDLC帧" class="headerlink" title="HDLC帧"></a>HDLC帧</h5><p>图3.33所示为HDLC的帧格式，它由标志、地址、控制、信息和帧校验序列(FCS）等字段构成。</p>
<details><summary>图3.33 HDLC的帧格式</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004110413.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004110413.png';" /></details>

<p>标志字段F，为01111110。在接收端只要找到标志字段就可确定一个帧的位置。HDLC 协议采用比特填充的首尾标志法实现透明传输。在发送端，当一串比特流数据中有5个连续的1时，就立即在其后填入一个0。接收帧时，先找到F字段以确定帧的边界，接着对比特流进行扫描。每当发现5个连续的1时，就将其后的一个0删除，以还原成原来的比特流。</p>
<p>地址字段A，共8位，在使用非平衡方式传送数据时，站地址字段总是写入从站的地址;在使用平衡方式传送数据时，站地址字段填入的是应答站的地址。</p>
<p>控制字段C，共8位，是最复杂的字段。HDLC的许多重要功能都靠控制字段来实现。根据其第1位或第1、2位的取值，可将HDLC 帧划分为三类:</p>
<p>1）信息帧(I)，第1位为0，用来传输数据信息，或使用捎带技术对数据进行确认。</p>
<p>2）监督帧(S)，第1、2位分别为1、0，用于流量控制和差错控制，执行对信息帧的确认、请求重发和请求暂停发送等功能;</p>
<p>3）无编号帧(U)，第1、2位均为1，用于提供对链路的建立、拆除等多种控制功能。由图3.31和图3.33可知，PPP帧和 HDLC帧的格式很相似。但两者有以下几点不同:</p>
<p>由图3.31和图3.33可知，PPP帧和HDLC帧的格式很相似。但两者有以下几点不同:</p>
<p>1）PPP协议是面向字节的，HDLC协议是面向比特的。</p>
<p>2）PPP帧比HDLC帧多一个2字节的协议字段。当协议字段值为0x0021时，表示信息字段<br>是P数据报。</p>
<p>3）PPP协议不使用序号和确认机制，只保证无差错接收（通过硬件进行CRC检验)，而端<br>到端差错检测由高层协议负责。HDLC 协议的信息帧使用了编号和确认机制，能够提供可靠传输。</p>
<h3 id="数据链路层设备"><a href="#数据链路层设备" class="headerlink" title="数据链路层设备"></a>数据链路层设备</h3><h4 id="网桥的概念及其基本原理"><a href="#网桥的概念及其基本原理" class="headerlink" title="*网桥的概念及其基本原理"></a>*网桥的概念及其基本原理</h4><p>两个或多个以太网通过网桥连接后，就成为一个覆盖范围更大的以太网，而原来的每个以太网就称为一个网段。网桥工作在链路层的MAC子层，可以使以太网各网段成为隔离开的碰撞域。如果把网桥换成工作在物理层的转发器，那么就没有这种过滤通信量的功能。由于各网段相对独立，因此一个网段的故障不会影响到另一个网段的运行。</p>
<blockquote>
<p>注意:网桥处理数据的对象是帧，所以它是工作在数据链路层的设备，中继器、放大器处理数据的对象是信号,所以它是工作在物理层的设备。</p>
</blockquote>
<p>网络1和网络2通过网桥连接后，网桥接收网络1发送的数据帧，检查数据帧中的地址，如果是网络2的地址，那么就转发给网络2;如果是网络1的地址，那么就将其丢弃，因为源站和目的站处在同一个网段，目的站能够直接收到这个帧而不需要借助网桥转发。</p>
<p>如图3.34所示，设每个网段的数据率都是10Mb/s，那么三个网段合起来的最大吞吐量就变成了30Mb/s。如果把两个网桥换成集线器或转发器,那么整个网络仍然是一个碰撞域(即冲突域)，当A和B通信时，所有其他站点都不能通信，整个碰撞域的最大吞吐量仍然是10Mb/s。</p>
<p>网桥的基本特点:①网桥必须具备寻址和路径选择能力，以确定帧的传输方向;②从源网络接收帧，以目的网络的介质访问控制协议向目的网络转发该帧;③网桥在不同或相同类型的LAN之间存储并转发帧，必要时还进行链路层上的协议转换。注意，一股情况下，仔T投反尖仅奋排能进行协议转换，即连接的两个网段可以使用不同的协议;④网桥对接收到的帧个似仕何修以，或只对帧的封装格式做很少的修改;⑤网桥可以通过执行帧翻译互联不同类型的局域网，即把原协议的信息段的内容作为另一种协议的信息部分封装在帧中;⑥网桥应有足够大的缓冲空间，因为在短时间内帧的到达速率可能高于转发速率。</p>
<details><summary>图3.34网桥使各网段成为隔离开的碰撞域</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004111006.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004111006.png';" /></details>

<p>网桥的优点:①能过滤通信量;②扩大了物理范围;③可使用不同的物理层;④可互联不同类型的局域网;⑤提高了可靠性;⑥性能得到改善。</p>
<p>网桥的缺点:①增大了时延;②MAC子层没有流量控制功能（流量控制需要用到编号机制,编号机制的实现在LLC子层);③不同MAC子层的网段桥接在一起时，需要进行帧格式的转换;④网桥只适合于用户数不多和通信量不大的局域网，否则有时还会因传播过多的广播信息而产生网络拥塞，这就是所谓的广播风暴。</p>
<p>网桥必须具有路径选择的功能，接收到帧后，要决定正确的路径，将该帧转送到相应的目的局域网站点。根据路径选择算法的不同，可将网桥分为透明网桥和源路由网桥。</p>
<h5 id="透明网桥（选择的不是最佳路由"><a href="#透明网桥（选择的不是最佳路由" class="headerlink" title="透明网桥（选择的不是最佳路由)"></a>透明网桥（选择的不是最佳路由)</h5><p>透明网桥以混杂方式工作，它接收与之连接的所有LAN传送的每一帧。到达帧的路由选择过程取决于源LAN和目的LAN:①如果源LAN和目的LAN相同,那么丢弃该帧;②如果源LAN和目的LAN不同，那么转发该帧;③如果目的LAN未知，那么扩散该帧。</p>
<p>当网桥刚连接到以太网时，其转发表是空的，网桥按照自学习算法处理收到的帧。该算法的基本思想是:若从站A发出的帧从某端口进入网桥，那么从这个端口出发沿相反方向一定可把一个帧传送到站A。所以网桥每收到一个帧，就记下其源地址和进入网桥的端口，作为转发表中的一个项目（源地址、进入的接口和时间)。在建立转发表时，把帧首部中的源地址写在“地址”一栏的下面。在转发帧时，则根据收到的帧首部中的目的地址来转发。这时就把在“地址”栏下面已经记下的源地址当作目的地址，而把记下的进入端口当作转发端口。网桥就是在这样的转发过程中逐渐将其转发表建立起来的。</p>
<p>为了避免转发的帧在网络中不断地“兜圈子”，透明网桥使用了一种生成树算法（无环)，以确保每个源到每个目的地只有唯一的路径。生成树使得整个扩展局域网在逻辑上形成树形结构，所以工作时逻辑上没有环路，但生成树一般不是最佳路由。</p>
<h5 id="源路由网桥（选择的是最佳路由"><a href="#源路由网桥（选择的是最佳路由" class="headerlink" title="源路由网桥（选择的是最佳路由)"></a>源路由网桥（选择的是最佳路由)</h5><p>在源路由网桥中，路由选择由发送数据帧的源站负责，网桥只根据数据真正的路由信息对帧进行接收和转发。</p>
<p>源路由网桥对主机是不透明的，主机必须知道网桥的亦佐o连按到哪个网权上。p出远开口发送帧的源站负责，那么源站如何知道应当选择什么样的路由呢?为了找到取住的路出，源项女广播方式向目的站发送一个发现帧（Discovery Frame）作为探测之用。源路出的T从过任Ni人未知路径前，源站要先发送一个发现帧;途中的每个网桥都转发此明，取资该尺境队中从少白途径到达目的站;目的站也将一一发送应答帧;每个应答帧将逋过原路在伛凹，迷红的内.2r1LH己的标志记录在应答帧中;源站选择出一个最佳路由。以后，凡从这个源站向该目的站发送的帧的首部，都必须携带这一路由信息。</p>
<p>此外，发送帧还可以帮助源站确定整个网络可以通过的帧的最大长度。由于发现帧的数量指数式增加，可能会使网络严重拥塞。</p>
<h5 id="两种网桥的比较"><a href="#两种网桥的比较" class="headerlink" title="两种网桥的比较"></a>两种网桥的比较</h5><p>使用源路由网桥可以利用最佳路由。若在两个以太网之间使用并联的源路由网桥，则还可使通信量较平均地分配给每个网桥。采用透明网桥时，只能使用生成树，而使用生成树一般并不能保证所用的路由是最佳的，也不能在不同的链路中进行负载均衡。</p>
<blockquote>
<p>注意:透明网桥和源路由网桥中提到的最佳路由并不是经过路由器最少的路由，而可以是发送帧往返时间最短的路由，这样才能真正地进行负载平衡，因为往返时间长说明中间某个路由器可能超载了，所以不走这条路，换个往返时间短的路走。</p>
</blockquote>
<h4 id="局域网交换机及其工作原理"><a href="#局域网交换机及其工作原理" class="headerlink" title="局域网交换机及其工作原理"></a>局域网交换机及其工作原理</h4><h5 id="局域网交换机"><a href="#局域网交换机" class="headerlink" title="局域网交换机"></a>局域网交换机</h5><p>桥接器的主要限制是在任一时刻通常只能执行一个帧的转发操作，于是出现了局域网交换机，又称以太网交换机。从本质上说，以太网交换机是一个多端口的网桥，它工作在数据链路层。交换机能经济地将网络分成小的冲突域，为每个工作站提供更高的带宽。</p>
<p>以太网交换机对工作站是透明的，因此管理开销低廉，简化了网络结点的增加、移动和网络变化的操作。利用以太网交换机还可以方便地实现虚拟局域网（Virtual LAN，VLAN)， $\color{red}{\text{VLAN}}$ 不仅可以隔离 $\color{green}{\text{冲突域}}$ ，而且可以隔离 $\color{green}{\text{广播域}}$ 。</p>
<h5 id="原理"><a href="#原理" class="headerlink" title="原理"></a>原理</h5><p>以太网交换机的原理是，它检测从以太端口来的数据帧的源和目的地的MAC(介质访问层)地址，然后与系统内部的动态查找表进行比较，若数据帧的MAC地址不在查找表中，则将该地址加入查找表，并将数据帧发送给相应的目的端口。</p>
<h5 id="特点"><a href="#特点" class="headerlink" title="特点"></a>特点</h5><p>以太网交换机的特点如下:</p>
<p>1）以太网交换机的每个端口都直接与单台主机相连（普通网桥的端口往往连接到以太网的一个网段)，并且一般都工作在 $\color{green}{\text{全双工}}$ 方式。</p>
<p>2）以太网交换机能同时连通许多对端口，使每对相互通信的主机都能像独占通信媒体那样无碰撞地传输数据。</p>
<p>3）以太网交换机也是一种即插即用设备（和透明网桥一样)，其内部的帧的转发表也是通过自学习算法自动地逐渐建立起来的。</p>
<p>4）以太网交换机由于使用了专用的交换结构芯片，因此交换速率较高。</p>
<p>5)以太网交换机独占传输媒体的带宽。</p>
<p>对于传统10Mb/s的共享式以太网，若共有N个用户，则每个用户占有的平均带宽只有总带宽(10Mb/s）的1/N。在使用以太网交换机时，虽然在每个端口到主机的带宽还是10Mb/s，但由于一个用户在通信时是独占而不是和其他网络用户共享传输媒体的带宽，因此拥有N个端口的交换机的总容量为N×1OMb/s。这正是交换机的最大优点。</p>
<p>以太网交换机一般都具有多种速率的端口，例如可以具有10Mb/s、100Mb/s和 1Gb/s的端口的各种组合,因此大大方便了各种不同情况的用户。</p>
<h5 id="两种交换模式"><a href="#两种交换模式" class="headerlink" title="两种交换模式"></a>两种交换模式</h5><p>目前，以太网交换机主要采用两种交换模式，即直通式和存储转发式。</p>
<p>1） $\color{green}{\text{直通式交换机}}$ 只检查帧的目的地址，这使得帧在接收后几乎能马上被传出去。这种方式速度快，但缺乏智能性和安全性，也无法支持具有不同速率的端口的交换。</p>
<p>2） $\color{green}{\text{存储转发式交换机}}$ 先将接收到的帧缓存到高速缓存器中，并检查数据是否正确，确认无误后通过查找表转换成输出端口将该帧发送出去。如果发现帧有错，那么就将其丢弃。存储转发式的优点是可靠性高，并能支持不同速率端口间的转换，缺点是延迟较大。</p>
<h3 id="本章小结及疑难点"><a href="#本章小结及疑难点" class="headerlink" title="本章小结及疑难点"></a>本章小结及疑难点</h3><h4 id="“链路”和“数据链路”有何区别-“电路接通”-与“数据链路接通”有何区别"><a href="#“链路”和“数据链路”有何区别-“电路接通”-与“数据链路接通”有何区别" class="headerlink" title="“链路”和“数据链路”有何区别?“电路接通” 与“数据链路接通”有何区别?"></a>“链路”和“数据链路”有何区别?“电路接通” 与“数据链路接通”有何区别?</h4><p>所谓 $\color{green}{\text{链路}}$ （Link)，是指从一个结点到相邻结点的一段物理线路，其中间没有其他任何的交换结点。在进行数据通信时，两台计算机之间的通信路径往往要经过许多段这样的链路。可见,链路只是一条路径的组成部分。</p>
<p>$\color{green}{\text{数据链路}}$ （Data Link）则是另一个概念。因为在一条线路上传送数据时，除必须有一条物理线路外，还必须有一些通信协议来控制这些数据的传输。若把实现这些协议的硬件和软件加到链路上，就构成了数据链路。有时也把链路分为物理链路和逻辑链路。物理链路就是指上面所说的链路，逻辑链路就是上面的数据链路，即物理链路加上必要的通信协议。</p>
<p>“电路接通”表示链路两端的结点交换机已经开机，物理连接已经能够传送比特流，但数据传输并不可靠，在物理连接基础上，再建立数据链路连接，才能说“数据链路接通”。此后，由于数据链路连接具有检测、确认和重传功能，才使得不太可靠的物理链路变成可靠的数据链路，进行可靠的数据传输。当数据链路断开连接时，物理电路连接不一定跟着断开连接。</p>
<h4 id="在停止-等待协议中，确认帧为什么不需要序号（如用ACKO和-ACK1"><a href="#在停止-等待协议中，确认帧为什么不需要序号（如用ACKO和-ACK1" class="headerlink" title="在停止-等待协议中，确认帧为什么不需要序号（如用ACKO和 ACK1) ?"></a>在停止-等待协议中，确认帧为什么不需要序号（如用ACKO和 ACK1) ?</h4><p>在停止-等待协议中，发送方每发送一帧，都需要在收到接收方的确认帧后，才能进行下一帧的发送，而发送方收到的确认帧也一定是自己刚刚发出去的数据帧的确认帧，无须加序号标记。</p>
<h4 id="说明用-n-比特进行编号时，若接收窗口的大小为1，则只有在发送窗口的大小-W-T-leq-2-n-1-时，连续ARQ协议才能正确运行。"><a href="#说明用-n-比特进行编号时，若接收窗口的大小为1，则只有在发送窗口的大小-W-T-leq-2-n-1-时，连续ARQ协议才能正确运行。" class="headerlink" title="说明用 $n$ 比特进行编号时，若接收窗口的大小为1，则只有在发送窗口的大小 $W_T \leq 2^n-1$ 时，连续ARQ协议才能正确运行。"></a>说明用 $n$ 比特进行编号时，若接收窗口的大小为1，则只有在发送窗口的大小 $W_T \leq 2^n-1$ 时，连续ARQ协议才能正确运行。</h4><p>举一个具体的例子进行说明。例如用3比特可编出8个不同的序号，因而发送窗口的最大值似乎应为8。但实际上，设置发送窗口为8将使协议在某些情况下无法工作。现在我们就来说明这一点。</p>
<p>设发送窗口 $W_T$ = 8，发送端发送完0~7号共8个数据帧。因发送窗口已满，发送暂停。假定这8个数据帧均已正确到达接收端，并且对每个数据帧，接收端都发送出确认帧。下面考虑两种不同的情况。</p>
<p>第一种情况是:所有确认帧都正确到达了发送端，因而发送端接着又发送8个新的数据帧,其编号应是0～7。注意，序号是循环使用的。因此序号虽然相同，但8个帧都是新的帧。</p>
<p>第二种情况是:所有确认帧都丢失了。经过一段由超时计时器控制的时间后，发送端重传这8个旧的数据帧，其编号仍为0～7。</p>
<p>于是，当接收端第二次收到编号为0~7的8个数据帧时，就无法判定这是8个新的数据帧还是8个重传的旧数据帧。</p>
<p>因此，将发送窗口设置为8显然是不行的。</p>
<h4 id="证明-对于选择重传协议，若有-n-比特进行编号，则接收窗口的最大值为-W-R-leq-2-n-1-。"><a href="#证明-对于选择重传协议，若有-n-比特进行编号，则接收窗口的最大值为-W-R-leq-2-n-1-。" class="headerlink" title="证明:对于选择重传协议，若有 $n$ 比特进行编号，则接收窗口的最大值为 $W_R \leq 2^{n-1}$ 。"></a>证明:对于选择重传协议，若有 $n$ 比特进行编号，则接收窗口的最大值为 $W_R \leq 2^{n-1}$ 。</h4><p>设发送窗口大小为 $W_T$ 。因为 $W_T$ + $W_R \leq 2^n$， $W_T=W_R$ ，WR取最大值 $2^n/2$ = $2^{n-1}$ 。</p>
<p>注意，如果题目没有特别说明，那么一般情况下选择重传协议的发送窗口和接收窗口的大小是相等的。大家试想一下，SR协议中接收窗口值大于1，接收窗口要等到接收范围内所有帧收到才能更新，发送窗口要等接收窗口更新后才会更新，那么发送窗口比接收窗口多出来的那部分窗口就没有意义了。</p>
<ul>
<li>选择重传协议窗口大小的计算：<a target="_blank" rel="noopener" href="https://blog.csdn.net/u011617742/article/details/50387670">参考文献</a></li>
</ul>
<h4 id="数据链路层使用PPP协议或CSMA-CD协议时，既然不保证可靠传输，为什么要对所传输的帧进行差错检验"><a href="#数据链路层使用PPP协议或CSMA-CD协议时，既然不保证可靠传输，为什么要对所传输的帧进行差错检验" class="headerlink" title="数据链路层使用PPP协议或CSMA/CD协议时，既然不保证可靠传输，为什么要对所传输的帧进行差错检验?"></a>数据链路层使用PPP协议或CSMA/CD协议时，既然不保证可靠传输，为什么要对所传输的帧进行差错检验?</h4><p>当数据链路层使用PPP协议或CSMA/CD协议时，在数据链路层的接收端对所传输的帧进行差错检验是为了不将已发现有差错的帧（不管是什么原因造成的）接收下来。如果在接收端不进行差错检测，那么接收端上交给主机的帧就可能包括在传输中出』左销队的帐A什沿有传输差端主机是没有用处的。换言之，接收端进行差错检测的目的是:“上交主机的帧都是没有传输差错的，有差错的都已经丢弃了”，或者更加严格地说:“我们以很接近于1的概率认为，凡是上交主机的帧都是没有传输差错的”。</p>
<h4 id="为什么PPP协议不使用帧的编号和确认机制来实现可靠传输"><a href="#为什么PPP协议不使用帧的编号和确认机制来实现可靠传输" class="headerlink" title="为什么PPP协议不使用帧的编号和确认机制来实现可靠传输?"></a>为什么PPP协议不使用帧的编号和确认机制来实现可靠传输?</h4><p>PPP不使用序号和确认机制是出于以下考虑:</p>
<p>若使用能够实现可靠传输的数据链路层协议（如 HDLC)，开销就会增大。当数据链路层出现差错的概率不大时，使用比较简单的PPP较为合理。</p>
<p>在因特网环境下，PPP的信息字段放入的数据是IP数据报。假定我们采用了能实现可靠传输但十分复杂的数据链路层协议，当数据帧在路由器中从数据链路层上升到网络层后，仍有可能因网络拥塞而被丢弃。因此，数据链路层的可靠传输并不能保证网络层的传输也是可靠的。</p>
<p>PPP在帧格式中有帧校验序列FCS字段。对于每个收到的帧，PPP都要使用硬件进行CRC检验。若发现有差错，则丢弃该帧（一定不能把有差错的帧交给上一层)。端到端的差错控制最后由高层协议负责。因此，PPP可以保证无差错接收。</p>
<h4 id="两台计算机通过计算机网络传输一个文件时，有两种可行的确认策略。第一种是由发送端将文件分割成分组，接收端逐个确认分组-但就整体而言，文件并没有得到确认。第二种策略是接收端不确认单个分组，而是当文件全部收到后，对整个文件予以接收确认。试比较这两种方式的优缺点，以及它们各自适用的场合。"><a href="#两台计算机通过计算机网络传输一个文件时，有两种可行的确认策略。第一种是由发送端将文件分割成分组，接收端逐个确认分组-但就整体而言，文件并没有得到确认。第二种策略是接收端不确认单个分组，而是当文件全部收到后，对整个文件予以接收确认。试比较这两种方式的优缺点，以及它们各自适用的场合。" class="headerlink" title="两台计算机通过计算机网络传输一个文件时，有两种可行的确认策略。第一种是由发送端将文件分割成分组，接收端逐个确认分组;但就整体而言，文件并没有得到确认。第二种策略是接收端不确认单个分组，而是当文件全部收到后，对整个文件予以接收确认。试比较这两种方式的优缺点，以及它们各自适用的场合。"></a>两台计算机通过计算机网络传输一个文件时，有两种可行的确认策略。第一种是由发送端将文件分割成分组，接收端逐个确认分组;但就整体而言，文件并没有得到确认。第二种策略是接收端不确认单个分组，而是当文件全部收到后，对整个文件予以接收确认。试比较这两种方式的优缺点，以及它们各自适用的场合。</h4><p>在计算机网络中，数据的传输过程可能会引起数据的丢失、出错等，因此一个可靠的传输需要一定的差错控制机制，确认是实现差错控制的一个辅助手段。上面的两种确认策略都是可行的,但它们的性能取决于所应用的网络环境。</p>
<p>具体地说，当网络传输可靠性较低且分组容易丢失时，第一种策略即对每个分组逐一确认较好，此时仅需重传丢失或出错的分组。如果网络的传输可靠性较高，那么在不发生差错的情况下,即使仅对整个文件进行一次确认，从而减少了确认的次数，节省了网络带宽和网络资源;不过有单个分组丢失或出错，也需要重传整个文件。</p>
<h4 id="局域网、广域网和因特网之间的关系总结。"><a href="#局域网、广域网和因特网之间的关系总结。" class="headerlink" title="局域网、广域网和因特网之间的关系总结。"></a>局域网、广域网和因特网之间的关系总结。</h4><p>为方便理解，可将广域网视为一个大的局域网，专业地讲，就是通过交换机连接多个局域网，组成更大的局域网，即广域网。因此，广域网仍然是一个网络。而因特网是多个网络之间的互联，即因特网由大局域网（广域网）和小局域网共同通过路由器相连。因此局域网可以通过广域网与另一个相隔很远的局域网进行通信。</p>
<h4 id="IEEE-802局域网参考模型与OSI参考模型有何异同之处"><a href="#IEEE-802局域网参考模型与OSI参考模型有何异同之处" class="headerlink" title="IEEE 802局域网参考模型与OSI参考模型有何异同之处?"></a>IEEE 802局域网参考模型与OSI参考模型有何异同之处?</h4><p>局域网的体系结构只有OSI 参考模型的下两层（物理层和数据链路层)，而没有第三层以上的层次。即使是下两层，由于局域网是共享广播信道，而且产品的种类繁多，涉及多种媒体访问方法，所以两者存在明显的差别。</p>
<p>在局域网中，与OSI参考模型的物理层相同的是:该层负责物理连接并在媒体上传输比特流,主要任务是描述传输媒体接口的一些特性。在局域网中，数据链路层的主要作用与OSI参考模型的数据链路层相同:都通过一些数据链路层协议，在不可靠的传输信道上实现可靠的数据传输;负责帧的传送与控制，但在局域网中，由于各站共享网络公共信道，因此数据链路层必须具有媒体访问控制功能（如何分配信道、如何避免或解决信道争用)。又由于局域网采用的拓扑结构与传输媒体多种多样，相应的媒体访问控制方法也有多种，因此在数据链路功能中应该将与传输媒体有关的部分和无关的部分分开。这样，IEEE 802局域网参考模型中的数据链路层就划分为两个子层:媒体访问控制（MAC)子层和逻辑链路控制（LLC)子层。</p>
<p>与OSI参考模型不同的是:在IEEE 802局域网参考模型中没有网络层。局域网中，在任意两个结点之间只有唯一的一条链路，不需要进行路由选择和流量控制，所以在局域网中不单独设置网络层。</p>
<p>由上面的分析可知，局域网的参考模型只相当于OSI参考模型的最低两层，且两者的物理层和数据链路层之间也有很大差别。在 IEEE 802系列标准中，各个子标准的物理层和媒体访问控制(MAC)子层是有区别的，而逻辑链路控制（LLC）子层是相同的，也就是说，LLC子层实际上是高层协议与任何一种 MAC子层之间的标准接口。</p>
<h4 id="在IEEE802-3标准以太网中，为什么说如果有冲突，那么冲突一定发生在冲突窗口内-或者说一个帧如果在冲突窗口内没有发生冲突，那么该帧就不会再发生冲突"><a href="#在IEEE802-3标准以太网中，为什么说如果有冲突，那么冲突一定发生在冲突窗口内-或者说一个帧如果在冲突窗口内没有发生冲突，那么该帧就不会再发生冲突" class="headerlink" title="在IEEE802.3标准以太网中，为什么说如果有冲突，那么冲突一定发生在冲突窗口内?或者说一个帧如果在冲突窗口内没有发生冲突，那么该帧就不会再发生冲突?"></a>在IEEE802.3标准以太网中，为什么说如果有冲突，那么冲突一定发生在冲突窗口内?或者说一个帧如果在冲突窗口内没有发生冲突，那么该帧就不会再发生冲突?</h4><p>结点发送数据时，先侦听信道是否有载波，如果有，表示信道忙，那么继续侦听，直至检测到空闲为止;一个数据帧在从结点A向最远的结点传输的过程中，如果有其他结点也正在发送数据，那么此时就会发生冲突，冲突后的信号需要经过冲突窗口时间后传回结点A，结点A会检测到冲突，所以说如果有冲突，那么一定发生在冲突窗口内，如果在冲突窗口内没有发生冲突，之后如果其他结点再要发送数据，那么就会侦听到信道忙，而不会发送数据，从而不会再发生冲突。</p>
<h4 id="一个以太网的速率从10Mb-s升级到100Mb-s，满足CSMA-CD冲突条件。为使其正常工作，需做哪些调整-为什么"><a href="#一个以太网的速率从10Mb-s升级到100Mb-s，满足CSMA-CD冲突条件。为使其正常工作，需做哪些调整-为什么" class="headerlink" title="一个以太网的速率从10Mb/s升级到100Mb/s，满足CSMA/CD冲突条件。为使其正常工作，需做哪些调整?为什么?"></a>一个以太网的速率从10Mb/s升级到100Mb/s，满足CSMA/CD冲突条件。为使其正常工作，需做哪些调整?为什么?</h4><p>由于10BASE-T 要比10BASE2和10BASE5的优越性更明显，因此所有快速以太网系统都使用集线器(Hub)，而不使用同轴电缆。100BASE-T MAC 与10Mb/s的经典以太网MAC几乎一样,唯一不同的参数就是帧际间隙时间，10Mb/s 以太网是9.6 $\mu$ s(最小值)，快速以太网（100Mb/s)是0.96 $\mu$ s(最小值)。另外，为了维持最小分组尺寸不变，需要减小最大冲突域直径。所有这些调整的主要原因是速率提高到了原来以太网的10倍。</p>
<h4 id="HDLC协议是PPP协议的基础，它使用位填充来实现透明传输。但PPP协议却使用字符填充而不使用位填充，为什么"><a href="#HDLC协议是PPP协议的基础，它使用位填充来实现透明传输。但PPP协议却使用字符填充而不使用位填充，为什么" class="headerlink" title="HDLC协议是PPP协议的基础，它使用位填充来实现透明传输。但PPP协议却使用字符填充而不使用位填充，为什么?"></a>HDLC协议是PPP协议的基础，它使用位填充来实现透明传输。但PPP协议却使用字符填充而不使用位填充，为什么?</h4><p>PPP被明确地设计成以软件形式实现，而不像HDLC协议那样几乎总以硬件形式实现。对于软件实现，完全用字节操作比用单个位操作简单得多。此外，PPP被设计成与调制解调器一道使用，而调制解调器是以一个字节而非一个比特为单元接收和发送数据的。</p>
<h4 id="假定连接到透明网桥上的一台计算机把-一个数据帧发给网络上的一个不存在的设备，网桥将如何处理这个帧"><a href="#假定连接到透明网桥上的一台计算机把-一个数据帧发给网络上的一个不存在的设备，网桥将如何处理这个帧" class="headerlink" title="假定连接到透明网桥上的一台计算机把-一个数据帧发给网络上的一个不存在的设备，网桥将如何处理这个帧?"></a>假定连接到透明网桥上的一台计算机把-一个数据帧发给网络上的一个不存在的设备，网桥将如何处理这个帧?</h4><p>网桥不知道网络上是否存在该设备，它只知道在其转发表中没有这个设备的MAC地址。因此，当网桥收到这个目的地址未知的帧时，它将扩散该帧，即把该帧发送到所连接的除输入网段外的所有其他网段。</p>
<h4 id="关于冲突域（碰撞域）和广播域辨析。"><a href="#关于冲突域（碰撞域）和广播域辨析。" class="headerlink" title="关于冲突域（碰撞域）和广播域辨析。"></a>关于冲突域（碰撞域）和广播域辨析。</h4><p>一块网卡发送信息时，只要有可能和另一块网卡冲突，那么这些可能冲突的网卡就构成冲突域。一块网卡发出一个广播时，能收到这个广播的所有网卡的集合称为一个广播域。一般来说，-一个网段就是一个冲突域，一个局域网就是一个广播域。</p>
<h4 id="关于物理层、数据链路层、网络层设备对于隔离冲突域和广播域的总结。"><a href="#关于物理层、数据链路层、网络层设备对于隔离冲突域和广播域的总结。" class="headerlink" title="关于物理层、数据链路层、网络层设备对于隔离冲突域和广播域的总结。"></a>关于物理层、数据链路层、网络层设备对于隔离冲突域和广播域的总结。</h4><details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004113302.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004113302.png';" /></details>

<ul>
<li><a target="_blank" rel="noopener" href="https://www.cnblogs.com/bakari/archive/2012/09/08/2677086.html">冲突域和广播域的区别</a></li>
</ul>
<h4 id="与传统共享式局域网相比，使用局域网交换机的交换式局域网为什么能改善网络的性能和服务质量"><a href="#与传统共享式局域网相比，使用局域网交换机的交换式局域网为什么能改善网络的性能和服务质量" class="headerlink" title="与传统共享式局域网相比，使用局域网交换机的交换式局域网为什么能改善网络的性能和服务质量?"></a>与传统共享式局域网相比，使用局域网交换机的交换式局域网为什么能改善网络的性能和服务质量?</h4><p>传统共享式局域网的核心设备是集线器，而交换式局域网的核心是以太网交换机。在使用共享式集线器的传统局域网中，在任何时刻只能有一个结点能够通过共享通信信道发送数据;在使用交换机的交换式局域网中，交换机可以在它的多个端口之间建立多个并发连接，从而实现结点之间数据的并发传输，有效地改善网络性能和服务质量。</p>
<h4 id="试分析中继器、集线器、网桥和交换机这四种网络互联设备的区别与联系。"><a href="#试分析中继器、集线器、网桥和交换机这四种网络互联设备的区别与联系。" class="headerlink" title="试分析中继器、集线器、网桥和交换机这四种网络互联设备的区别与联系。"></a>试分析中继器、集线器、网桥和交换机这四种网络互联设备的区别与联系。</h4><p>这四种设备都是用于互联、扩展局域网的连接设备，但它们工作的层次和实现的功能不同。</p>
<p>$\color{green}{\text{中继器}}$ 工作在物理层，用来连接两个速率相同且数据链路层协议也相同的网仅，共力肥足们除数字信号在基带传输中由于经过一长段电缆而造成的失具和装减，便史信亏的波形个’独度o到从需的要求;其原理是信号再生。</p>
<p>$\color{green}{\text{集线器}}$ （Hub）也工作在物理层，相当于一个多接口的中继器，它可将多个结点连接成一个共享式的局域网，但任何时刻都只能有一个结点通过公共信道发送数据。</p>
<p>$\color{green}{\text{网桥}}$ 工作在数据链路层，可以互联不同的物理层、不同的MAC子层及不同速率的以太网。网桥具有过滤帧及存储转发帧的功能，可以隔离冲突域，但不能隔离广播域。</p>
<p>$\color{green}{\text{交换机}}$ 工作在数据链路层，相当于一个多端口的网桥，是父换式同域网的核心仅备。匕叶端口之间建立多个并发连接，实现多个结点之间的并发传输。因此，父换机的母个项口结点所白用的带宽不会因为端口结点数目的增加而减少，且整个交换机的忌带苋会阻看项口给点的增加增加。交换机一般工作在全双工方式，有的局域网交换机采用存储转发万式进仃转反，也有的父换机采用直通交换方式（即在收到帧的同时立即按帧的目的 MAC地址状疋以似的将及作，心不必先缓存再处理)。另外，利用交换机可以实现虚拟局域网（VLAN)，VLAN不仅可以隔离冲突域，而且可以隔离广播域。</p>
<h4 id="交换机和网桥的不同之处。"><a href="#交换机和网桥的不同之处。" class="headerlink" title="交换机和网桥的不同之处。"></a>交换机和网桥的不同之处。</h4><p>尽管交换机也称多端口网桥，但两者仍有许多不同之处。主要包括以下3点:</p>
<p>1）网桥的端口一般连接局域网，而交换机的端口一般直接与局域网的主机相连。</p>
<p>2）交换机允许多对计算机同时通信，而网桥仅允许每个网段上的计算机同时通信。</p>
<p>3）网桥采用存储转发进行转发，而以太网交换机还可以采用直通方式进行转发，且以太网交换机采用了专用的交换结构芯片，转发速度比网桥快。</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://ednow.github.io/2021/07/24/%E7%8E%8B%E9%81%93-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-ch2-%E7%89%A9%E7%90%86%E5%B1%82/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ednow">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ednow">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/24/%E7%8E%8B%E9%81%93-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-ch2-%E7%89%A9%E7%90%86%E5%B1%82/" class="post-title-link" itemprop="url">王道-计算机网络-ch2-物理层</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-07-24 09:09:07" itemprop="dateCreated datePublished" datetime="2021-07-24T09:09:07+08:00">2021-07-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-10-04 22:55:41" itemprop="dateModified" datetime="2021-10-04T22:55:41+08:00">2021-10-04</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%80%83%E7%A0%94/" itemprop="url" rel="index"><span itemprop="name">考研</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="王道"><a href="#王道" class="headerlink" title="王道"></a>王道</h1><h2 id="物理层"><a href="#物理层" class="headerlink" title="物理层"></a>物理层</h2><p>【考纲内容】</p>
<p>(一）通信基础</p>
<p>信道、信号、带宽、码元、波特、速率、信源与信宿等基本概念</p>
<p>奈奎斯特定理与香农定理;编码与调制</p>
<p>电路交换、报文交换与分组交换;数据报与虚电路</p>
<p>(二）传输介质</p>
<p>双绞线、同轴电缆、光纤与无线传输介质;物理层接口的特性</p>
<p>(三）物理层设备</p>
<p>中继器;集线器</p>
<p>【复习提示】</p>
<p>物理层考虑的是怎样才能在连接各台计算机的传输媒体上传输数据比特流，而不是指具体的传输媒体。本章概念较多，易出选择题，且涉及一些通信原理，读者不太明白的地方可以参考一些相关书籍，通信部分的内容也并非考研重点。复习时应抓住重点，如奈奎斯特定理和香农定理的应用、编码与调制技术、数据交换方式，以及电路交换、报文交换与分组交换技术等。</p>
<h3 id="通信基础"><a href="#通信基础" class="headerlink" title="通信基础"></a>通信基础</h3><h4 id="基本概念"><a href="#基本概念" class="headerlink" title="基本概念"></a>基本概念</h4><h5 id="数据、信号与码元"><a href="#数据、信号与码元" class="headerlink" title="数据、信号与码元"></a>数据、信号与码元</h5><p>通信的目的是传送信息，如文字、图像和视频等。数据是指传送信息的实体。信号则是数据的电气或电磁表现，是数据在传输过程中的存在形式。数据和信号都可用“模拟的”或“数字的”来修饰:①连续变化的数据（或信号）称为模拟数据（或$\color{green}{\text{模拟信号}}$);②取值仅允许为有限的几个离散数值的数据（或信号）称为数字数据（或$\color{green}{\text{数字信号}}$)。<br>数据传输方式可分为串行传输和并行传输。串行传输是指1比特1比特地按照时间顺序传输(远距离通信通常采用串行传输)，并行传输是指若干比特通过多条通信信道同时传输。</p>
<p>码元是指用一个固定时长的信号波形（数字脉冲）表示一位k进制数字，代表不同$\color{green}{\text{离散数值}}$的基本波形，是数字通信中数字信号的计量单位，这个时长内的信号称为k进制码元，而该时长称为码元宽度。1码元可以携带若干比特的信息量。例如，在使用二进制编码时，只有两种不同的码元:一种代表О状态，另一种代表1状态。</p>
<h5 id="信源、信道与信宿"><a href="#信源、信道与信宿" class="headerlink" title="信源、信道与信宿"></a>信源、信道与信宿</h5><p>数据通信是指数字计算机或其他数字终端之间的通信。一个数据通信系统主要划分为信源、信道和信宿三部分。信源是产生和发送数据的源头。信宿是接收数据的终点，它们通常都是计算机或其他数字终端装置。发送端信源发出的信息需要通过变换器转换成适合于在信道上传输的信号，而通过信道传输到接收端的信号先由反变换器转换成原始信息，再发送给信宿。</p>
<p>信道与电路并不等同，信道是信号的传输媒介。一个信道可视为一条线路的逻辑部件，一般用来表示向某个方向传送信息的介质，因此一条通信线路往往包含一条发送信道和一条接收信道。噪声源是信道上的噪声（即对信号的干扰）及分散在通信系统其他各处的噪声的集中表示。</p>
<p>图2.1所示为一个单向通信系统的模型。实际的通信系统大多为双向的，即往往包含一条发送信道和一条接收信道，信道可以进行双向通信。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210727094849.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210727094849.png';" /></details>

<p>信道按传输信号形式的不同，可分为传送模拟信号的模拟信道和传送数字信号的数字信道两大类;信道按传输介质的不同可分为无线信道和有线信道。</p>
<p>信道上传送的信号有$\color{green}{\text{基带信号}}$和$\color{green}{\text{宽带信号}}$之分。基带信号将数字信号1和0直接用两种不同的电压表示，然后送到数字信道上传输（称为基带传输);宽带信号将基带信号进行调制后形成频分复用模拟信号，然后送到模拟信道上传输（称为宽带传输)。</p>
<p>从通信双方信息的交互方式看，可分为三种基本方式:</p>
<p>1)单向通信。只有一个方向的通信而没有反方向的交互，仅需要一条信道。例如，无线电广播、电视广播就属于这种类型。</p>
<p>2）半双工通信。通信的双方都可以发送或接收信息，但任何一方都不能同时发送和接收信息，此时需要两条信道。</p>
<p>3）全双工通信。通信双方可以同时发送和接收信息，也需要两条信道。</p>
<p>信道的极限容量是指信道的最高码元传输速率或信道的极限信息传输速率。</p>
<h5 id="速率、波特与带宽"><a href="#速率、波特与带宽" class="headerlink" title="速率、波特与带宽"></a>速率、波特与带宽</h5><p>速率也称数据率，指的是数据传输速率，表示单位时间内传输的数据量。可以用码元传输速率和信息传输速率表示。</p>
<p>1）码元传输速率。又称波特率，它表示单位时间内数字通信系统所传输的码元个数（也可称为脉冲个数或信号变化的次数)，单位是波特（$\color{green}{\text{Baud}}$)。1波特表示数字通信系统每秒传输一个码元。码元可以是多进制的，也可以是二进制的，码元速率与进制数无关。</p>
<p>2）信息传输速率。又称信息速率、比特率等，它表示单位时间内数字通信系统传输的二进制码元个数（即比特数)，单位是比特/秒(b/s)。</p>
<blockquote>
<p>注意:波特和比特是两个不同的概念，码元传输速率也称调制速率、波形速率或符号速率。但码元传输速率与信息传输速率在数量上却又有一定的关系。若一个码元携带n比特的信息量，则M波特率的码元传输速率所对应的信息传输速率为Mn比特/秒。</p>
</blockquote>
<p>带宽原指信号具有的频带宽度，单位是赫兹（Hz)。在实际网络中，由于数据率是信道最重要的指标之一，而带宽与数据率存在数值上的互换关系，因此常用来表示网络的通信线路所能传输数据的能力。因此,带宽表示单位时间内从网络中的某一点到另一点所能通过的“最高数据率”。显然，此时带宽的单位不再是Hz，而是b/s。</p>
<h4 id="奈奎斯特定理与香农定理"><a href="#奈奎斯特定理与香农定理" class="headerlink" title="奈奎斯特定理与香农定理"></a>奈奎斯特定理与香农定理</h4><ul>
<li> $\color{green}{\text{Q}}$ ：奈奎斯特定理和香农定理的关系？</li>
<li> $\color{green}{\text{A}}$ ：奈奎斯特定理适用于 $\color{green}{\text{无噪声}}$ 的情况，香农定理适用于 $\color{green}{\text{有噪声}}$ 的情况</li>
</ul>
<h5 id="奈奎斯特定理"><a href="#奈奎斯特定理" class="headerlink" title="奈奎斯特定理"></a>奈奎斯特定理</h5><p>具体的信道所能通过的频率范围总是有限的。信号中的许多高频分量往往不能通过信道，否则在传输中会衰减，导致接收端收到的信号波形失去码元之间的清晰界限，这种现象称为码间串扰。奈奎斯特（Nyquist）定理又称奈氏准则，它规定:在理想低通（没有噪声、带宽有限)的信道中，为了避免码间串扰，极限码元传输速率为 $\color{green}{\text{2W波特}}$ ，其中W是理想低通信道的带宽。若用V表示每个码元离散电平的数目（码元的离散电平数目是指有多少种不同的码元，比如有16种不同的码元，则需要4个二进制位，因此数据传输速率是码元传输速率的4倍)，则极限数据率为</p>
<p>理想低通信道下的极限数据传输速率=$2W log_2V$(单位为 b/s)</p>
<ul>
<li>就是波特率直接转换为数据传输速率</li>
</ul>
<p>对于奈氏准则，可以得出以下结论:</p>
<p>1）在任何信道中，码元传输速率是有上限的。若传输速率超过此上限，就会出现严重的码间串扰问题，使得接收端不可能完全正确识别码元。</p>
<p>2）信道的频带越宽（即通过的信号高频分量越多)，就可用更高的速率进行码元的有效传输。</p>
<p>3）奈氏准则给出了码元传输速率的限制，但并未对信息传输速率给出限制，即未对一个码元可以对应多少个二进制位给出限制。</p>
<p>由于码元传输速率受奈氏准则的制约，所以要提高数据传输速率，就必须设法使每个码元携带更多比特的信息量，此时就需要采用多元制的调制方法。</p>
<blockquote>
<p>这个地方配合谢书看，来龙去脉讲的比较清楚</p>
</blockquote>
<details><summary>谢书：奈奎斯特定理</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210730143427.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210730143427.png';" /></details>

<h5 id="香农定理"><a href="#香农定理" class="headerlink" title="香农定理"></a>香农定理</h5><p>香农（Shannon）定理给出了带宽受限且有高斯白噪声干扰的信道的极限数据传输速率，当用此速率进行传输时，可以做到不产生误差。香农定理定义为</p>
<p>信道的极限数据传输速率= W$log_2$( 1 +S/N)(单位为b/s)</p>
<p>式中，W为信道的带宽，S为信道所传输信号的平均功率，N为信道内部的高斯噪声功率。 $\color{green}{\text{S/N}}$ 为 $\color{green}{\text{信噪比}}$ ，即信号的平均功率与噪声的平均功率之比，信噪比 = 10$log_{10}$(S/N)(单位为dB)，例如当S/N= 10时，信噪比为10dB，而当S/N = 1000时，信噪比为30dB。</p>
<p>对于香农定理，可以得出以下结论:</p>
<p>1）信道的带宽或信道中的信噪比越大，信息的极限传输速率越高。</p>
<p>2）对一定的传输带宽和一定的信噪比，信息传输速率的上限是确定的。</p>
<p>3）只要信息传输速率低于信道的极限传输速率，就能找到某种方法来实现无差错的传输。</p>
<p>4）香农定理得出的是极限信息传输速率，实际信道能达到的传输速率要比它低不少。</p>
<p>奈氏准则只考虑了带宽与极限码元传输速率的关系，而香农定理不仅考虑到了带宽，也考虑到了信噪比。这从另一个侧面表明，一个码元对应的二进制位数是有限的。</p>
<blockquote>
<p>怎么表明的？？？啥叫对应的二进制位数</p>
</blockquote>
<h4 id="编码与调制"><a href="#编码与调制" class="headerlink" title="编码与调制"></a>编码与调制</h4><p>数据无论是数字的还是模拟的，为了传输的目的都必须转变成信号。把数据变换为模拟信号的过程称为$\color{green}{\text{调制}}$，把数据变换为数字信号的过程称为$\color{green}{\text{编码}}$。</p>
<p>信号是数据的具体表示形式，它和数据有一定的关系，但又和数据不同。数字数据可以通过数字发送器转换为数字信号传输，也可以通过调制器转换成模拟信号传输;同样，模拟数据可以通过 PCM（Pulse Code Modulation，脉冲编码调制）编码器转换成数字信号传输，也可以通过放大器调制器转换成模拟信号传输。这样，就形成了下列4种编码方式。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210730143939.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210730143939.png';" /></details>


<h5 id="数字数据编码为数字信号"><a href="#数字数据编码为数字信号" class="headerlink" title="数字数据编码为数字信号"></a>数字数据编码为数字信号</h5><p>数字数据编码用于基带传输中，即在基本不改变数字数据信号频率的情况下，直接传输数字信号。具体用什么样的数字信号表示0及用什么样的数字信号表示1就是所谓的编码。编码的规则有多种，只要能有效地把1和0区分开即可，常用的数据数据编码有以下几种，如图2.2所示。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210727100211.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210727100211.png';" /></details>

<blockquote>
<p>时钟周期的作用<a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_30652105/article/details/112576297">参考文献</a><br>为什么归零编码可以传递时钟周期,波形应该画成参考文献那样理解，传递时钟周期可以立即为我要分辨出来连续1的话到底是几个1</p>
</blockquote>
<p>(1）归零编码</p>
<p>在归零编码(RZ）中用高电平代表1、低电平代表0(或者相反)，每个时钟周期的中间均跳变到低电平（归零)，接收方根据该跳变调整本方的时钟基准，这就为传输双方提供了自同步机制。由于归零需要占用一部分带宽，因此传输效率受到了一定的影响。</p>
<p>(2）非归零编码</p>
<p>非归零编码(NRZ)与RZ编码的区别是不用归零，一个周期可以全部用来传输数据。但NRZ编码无法传递时钟信号，双方难以同步，因此若想传输高速同步数据，则需要都带有时钟线。</p>
<p>(3）反向非归零编码</p>
<p>反向非归零编码(NRZI)与NRZ编码的区别是用信号的翻转代表0、信号保持不变代表1。翻转的信号本身可以作为一种通知机制。这种编码方式集成了前两种编码的优点，既能传输时钟信号，又能尽量不损失系统带宽。$\color{green}{\text{USB 2.0}}$通信的编码方式就是NRZI编码。</p>
<blockquote>
<p>$\color{red}{\text{Q}}$:感觉反向非归零编码并不能传递时钟周期啊？？<br>$\color{red}{\text{A}}$:<a target="_blank" rel="noopener" href="http://www.360doc.com/content/19/0428/20/6828497_832142706.shtml">参考文献</a></p>
</blockquote>
<p>(4）曼彻斯特编码</p>
<p>曼彻斯特编码（Manchester Encoding)将一个码元分成两个相等的间隔，前一个间隔为高电平而后一个间隔为低电平表示码元1;码元0的表示方法则正好相反。当然，也可采用相反的规定。该编码的特点是，在每个码元的中间出现电平跳变，位中间的跳变既作为时钟信号(可用于同步)，又作为数据信号，但它所占的频带宽度是原始基带宽度的两倍。</p>
<blockquote>
<p>注意:$\color{green}{\text{以太网}}$使用的编码方式就是曼彻斯特编码。</p>
</blockquote>
<p>(5）差分曼彻斯特编码</p>
<p>差分曼彻斯特编码常用于$\color{green}{\text{局域网}}$传输，其规则是:若码元为1，则前半个码元的电平与上一码元的后半个码元的电平相同;若码元为0，则情形相反。该编码的特点是，在每个码元的中间都有一次电平的跳转，可以实现自同步，且抗干扰性较好。</p>
<blockquote>
<p>局域网, 以太网的区别，<a target="_blank" rel="noopener" href="https://blog.csdn.net/digitalkee/article/details/105054541">参考文献</a></p>
</blockquote>
<blockquote>
<p>在每个时钟周期的起始处：跳变则说明该比特是0，不跳变则说明该比特是1。<a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E5%B7%AE%E5%88%86%E6%9B%BC%E5%BD%BB%E6%96%AF%E7%89%B9%E7%BC%96%E7%A0%81/7486879?fr=aladdin">百度百科</a></p>
</blockquote>
<p>(6）4B/5B编码</p>
<p>将欲发送数据流的每4位作为一组，然后按照4B/5B编码规则将其转换成相应的5位码。5位码共32种组合，但只采用其中的16种对应16种不同的4位码，其他16种作为控制码（帧的开始和结束、线路的状态信息等）或保留。</p>
<h5 id="数字数据调制为模拟信号"><a href="#数字数据调制为模拟信号" class="headerlink" title="数字数据调制为模拟信号"></a>数字数据调制为模拟信号</h5><p>数字数据调制技术在发送端将数字信号转换为模拟信号，而在接收端将模拟信号还原为数字信号，分别对应于调制解调器的调制和解调过程。基本的数字调制方法有如下几种:</p>
<p>1）幅移键控（ASK)。通过改变载波信号的振幅来表示数字信号1和0，而载波的频率和相位都不改变。比较容易实现，但抗干扰能力差。</p>
<p>2）频移键控(FSK)。通过改变载波信号的频率来表示数字信号1和0，而载波的振幅和相位都不改变。容易实现，抗干扰能力强，目前应用较为广泛。</p>
<p>3）相移键控（PSK)。通过改变载波信号的相位来表示数字信号1和0，而载波的振幅和频率都不改变。它又分为绝对调相和相对调相。</p>
<p>4）正交振幅调制(QAM)。在频率相同的前提下，将ASK与PSK结合起来，形成叠加信号。设波特率为B，采用m个相位，每个相位有n种振幅，则该QAM技术的数据传输速率R为</p>
<p>R= B$log_2$(mn)(单位为b/s)</p>
<p>图2.3所示是二进制幅移键控、频移键控和相移键控的例子。2ASK中用载波有幅度和无幅度分别表示数字数据的1和0;2FSK中用两种不同的频率分别表示数字数据1和0;2PSK中用相位0和相位T分别表示数字数据的1和0，是一种绝对调相方式。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210727100922.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210727100922.png';" /></details>


<h5 id="模拟数据编码为数字信号"><a href="#模拟数据编码为数字信号" class="headerlink" title="模拟数据编码为数字信号"></a>模拟数据编码为数字信号</h5><p>这种编码方式最典型的例子是常用于对音频信号进行编码的脉码调制(PCM, Pulse Code Modulation,<p style='color:green'>脉冲编码调制</p>)。它主要包括三个步骤，即采样、量化和编码。</p>
<p>先来介绍采样定理:在通信领域，带宽是指信号最高频率与最低频率之差，单位为Hz。因此，将模拟信号转换成数字信号时，假设原始信号中的最大频率为f，那么采样频率$f_{\text{采样}}$必须大于等于最大频率f的两倍，才能保证采样后的数字信号完整保留原始模拟信号的信息（(只需记住结论)。另外，$\color{green}{\text{采样定理}}$又称奈奎斯特定理。</p>
<p>1）采样是指对模拟信号进行周期性扫描，把时间上连续的信号变成时间上离散的信号。根据采样定理，当采样的频率大于等于模拟数据的频带带宽（最高变化频率）的两倍时，所得的离散信号可以无失真地代表被采样的模拟数据。</p>
<p>2）量化是把采样取得的电平幅值按照一定的分级标度转化为对应的数字值并取整数,这样就把连续的电平幅值转换为了离散的数字量。采样和量化的实质就是分割和转换。</p>
<p>3）编码是把量化的结果转换为与之对应的二进制编码。</p>
<h5 id="模拟数据调制为模拟信号"><a href="#模拟数据调制为模拟信号" class="headerlink" title="模拟数据调制为模拟信号"></a>模拟数据调制为模拟信号</h5><p>为了实现传输的有效性，可能需要较高的频率。这种调制方式还可以使用频分复用（FDM,Frequency Division Multiplexing)技术，充分利用带宽资源。电话机和本地局交换机采用模拟信号传输模拟数据的编码方式,模拟的声音数据是加载到模拟的载波信号中传输的。</p>
<h4 id="电路交换、报文交换与分组交换"><a href="#电路交换、报文交换与分组交换" class="headerlink" title="电路交换、报文交换与分组交换"></a>电路交换、报文交换与分组交换</h4><p>$\color{red}{\text{Q}}$：数据报交换和分组交换的区别</p>
<p>$\color{red}{\text{A}}$：数据报交换是分组交换的一种</p>
<h5 id="电路交换"><a href="#电路交换" class="headerlink" title="电路交换"></a>电路交换</h5><p>在进行数据传输前，两个结点之间必须先建立一条专用（双方独占）的物理通信路径（由通信双方之间的交换设备和链路逐段连接而成)，该路径可能经过许多中间结点。这一路径在整个数据传输期间一直被独占，直到通信结束后才被释放。因此，电路交换技术分为三个阶段<br>连接建立、数据传输和连接释放。</p>
<p>从通信资源的分配角度来看，“交换”就是按照某种方式动态地分配传输线路的资源电路交换的关键点是，在数据传输的过程中，用户始终占用端到端的固定传输带宽。</p>
<p>电路交换技术的优点如下:</p>
<p>1）通信时延小。由于通信线路为通信双方用户专用，数据直达，因此传输数据的时延非常小。当传输的数据量较大时，这一优点非常明显。</p>
<p>2）有序传输。双方通信时按发送顺序传送数据，不存在失序问题。</p>
<p>3）没有冲突。不同的通信双方拥有不同的信道，不会出现争用物理信道的问题。</p>
<p>4）适用范围广。电路交换既适用于传输模拟信号，又适用于传输数字信号。</p>
<p>5）实时性强。通信双方之间的物理通路一旦建立，双方就可以随时通信。</p>
<p>6）控制简单。电路交换的交换设备（交换机等）及控制均较简单。</p>
<p>电路交换技术的缺点如下:</p>
<p>1）建立连接时间长。电路交换的平均连接建立时间对计算机通信来说太长。</p>
<p>2）线路独占，使用效率低。电路交换连接建立后，物理通路被通信双方独占，即使通信线路空闲，也不能供其他用户使用，因而信道利用率低。</p>
<p>3）灵活性差。只要在通信双方建立的通路中的任何一点出了故障，就必须重新拨号建立新的连接，这对十分紧急和重要的通信是很不利的。</p>
<p>4）难以规格化。电路交换时，数据直达，不同类型、不同规格、不同速率的终端很难相互进行通信，也难以在通信过程中进行差错控制。</p>
<blockquote>
<p>注意，电路建立后，除源结点和目的结点外，电路上的任何结点都采取“直通方式”接收数据和发送数据，即不会存在存储转发所耗费的时间。</p>
</blockquote>
<h5 id="报文交换"><a href="#报文交换" class="headerlink" title="报文交换"></a>报文交换</h5><p>数据交换的单位是报文，报文携带有目标地址、源地址等信息。报文交换在交换结点采用的是存储转发的传输方式。</p>
<p>报文交换技术的优点如下:</p>
<p>1）无须建立连接。报文交换不需要为通信双方预先建立一条专用的通信线路，不存在建立连接时延，用户可以随时发送报文。</p>
<p>2）动态分配线路。当发送方把报文交给交换设备时，交换设备先存储整个报文，然后选择一条合适的空闲线路，将报文发送出去。</p>
<p>3）提高线路可靠性。如果某条传输路径发生故障，那么可重新选择另一条路径传输数据，因此提高了传输的可靠性。</p>
<p>4）提高线路利用率。通信双方不是固定占有一条通信线路，而是在不同的时间一段段地部分占有这条物理通道，因而大大提高了通信线路的利用率。</p>
<p>5)提供多目标服务。一个报文可以同时发送给多个目的地址，这在电路交换中是很难实现的。</p>
<p>报文交换技术的缺点如下:</p>
<p>1）由于数据进入交换结点后要经历存储、转发这一过程，因此会引起转发时延（包括接收报文、检验正确性、排队、发送时间等)。</p>
<p>2）报文交换对报文的大小没有限制，这就要求网络结点需要有较大的缓存空间。</p>
<blockquote>
<p>注意:报文交换主要使用在早期的电报通信网中，现在较少使用，通常被较先进的分组交换方式所取代。</p>
</blockquote>
<h5 id="分组交换"><a href="#分组交换" class="headerlink" title="分组交换"></a>分组交换</h5><p>同报文交换一样，分组交换也采用存储转发方式，但解决了报文交换中大报文传输的问题。</p>
<p>分组交换限制了每次传送的数据块大小的上限，把大的数据块划分为合理的小数据块，再加上一些必要的控制信息（如源地址、目的地址和编号信息等)，构成分组(Packet)。网络绐点恨掂控制信息把分组送到下一个结点，下一个结点接收到分组后，暂时保存并排队等待传输，然后根据分组控制信息选择它的下一个结点，直到到达目的结点。</p>
<p>分组交换的优点如下:</p>
<p>1）无建立时延。不需要为通信双方预先建立一条专用的通信线路，不存在连接建立时延,用户可随时发送分组。</p>
<p>2）线路利用率高。通信双方不是固定占有一条通信线路，而是在不同的时间一段一段地部分占有这条物理通路，因而大大提高了通信线路的利用率。</p>
<p>3)简化了存储管理（相对于报文交换)。因为分组的长度固定，相应的缓冲区的大小也固定在交换结点中存储器的管理通常被简化为对缓冲区的管理，相对比较容易。</p>
<p>4)加速传输。分组是逐个传输的，可以使后一个分组的存储操作与前一个分组的转发操作并行，这种流水线方式减少了报文的传输时间。此外，传输一个分组所需的缓冲区比传输次报文所需的缓冲区小得多，这样因缓冲区不足而等待发送的概率及时间也必然少得多.</p>
<p>5）减少了出错概率和重发数据量。因为分组较短，其出错概率必然减小，所以每次重发的数据量也就大大减少，这样不仅提高了可靠性，也减少了传输时延。</p>
<p>分组交换的缺点如下:</p>
<p>1)存在传输时延。尽管分组交换比报文交换的传输时延少，但相对于电路交换仍存在存储转发时延，而且其结点交换机必须具有更强的处理能力。</p>
<p>2）需要传输额外的信息量。每个小数据块都要加上源地址、目的地址和分组编号等信息从而构成分组，因此使得传送的信息量增大了5%～10%，一定程度上降低了通信效率增加了处理的时间，使控制复杂，时延增加。</p>
<p>3）当分组交换采用数据报服务时，可能会出现失序、丢失或重复分组，分组到达目的结点时，要对分组按编号进行排序等工作，因此很麻烦。若采用虚电路服务，虽无失序问题，但有呼叫建立、数据传输和虚电路释放三个过程。</p>
<p>图2.4给出了三种数据交换方式的比较。要传送的数据量很大且其传送时间远大于呼叫时间时，采用电路交换较为合适。端到端的通路由多段链路组成时，采用分组交换传送数据较为合适。从提高整个网络的信道利用率上看，报文交换和分组交换优于电路交换，其中分组交换比报文交换的时延小，尤其适合于计算机之间的突发式数据通信。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210727103336.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210727103336.png';" /></details>

<h4 id="数据报与虚电路"><a href="#数据报与虚电路" class="headerlink" title="数据报与虚电路"></a>数据报与虚电路</h4><p>分组交换根据其通信子网向端点系统提供的服务，还可进一步分为面向连接的虚电路方式和无连接的数据报方式。这两种服务方式都由网络层提供。要注意$\color{green}{\text{数据报方式}}$和$\color{green}{\text{虚电路方式}}$是$\color{green}{\text{分组交换}}$的两种方式。</p>
<p>ATM网中存在虚电路：<a target="_blank" rel="noopener" href="https://www.zhihu.com/question/434759624/answer/1631222452">参考文献</a></p>
<h5 id="数据报"><a href="#数据报" class="headerlink" title="数据报"></a>数据报</h5><p>作为通信子网用户的端系统发送一个报文时，在端系统中实现的高层协议先把报文拆成若干带有序号的数据单元，并在网络层加上地址等控制信息后形成数据报分组（即网络层的 PDU)。中间结点存储分组很短一段时间，找到最佳的路由后，尽快转发每个分组。不同的分组可以走不同的路径，也可以按不同的顺序到达目的结点。</p>
<p>我们用图2.5的例子来说明数据报服务的原理。假定主机A要向主机B发送分组。</p>
<p>1）主机A先将分组逐个发往与它直接相连的交换结点A，交换结点A缓存收到的分组。</p>
<p>2）然后查找自己的转发表。由于不同时刻的网络状态不同，因此转发表的内容可能不完全相同，所以有的分组转发给交换结点C，有的分组转发给交换结点D。</p>
<p>3）网络中的其他结点收到分组后，类似地转发分组，直到分组最终到达主机B。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210727103443.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210727103443.png';" /></details>

<p>当分组正在某-链路上传送时，分组并不占用网络的其他部分资源。因为采用存储转发技术，资源是共享的，所以主机A在发送分组时，主机B也可同时向其他主机发送分组。</p>
<p>通过上面的例子，我们可以总结出数据报服务具有如下特点:</p>
<p>1)发送分组前不需要建立连接。发送方可随时发送分组，网络中的结点可随时接收分组。</p>
<p>2)网络尽最大努力交付，传输不保证可靠性，所以可能丢失;为每个分组独立地选择路由，转发的路径可能不同，因而分组不一定按序到达目的结点。</p>
<p>3)发送的分组中要包括发送端和接收端的完整地址，以便可以独立传输。</p>
<p>4)分组在交换结点存储转发时，需要排队等候处理，这会带来一定的时延。通过交换结点的通信量较大或网络发生拥塞时，这种时延会大大增加，交换结点还可根据情况丢弃部分分组。</p>
<p>5)网络具有冗余路径，当某个交换结点或一条链路出现故障时，可相应地更新转发表，寻找另一条路径转发分组，对故障的适应能力强。</p>
<p>6)存储转发的延时一般较小， 提高了网络的吞吐量。</p>
<p>7)收发双方不独占某条链路，资源利用率较高。</p>
<h5 id="虚电路"><a href="#虚电路" class="headerlink" title="虚电路"></a>虚电路</h5><p>虚电路方式试图将数据报方式与电路交换方式结合起来，充分发挥两种方法的优点，以达到最佳的数据交换效果。在分组发送之前，要求在发送方和接收方建立一条逻辑上相连的虚电路，并且连接一旦建立，就固定了虚电路所对应的物理路径。与电路交换类似，整个通信过程分为三个阶段:虚电路建立、数据传输与虚电路释放。</p>
<p>在虚电路方式中，端系统每次建立虚电路时，选择一个未用过的虚电路号分配给该虚电路，以区别于本系统中的其他虚电路。在传送数据时，每个数据分组不仅要有分组号、校验和等控制信息，还要有它要通过的虚电路号，以区别于其他虚电路上的分组。在虚电路网络中的每个结点上都维持一张虚电路表，表中的每项记录了一个打开的虚电路的信息，包括在接收链路和发送链路上的虚电路号、前一结点和下一结点的标识。数据的传输是双向进行的，上述信息是在虚电路的建立过程中确定的。</p>
<p>虚电路方式的工作原理如图2.6所示。</p>
<p>1）为进行数据传输，主机A与主机B之间先建立一条逻辑通路，主机A发出一个特殊的“呼叫请求”分组，该分组通过中间结点送往主机B，若主机B同意连接，则发送“呼叫应答”分组予以确认。</p>
<p>2）虚电路建立后，主机A就可向主机B发送数据分组。当然，主机B也可在该虚电路上向主机A发送数据。</p>
<p>3）传送结束后主机A通过发送“释放请求”分组来拆除虚电路，逐段断开整个连接。</p>
<p>通过上面的例子，可以总结出虚电路服务具有如下特点:</p>
<p>1）虚电路通信链路的建立和拆除需要时间开销，对交互式应用和小量的短分组情况显得很浪费，但对长时间、频繁的数据交换效率较高。</p>
<p>2）虚电路的路由选择体现在连接建立阶段，连接建立后，就确定了传输路径。</p>
<p>3）虚电路提供了可靠的通信功能，能保证每个分组正确且有序到达。此外，还可以对两个数据端点的流量进行控制，当接收方来不及接收数据时，可以通知发送方暂缓发送。</p>
<p>4）虚电路有一个致命的弱点，即当网络中的某个结点或某条链路出现故障而彻底失效时，所有经过该结点或该链路的虚电路将遭到破坏。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210727103748.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210727103748.png';" /></details>

<p>5）分组首部不包含目的地址，包含的是虚电路标识符，相对于数据报方式，其开销小。</p>
<p>虚电路之所以是“虚”的，是因为这条电路不是专用的，每个结点到其他结点之间的链路可能同时有若干虚电路通过，也可能同时与多个结点之间建立虚电路。每条虚电路支持特定的两个端系统之间的数据传输，两个端系统之间也可以有多条虚电路为不同的进程服务，这些虚电路的实际路由可能相同也可能不同。</p>
<blockquote>
<p>注意，图2.6所示的数据传输过程是有确认的传输（由高层实现)，主机B收到分组后要发回相应分组的确认。网络中的传输是否有确认与网络层提供的两种服务没有任何关系。</p>
</blockquote>
<p>数据报服务和虚电路服务的比较见表2.1。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210727103909.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210727103909.png';" /></details>

<h3 id="传输介质"><a href="#传输介质" class="headerlink" title="传输介质"></a>传输介质</h3><h4 id="双绞线、同轴电缆、光纤与无线传输介质"><a href="#双绞线、同轴电缆、光纤与无线传输介质" class="headerlink" title="双绞线、同轴电缆、光纤与无线传输介质"></a>双绞线、同轴电缆、光纤与无线传输介质</h4><p>传输介质也称传输媒体，它是数据传输系统中发送设备和接收设备之间的物理通路。传输介质可分为 $\color{red}{\text{导向传输介质}}$ 和 $\color{red}{\text{非导向传输介质}}$ 。在导向传输介质中，电磁波被导向沿着固体媒介（铜线或光纤）传播，而非导向传输介质可以是 $\color{green}{\text{空气}}$ 、 $\color{green}{\text{真空}}$ 或 $\color{green}{\text{海水}}$ 等。</p>
<h5 id="color-red-text-双绞线"><a href="#color-red-text-双绞线" class="headerlink" title="$\color{red}{\text{双绞线}}$"></a>$\color{red}{\text{双绞线}}$</h5><p>双绞线是最常用的古老传输介质，它由两根采用一定规则并排绞合的、相互绝缘的铜导线组成。绞合可以减少对相邻导线的电磁干扰。为了进一步提高抗电磁干扰的能力，可在双绞线的外面再加上一层，即用金属丝编织成的屏蔽层，这就是 $\color{green}{\text{屏蔽双绞线}}$ (STP)。无屏蔽层的双绞线称为 $\color{green}{\text{非屏蔽双绞线}}$ （UTP)。它们的结构如图2.7所示。</p>
<p>双绞线的价格便宜，是最常用的传输介质之一，在局域网和传统电话网中普遍使用。双绞线的带宽取决于铜线的粗细和传输的距离。模拟传输和数字传输都可使用双绞线，其通信距离一般为几千米到数十千米。距离太远时，对于模拟传输，要用放大器放大衰减的信号;对于数字传输,要用中继器将失真的信号整形。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210727104043.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210727104043.png';" /></details>


<h5 id="color-red-text-同轴电缆"><a href="#color-red-text-同轴电缆" class="headerlink" title="$\color{red}{\text{同轴电缆}}$"></a>$\color{red}{\text{同轴电缆}}$</h5><p>同轴电缆由 $\color{green}{\text{内导体}}$ 、 $\color{green}{\text{绝缘层}}$ 、 $\color{green}{\text{网状编织屏蔽层}}$ 和 $\color{green}{\text{塑料外层}}$ 构成，如图2.8所示。按特性阻抗数值的不同，通常将同轴电缆分为两类:50$\Omega$同轴电缆和75$\Omega$同轴电缆。其中，50$\Omega$同轴电缆主要用于传送基带数字信号，又称 $\color{green}{\text{基带同轴电缆}}$ ，它在 $\color{green}{\text{局域网}}$ 中应用广泛;75$\Omega$同轴电缆主要用于传送宽带信号，又称 $\color{green}{\text{宽带同轴电缆}}$ ，主要用于 $\color{green}{\text{有线电视系统}}$ 。</p>
<p>由于外导体屏蔽层的作用，同轴电缆具有良好的抗干扰特性，被广泛用于传输较高速率的数据，其传输距离更远，但价格较双绞线贵。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210727104152.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210727104152.png';" /></details>

<h5 id="color-red-text-光纤"><a href="#color-red-text-光纤" class="headerlink" title="$\color{red}{\text{光纤}}$"></a>$\color{red}{\text{光纤}}$</h5><p>光纤通信就是利用光导纤维（简称光纤）传递光脉冲来进行通信。有光脉冲表示1，无光脉冲表示0。可见光的频率约为$10^8$MHz，因此光纤通信系统的带宽范围极大。</p>
<p>光纤主要由 $\color{green}{\text{纤芯}}$ 和 $\color{green}{\text{包层}}$ 构成(见图2.9)，纤芯很细，其直径只有8至100$\mu$m，光波通过纤芯进行传导，包层较纤芯有较低的折射率。当光线从高折射率的介质射向低折射率的介质时，其折射角将大于入射角。因此，只要入射角大于某个临界角度，就会出现 $\color{green}{\text{全反射}}$ ，即光线碰到包层时就会折射回纤芯，这个过程不断重复，光也就沿着光纤传输下去。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210727104315.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210727104315.png';" /></details>

<p>利用光的全反射特性，可以将从不同角度入射的多条光线在一根光纤中传输，这种光纤称为 $\color{green}{\text{多模光纤}}$ （见图2.10)，多模光纤的光源为 $\color{green}{\text{发光二极管}}$ 。光脉冲在多模光纤中传输时会逐渐展宽，造成失真，因此多模光纤只适合于 $\color{green}{\text{近距离传输}}$ 。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210727104338.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210727104338.png';" /></details>

<p>光纤的直径减小到只有一个光的波长时，光纤就像一根波导那样，可使光线一直向前传播，而不会产生多次反射，这样的光纤就是 $\color{green}{\text{单模光纤}}$ （见图2.11)。单模光纤的纤芯很细，直径只有几微米，制造成本较高。同时，单模光纤的光源为定向性很好的半导体激光器，因此单模光纤的衰减较小，可传输数公里甚至数十千米而不必采用中继器，适合远距离传输。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210727104506.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210727104506.png';" /></details>

<p>光纤不仅具有通信容量非常大的优点，还具有如下特点:</p>
<p>1） $\color{green}{\text{传输损耗小}}$ ，中继距离长，对远距离传输特别经济。</p>
<p>2） $\color{green}{\text{抗雷电和电磁干扰性能好}}$ 。这在有大电流脉冲干扰的环境下尤为重要。</p>
<p>3） $\color{green}{\text{无串音干扰}}$ ，保密性好，也不易被窃听或截取数据。</p>
<p>4） $\color{green}{\text{体积小}}$ ，重量轻。这在现有电缆管道已拥塞不堪的情况下特别有利。</p>
<h5 id="无线传输介质"><a href="#无线传输介质" class="headerlink" title="无线传输介质"></a>无线传输介质</h5><p>无线通信已广泛应用于移动电话领域，构成蜂窝式无线电话网。随着便携式计算机的出现，以及在军事、野外等特殊场合下移动通信联网的需要，促进了数字化移动通信的发展，现在无线局域网产品的应用已非常普遍。</p>
<p>(1） $\color{green}{\text{无线电波}}$ </p>
<p>无线电波具有较强的 $\color{green}{\text{穿透能力}}$ ，可以传输很长的距离，所以它被广泛应用于通信领域,如无线手机通信、计算机网络中的无线局域网（WLAN)等。因为无线电波使信号向 $\color{green}{\text{所有方向}}$ 散播，大大简化了通信连接，因此有效距离范围内的接收设备无须对准某个方向，就可与无线电波发射者进行通信连接,简化了通信连接。这也是无线电传输的最重要优点之一。</p>
<p>(2） $\color{green}{\text{微波、红外线和激光}}$ </p>
<p>目前高带宽的无线通信主要使用三种技术:微波、红外线和激光。它们都需要发送方和接收方之间存在一条视线(Line-of-sight）通路，有很强的 $\color{green}{\text{方向性}}$ ，都沿直线传播，有时统称这三者为视线介质。不同的是，红外通信和激光通信把要传输的信号分别转换为各自的信号格式，即红外光信号和激光信号，再直接在空间中传播。</p>
<blockquote>
<p>$\color{red}{\text{Q}}$:视线介质是什么？</p>
</blockquote>
<p>微波通信的频率较高，频段范围也很宽，载波频率通常为2～40GHz，因而通信信道的容量大。例如，一个带宽为2MHz的频段可容纳500条语音线路，若用来传输数字信号，数据率可达数兆比特/秒。与通常的无线电波不同，微波通信的信号是沿直线传播的，因此在地面的传播距离有限，超过一定距离后就要用中继站来接力。</p>
<p>卫星通信利用地球同步卫星作为中继来转发微波信号，可以克服地面微波通信距离的限制。三颗相隔120°的同步卫星几乎能覆盖整个地球表面，因而基本能实现全球通信。卫星通信的优点是通信容量大、距离远、覆盖广，缺点是保密性差、端到端传播时延长。</p>
<h4 id="物理层接口的特性"><a href="#物理层接口的特性" class="headerlink" title="物理层接口的特性"></a>物理层接口的特性</h4><p>物理层考虑的是如何在连接到各种计算机的传输媒体上传输数据比特流，而不指具体的传输媒体。网络中的硬件设备和传输介质的种类繁多，通信方式也各不相同。物理层应尽可能屏蔽这些差异，让数据链路层感觉不到这些差异，使数据链路层只需考虑如何完成本层的协议和服务。<br>物理层的主要任务可以描述为确定与传输媒体的接口有关的一些特性:</p>
<p>1） $\color{red}{\text{机械特性}}$ 。指明接口所用接线器的 $\color{green}{\text{形状}}$ 和 $\color{green}{\text{尺寸}}$ 、 $\color{green}{\text{引脚数目}}$ 和 $\color{green}{\text{排列}}$ 、 $\color{green}{\text{固定}}$ 和 $\color{green}{\text{锁定装置}}$ 等。</p>
<p>2） $\color{red}{\text{电气特性}}$ 。指明在接口电缆的各条线上出现的 $\color{green}{\text{电压的范围}}$ 。</p>
<p>3） $\color{red}{\text{功能特性}}$ 。指明某条线上出现的某一电平的电压表示何种 $\color{green}{\text{意义}}$ 。</p>
<p>4） $\color{red}{\text{过程特性}}$ 。或称规程特性。指明对于不同功能的各种可能事件的 $\color{green}{\text{出现顺序}}$ 。常用的物理层接口标准有EIARS-232-C、ADSL和 SONET/SDH等。</p>
<h3 id="物理层设备"><a href="#物理层设备" class="headerlink" title="物理层设备"></a>物理层设备</h3><h4 id="中继器"><a href="#中继器" class="headerlink" title="中继器"></a>中继器</h4><p>中继器的主要功能是将信号整形并放大再转发出去，以消除信号经过一长段电缆后而产生的失真和衰减，使信号的波形和强度达到所需要的要求，进而扩大网络传输的距离。其原理是信号再生（而非简单地将衰减的信号放大)。中继器有两个端口，数据从一个端口输入，再从另一个端口发出。端口仅作用于信号的电气部分，而不管是否有错误数据或不适于网段的数据。</p>
<p>中继器是用来扩大网络规模的最简单廉价的互联设备。中继器两端的网络部分是 $\color{green}{\text{网段}}$ 而不是 $\color{green}{\text{子网}}$ ，使用中继器连接的几个网段仍然是一个局域网。中继器若出现故障，对相邻两个网段的工作都将产生影响。由于中继器工作在物理层，因此它不能连接两个具有不同速率的局域网。</p>
<ul>
<li> $\color{red}{\text{Q}}$： 一个局域网，网段，子网，都是在讲些啥</li>
</ul>
<blockquote>
<p>注意:如果某个网络设备具有存储转发的功能，那么可以认为它能连接两个不同的协议;如果该网络设备没有存储转发功能，那么认为它不能连接两个不同的协议。中继器没有 $\color{green}{\text{存储转发}}$ 功能，因此它不能连接两个速率不同的网段，中继器两端的网段一定要使用 $\color{green}{\text{同一个协议}}$ 。</p>
</blockquote>
<p>从理论上讲，中继器的使用数目是无限的，网络因而也可以无限延长。但事实上这不可能，因为网络标准中对信号的延迟范围做了具体的规定，中继器只能在此规定范围内进行有效的工作，否则会引起网络故障。例如，在采用粗同轴电缆的10BASE5 以太网规范中，互相串联的 $\color{green}{\text{中继器}}$ 的个数不能超过 $\color{green}{\text{4}}$ 个，而且用4个中继器串联的 $\color{green}{\text{5}}$ 段 $\color{green}{\text{通信介质}}$ 中只有 $\color{green}{\text{3}}$ 段可以挂接 $\color{green}{\text{计算机}}$ ，其余两段只能用作扩展通信范围的链路段，不能挂接计算机。这就是所谓的“5-4-3规则”。</p>
<blockquote>
<p>注意:放大器和中继器都起放大作用，只不过 $\color{red}{\text{放大器}}$ 放大的是 $\color{green}{\text{模拟信号}}$ ，原理是将衰减的信号放大，而 $\color{red}{\text{中继器}}$ 放大的是 $\color{green}{\text{数字信号}}$ ，原理是将衰减的信号整形再生。</p>
</blockquote>
<h4 id="集线器"><a href="#集线器" class="headerlink" title="集线器"></a>集线器</h4><p>$\color{red}{\text{集线器}}$ （Hub）实质上是一个 $\color{green}{\text{多端口}}$ 的中继器。当Hub 工作时，一个端口接收到数据信号后，由于信号在从端口到Hub 的传输过程中已有衰减，所以Hub便将该信号进行整形放大，使之再生（恢复）到发送时的状态，紧接着转发到其他所有（除输入端口外）处于工作状态的端口。如果同时有两个或多个端口输入，那么输出时会发生冲突，致使这些数据都无效。从 Hub的工作方式可以看出，它在网络中只起信号放大和转发作用，目的是扩大网络的传输范围，而不具备信号的定向传送能力，即信号传输的方向是固定的，是一个标准的 $\color{green}{\text{共享式设备}}$ 。</p>
<p>Hub主要使用双绞线组建共享网络，是从服务器连接到桌面的最经济方案。在交换式网络中，Hub直接与交换机相连，将交换机端口的数据送到桌面上。使用Hub组网灵活，它把所有结点的通信集中在以其为中心的结点上，对结点相连的工作站进行集中管理，不让出问题的工作站影响整个网络的正常运行，并且用户的加入和退出也很自由。由Hub组成的网络是共享式网络，但逻辑上仍是一个 $\color{green}{\text{总线网}}$ 。Hub的每个端口连接的网络部分是同一个网络的不同网段，同时Hub也只能在半双工状态下工作，网络的吞吐率因而受到限制。</p>
<blockquote>
<p>注意:多台计算机必然会发生同时通信的情形，因此集线器不能分割冲突域，所有集线器的端口都属于同一个 $\color{green}{\text{冲突域}}$ 。集线器在一个时钟周期中只能传输一组信息，如果一台集线器连接的机器数目较多，且多台机器经常需要同时通信，那么将导致信息碰撞，使得集线器的工作效率很差。比如，一个带宽为10Mb/s的集线器上连接了8台计算机，当这8台计算机同时工作时，每台计算机真正所拥有的带宽为10/8Mb/s = 1.25Mb/s。</p>
</blockquote>
<h3 id="本章小结及疑难点"><a href="#本章小结及疑难点" class="headerlink" title="本章小结及疑难点"></a>本章小结及疑难点</h3><h4 id="传输媒体是物理层吗-传输媒体和物理层的主要区别是什么"><a href="#传输媒体是物理层吗-传输媒体和物理层的主要区别是什么" class="headerlink" title="传输媒体是物理层吗?传输媒体和物理层的主要区别是什么?"></a>传输媒体是物理层吗?传输媒体和物理层的主要区别是什么?</h4><p>传输媒体并不是物理层。由于传输媒体在物理层的下面，而物理层是体系结构的第一层，因此有时称传输媒体为0层。在传输媒体中传输的是信号，但传输媒体并不知道所传输的信号代表什么。也就是说，传输媒体不知道所传输的信号什么时候是1什么时候是0。但物理层由于规定了电气特性，因此能够识别所传送的比特流。图2.12描述了上述概念。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210730184003.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210730184003.png';" /></details>


<h4 id="什么是基带传输、频带传输和宽带传输-三者的区别是什么"><a href="#什么是基带传输、频带传输和宽带传输-三者的区别是什么" class="headerlink" title="什么是基带传输、频带传输和宽带传输?三者的区别是什么?"></a>什么是基带传输、频带传输和宽带传输?三者的区别是什么?</h4><p>在计算机内部或在相邻设备之间近距离传输时，可以不经过调制就在信道上直接进行的传输方式称为 $\color{green}{\text{基带传输}}$ 。它通常用于局域网。数字基带传输就是在信道中直接传输数字信号，且传输媒体的整个带宽都被基带信号占用，双向地传输信息。最简单的方法是用两个高低电平来表示二.进制数字，常用的编码方法有不归零编码和曼彻斯特编码。例如，要传输1010，低电平代表0，高电平代表1，那么在基带传输下，1010需要向通信线路传输（高、低、高、低电平)。用数字信号对特定频率的载波进行调制（数字调制)，将其变成适合于传送的信号后再进行传输，这种传输方式就是频带传输。远距离传输或无线传输时，数字信号必须用频带传输技术进行传输。利用频带传输，不仅解决了电话系统传输数字信号的问题，而且可以实现多路复用，进而提高传输信道的利用率。同样传输1010，经过调制，一个码元对应4个二进制位，假设码元A代表1010，那么在模拟信道上传输码元A就相当于传输了1010，这就是频带传输。</p>
<p>借助频带传输，可将链路容量分解成两个或多个信道，每个信道可以携带不同的信号，这就是 $\color{green}{\text{宽带传输}}$ 。宽带传输中所有的信道能同时互不干扰地发送信号，链路容量大大增加。比如把信道进行频分复用，划分为2条互不相关的子信道，分别在两条子信道上同时进行频带传输，链路容量就大大增加了，这就是宽带传输。</p>
<!-- > 宽带传输是频带传输的一种 -->

<h4 id="如何理解同步和异步-什么是同步通信和异步通信"><a href="#如何理解同步和异步-什么是同步通信和异步通信" class="headerlink" title="如何理解同步和异步?什么是同步通信和异步通信?"></a>如何理解同步和异步?什么是同步通信和异步通信?</h4><p>在计算机网络中，同步(Synchronous）的意思很广泛，没有统一的定义。例如，协议的三个要素之一就是“同步”。在网络编程中常提到的“同步”则主要指某函数的执行方式，即函数调用者需等待函数执行完后才能进入下一步。异步(Asynchronous）可简单地理解为“非同步”。</p>
<p>在数据通信中，同步通信与异步通信区别较大。</p>
<p>同步通信的通信双方必须先建立同步，即双方的时钟要调整到同一个频率。收发双方不停地发送和接收连续的同步比特流。主要有两种同步方式:一种是全网同步，即用一个非常精确的主时钟对全网所有结点上的时钟进行同步;另一种是准同步，即各结点的时钟之间允许有微小的误差，然后采用其他措施实现同步传输。同步通信数据率较高，但实现的代价也较高。</p>
<p>异步通信在发送字符时，所发送的字符之间的时间间隔可以是任意的，但接收端必须时刻做好接收的准备。发送端可以在任意时刻开始发送字符，因此必须在每个字符开始和结束的地方加上标志，即开始位和停止位，以便使接收端能够正确地将每个字符接收下来。异步通信也可以帧作为发送的单位。这时，帧的首部和尾部必须设有一些特殊的比特组合，使得接收端能够找出一帧的开始（即帧定界)。异步通信的通信设备简单、便宜，但传输效率较低（因为标志的开销所占比例较大)。图2.13给出了以字符、帧为单位的异步通信示意图。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210730184534.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210730184534.png';" /></details>


<h4 id="奈氏准则和香农定理的主要区别是什么-这两个定理对数据通信的意义是什么"><a href="#奈氏准则和香农定理的主要区别是什么-这两个定理对数据通信的意义是什么" class="headerlink" title="奈氏准则和香农定理的主要区别是什么?这两个定理对数据通信的意义是什么?"></a>奈氏准则和香农定理的主要区别是什么?这两个定理对数据通信的意义是什么?</h4><p>奈氏准则指出，码元传输的速率是受限的，不能任意提高，否则接收端就不能正确判定码元所携带的比特是1还是0(因为存在码元之间的相互干扰)。</p>
<p>奈氏准则是在理想条件下推导出来的。在实际条件下，最高码元传输速率要比理想条件下得出的数值小很多。电信技术人员的任务就是要在实际条件下，寻找出较好的传输码元波形，将比特转换为较为合适的传输信号。</p>
<p>需要注意的是，奈氏准则并未限制信息传输速率（b/s)。要提高信息传输速率，就必须使每个传输的码元能够代表许多比特的信息，这就需要有很好的编码技术。但码元所载的比特数确定后,信道的极限数据率也就确定了。</p>
<p>香农定理给出了信息传输速率的极限，即对于一定的传输带宽（单位为Hz)和一定的信噪比，信息传输速率的上限就确定了，这个极限是不能突破的。要想提高信息传输速率，要么设法提高传输线路的带宽，要么设法提高所传信道的信噪比，此外没有其他任何办法。</p>
<p>香农定理告诉我们，若要得到无限大的信息传输速率，只有两个办法:要么使用无限大的传输带宽（这显然不可能)，要么使信号的信噪比无限大，即采用没有噪声的传输信道或使用无限大的发送功率（显然这也不可能)。注意，奈氏准则和香农定理中“带宽”的单位都是Hz。</p>
<h4 id="信噪比为S-N，为什么还要取对数10-log-10-S-N"><a href="#信噪比为S-N，为什么还要取对数10-log-10-S-N" class="headerlink" title="信噪比为S/N，为什么还要取对数10$log_{10}(S/N)$?"></a>信噪比为S/N，为什么还要取对数10$log_{10}(S/N)$?</h4><p>1）数字形式表示，即一般数值。如噪声功率为1，信号功率为100，信噪比为100/1 = 100。</p>
<p>2）以分贝形式表示，同样还是上面这些数字，以分贝形式表示的信噪比为10log1o(S/N) =10$log_{10}100$ = 20 dB</p>
<p>两者的区别在于，前者（数值）是没有单位的，后者必须加dB，代表分贝。两者数值上等价。采用分贝表示的原因是:很多时候，信号要比噪声强得多，比如信号比噪声强10亿倍，如果用数值表示的话，那么1后面有9个0，很容易丢失一个0。如果用分贝表示，那么仅为90dB,因此要简单得多，而且不容易出错。分贝对于表示特别大或特别小的数值极为有利，这种表示方式在电子通信领域用途很广。</p>
<h4 id="比特率和波特率和数据传输速率的区别"><a href="#比特率和波特率和数据传输速率的区别" class="headerlink" title="比特率和波特率和数据传输速率的区别"></a>比特率和波特率和数据传输速率的区别</h4><p>$\color{red}{\text{比特率}}$ (bit rate)又称传信率、信息传输速率(简称信息速率，information rate)。其定义是：通信线路(或系统)单位时间(每秒)内传输的信息量，即每秒能传输的二进制位数，通常用Rb表示，其单位是比特/秒(bit/s或b/s，英文缩略语为bps)。</p>
<p>$\color{red}{\text{波特率}}$ (Baud rate)又称传码率、码元传输速率(简称码元速率)、信号传输速率(简称信号速率，signaling rate)或调制速率。其定义是：通信线路(或系统)单位时间(每秒)内传输的码元( $\color{green}{\text{脉冲}}$ )个数；</p>
<p>$\color{red}{\text{数据传输率}}$ (data transfer rate)又称数据传输速率、数据传送率。其定义是：通信线路(或系统)单位时间(每秒)内传输的字符个数；或者单位时间(每秒)内传输的码组(字块)数或比特数。其单位是字符/秒；或者码组/秒、比特/秒(可见，当数据传输率用 $\color{green}{\text{“bit/s”}}$ 作单位时，即等于 $\color{green}{\text{比特率}}$ )。</p>
<p><a target="_blank" rel="noopener" href="https://blog.csdn.net/u011392772/article/details/51496067">参考文献</a></p>
<h4 id="MISC"><a href="#MISC" class="headerlink" title="MISC"></a>MISC</h4><p>通信领域的k，进位是1000</p>
<h5 id="电路交换和分组交换的对比"><a href="#电路交换和分组交换的对比" class="headerlink" title="电路交换和分组交换的对比"></a>电路交换和分组交换的对比</h5><ul>
<li>可以理解为赛车比赛，有十辆车，一个安全检车员匀速检查每辆车主的安全性，如果等10辆车都检查完了，再一起发车，检查完的车也要和没检查完的一起等到所有都检查完才能走；但检查一部车，发一部车，就不存在这样的阻塞（好像哪里不对-）</li>
<li>配合计组的多通道内存传输的图，理解时间重叠</li>
</ul>
<p><a href="/2021/07/19/%E6%80%BB%E7%BB%93-%E4%B9%A0%E9%A2%98-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C#%E5%88%86%E7%BB%84%E4%BA%A4%E6%8D%A2%E4%B8%8E%E6%8A%A5%E6%96%87%E4%BA%A4%E6%8D%A2%E7%9A%84%E7%AE%80%E5%8D%95%E5%AF%B9%E6%AF%94">参考习题</a></p>
<details><summary>电路交换和分组交换的对比</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20211004210359.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20211004210359.png';" /></details>
      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  

      
  
  
  <article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN">
    <link itemprop="mainEntityOfPage" href="http://ednow.github.io/2021/07/24/%E7%8E%8B%E9%81%93-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-ch1-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/">

    <span hidden itemprop="author" itemscope itemtype="http://schema.org/Person">
      <meta itemprop="image" content="/images/avatar.gif">
      <meta itemprop="name" content="ednow">
      <meta itemprop="description" content="">
    </span>

    <span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization">
      <meta itemprop="name" content="ednow">
    </span>
      <header class="post-header">
        <h2 class="post-title" itemprop="name headline">
          
            <a href="/2021/07/24/%E7%8E%8B%E9%81%93-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C-ch1-%E8%AE%A1%E7%AE%97%E6%9C%BA%E7%BD%91%E7%BB%9C%E4%BD%93%E7%B3%BB%E7%BB%93%E6%9E%84/" class="post-title-link" itemprop="url">王道-计算机网络-ch1-计算机网络体系结构</a>
        </h2>

        <div class="post-meta">
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-calendar"></i>
              </span>
              <span class="post-meta-item-text">发表于</span>

              <time title="创建时间：2021-07-24 09:08:58" itemprop="dateCreated datePublished" datetime="2021-07-24T09:08:58+08:00">2021-07-24</time>
            </span>
              <span class="post-meta-item">
                <span class="post-meta-item-icon">
                  <i class="far fa-calendar-check"></i>
                </span>
                <span class="post-meta-item-text">更新于</span>
                <time title="修改时间：2021-10-04 19:33:14" itemprop="dateModified" datetime="2021-10-04T19:33:14+08:00">2021-10-04</time>
              </span>
            <span class="post-meta-item">
              <span class="post-meta-item-icon">
                <i class="far fa-folder"></i>
              </span>
              <span class="post-meta-item-text">分类于</span>
                <span itemprop="about" itemscope itemtype="http://schema.org/Thing">
                  <a href="/categories/%E8%80%83%E7%A0%94/" itemprop="url" rel="index"><span itemprop="name">考研</span></a>
                </span>
            </span>

          

        </div>
      </header>

    
    
    
    <div class="post-body" itemprop="articleBody">

      
          <h1 id="王道"><a href="#王道" class="headerlink" title="王道"></a>王道</h1><h2 id="计算机网络体系结构"><a href="#计算机网络体系结构" class="headerlink" title="计算机网络体系结构"></a>计算机网络体系结构</h2><details><summary>计算机网络总考纲</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210917185520.jpg" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210917185520.png';" /></details>

<p>【考纲内容】</p>
<p>(一）计算机网络概述</p>
<p>计算机网络的概念、组成与功能;</p>
<p>计算机网络的分类</p>
<p>计算机网络的性能指标</p>
<p>(二）计算机网络体系结构与参考模型</p>
<p>计算机网络分层结构;计算机网络协议、接口、服务的概念</p>
<p>ISO/OSI参考模型和 TCP/IP模型</p>
<p>【复习提示】</p>
<p>本章主要介绍计算机网络体系结构的基本概念，读者可以在理解的基础上适当地记忆。重点掌握网络的分层结构（包括5层和7层结构)，尤其是ISO/OSI参考模型各层的功能及相关协议、接口和服务等概念。掌握有关网络的各种性能指标，特别是时延、带宽、速率和吞吐量等的计算。</p>
<p>OSI参考模型的功能</p>
<table>
<thead>
<tr>
<th>层</th>
<th>物理寻址</th>
<th>流量控制</th>
<th>差错校验</th>
<th>拥塞控制</th>
<th>其他</th>
</tr>
</thead>
<tbody><tr>
<td>物理层</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>处理信号通过介质的传输</td>
</tr>
<tr>
<td>数据链路层</td>
<td>$\color{green}{\checkmark}$</td>
<td>$\color{green}{\checkmark}$</td>
<td>$\color{green}{\checkmark}$</td>
<td>$\color{red}{\times}$(王道)</td>
<td>保证数据正确的顺序和完整性;链路连接的建立、拆除、分离;帧界定和帧同步;差错检测;物理寻址;数据重发</td>
</tr>
<tr>
<td>网络层</td>
<td></td>
<td></td>
<td></td>
<td>$\color{green}{\checkmark}$</td>
<td>控制报文通过网络的路由选择；提供面向连接和无连接的服务</td>
</tr>
<tr>
<td>传输层</td>
<td></td>
<td>$\color{green}{\checkmark}$</td>
<td></td>
<td>$\color{green}{\checkmark}$</td>
<td>仅提供面向连接的服务</td>
</tr>
<tr>
<td>会话层</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>表示层</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td></td>
</tr>
<tr>
<td>应用层</td>
<td></td>
<td></td>
<td></td>
<td></td>
<td>提供用户和网络的接</td>
</tr>
</tbody></table>
<p>(raw page:课本的页码，pp:pdf page, pdf书籍阅读器的index)</p>
<blockquote>
<p>(王相林.rp.159)$\color{red}{\text{流量控制}}$是对一条通信路径上的流量进行控制，其目的是保证发送者的发送速度不超过接收者的接收速度。流量控制只设计一个发送者和一个接收者，涉及两个节点，可以是相邻节点之间，也可以是端节点之间，分别对应$\color{green}{\text{数据链路层}}$的相邻节点之间的流量控制，以及$\color{green}{\text{运输层}}$的端节点之间的流量控制。流量控制只涉及两个网络节点，属于是局部控制。<br>(王相林.rp.159)$\color{red}{\text{拥塞控制}}$是对整个通信子网的流量进行控制，其目的是保证通信子网的流量与网络可提供资源相匹配，不出现网络系统性能恶化、甚至崩溃的局面。拥塞控制涉及网络中多个节点（主机、路由器、交换机），属于是全局控制。拥塞控制的最终目标是，防止由于网络中PDU过载而使网络的吞吐量下降，合理分配网络资源；避免死锁；匹配速度传输速度。拥塞控制涉及的网络层次有：$\color{green}{\text{数据链路层}}$、$\color{green}{\text{网络层}}$和$\color{green}{\text{运输层}}$，而这些层次上对拥塞控制产生影响的策略，见表5-3。<br>流量控制涉及的层次有数据链路层和运输层。流量控制一般是由接收方根据自己缓冲区的大小和接收数据的情况，给发送方反馈流量控制信息，也就是说，是由接收方控制发送方发送数据的速率。流量控制与拥寒控制是相关的，若局部的流量控制做得很好，可以对全局的拥塞控制起到缓解的作用。</p>
</blockquote>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210725152627.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210725152627.png';" /></details>


<p>五层协议栈（Five-layer Internet protocol stack）</p>
<table>
<thead>
<tr>
<th>名称(name)</th>
<th>协议（protocols)</th>
<th>分组名称（packet name）</th>
</tr>
</thead>
<tbody><tr>
<td>应用层（Application Layer）</td>
<td>HTTP（WEB文档的请求和传送）<br /> SMTP（电子邮件）<br /> FTP（端系统间文件传送）</td>
<td>报文（message）</td>
</tr>
<tr>
<td>传输层（Transport Layer）</td>
<td>TCP<br />   UDP</td>
<td>报文段(segment);报文段(TCP);用户数据报(UDP)</td>
</tr>
<tr>
<td>网络层（Network Layer）</td>
<td>IP层</td>
<td>数据报(datagram)</td>
</tr>
<tr>
<td>链路层（Link Layer）</td>
<td>以太网<br />    WiFi<br />               电缆接入网的DOCSIS协议</td>
<td>帧(frame)</td>
</tr>
<tr>
<td>物理层（physical Layer）</td>
<td></td>
<td>比特(bit)</td>
</tr>
</tbody></table>
<p>物理设备</p>
<table>
<thead>
<tr>
<th>设备名</th>
<th>工作层</th>
<th>描述</th>
</tr>
</thead>
<tbody><tr>
<td>集线器(HUB)</td>
<td>物理层</td>
<td>多端口中继器</td>
</tr>
<tr>
<td>交换机(Switch)</td>
<td>数据链路层</td>
<td>多端口网桥</td>
</tr>
<tr>
<td>路由器(Router)</td>
<td>网络层</td>
<td>实现了网络模型的下三层，即物理层、数据链路层和网络层</td>
</tr>
</tbody></table>
<h3 id="计算机网络概述"><a href="#计算机网络概述" class="headerlink" title="计算机网络概述"></a>计算机网络概述</h3><h4 id="计算机网络的概念"><a href="#计算机网络的概念" class="headerlink" title="计算机网络的概念"></a>计算机网络的概念</h4><p>一般认为，计算机网络是一个将分散的、具有独立功能的计算机系统，通过通信设备与线路连接起来，由功能完善的软件实现资源共享和信息传递的系统。简而言之，计算机网络就是一些$\color{green}{\text{互联的}}$、$\color{green}{\text{自治}}$的$\color{green}{\text{计算机系统}}$的$\color{green}{\text{集合}}$。</p>
<p>在计算机网络发展的不同阶段，人们对计算机网络给出了不同的定义，这些定义反映了当时网络技术发展的水平。这些定义可分为以下三类。</p>
<h5 id="广义观点"><a href="#广义观点" class="headerlink" title="广义观点"></a>广义观点</h5><p>这种观点认为，只要是能实现远程信息处理的系统或能进一步达到资源共享的系统，都是计算机网络。广义的观点定义了一个计算机通信网络，它在物理结构上具有计算机网络的雏形，但资源共享能力弱，是计算机网络发展的低级阶段。</p>
<h5 id="资源共享观点"><a href="#资源共享观点" class="headerlink" title="资源共享观点"></a>资源共享观点</h5><p>这种观点认为，计算机网络是“以能够相互共享资源的方式互联起来的自治计算机系统的集合”。该定义包含三层含义:①目的——资源共享;②组成单元——分布在不同地理位置的多台独立的“自治计算机”;③网络中的计算机必须遵循的统一规则——网络协议。该定义符合目前计算机网络的基本特征。</p>
<h5 id="用户透明性观点"><a href="#用户透明性观点" class="headerlink" title="用户透明性观点"></a>用户透明性观点</h5><p>这种观点认为，存在一个能为用户自动管理资源的网络操作系统，它能够调用用户所需要的资源，而整个网络就像一个大的计算机系统一样对用户是透明的。用户使用网络就像使用一台单一的超级计算机，无须了解网络的存在、资源的位置信息。用户透明性观点的定义描述了一个分布式系统，它是网络未来发展追求的目标。</p>
<h4 id="计算机网络的组成"><a href="#计算机网络的组成" class="headerlink" title="计算机网络的组成"></a>计算机网络的组成</h4><p>从不同的角度，可以将计算机网络的组成分为如下几类。</p>
<p>1)从组成部分上看，一个完整的计算机网络主要由$\color{green}{\text{硬件}}$、$\color{green}{\text{软件}}$、$\color{green}{\text{协议}}$三大部分组成，缺一不可。硬件主要由主机（也称端系统)、通信链路（如双绞线、光纤)、交换设备（如路由器、交换机等）和通信处理机（如网卡）等组成。软件主要包括各种实现资源共享的软件和方便用户使用的各种工具软件（如网络操作系统、邮件收发程序、FTP程序、聊天程序等)。软件部分多属于应用层。协议是计算机网络的核心，如同交通规则制约汽车驾驶一样，协议规定了网络传输数据时所遵循的规范。</p>
<p>2）从工作方式上看，计算机网络（这里主要指Internet，即因特网）可分为$\color{green}{\text{边缘部分}}$和$\color{green}{\text{核心部分}}$。边缘部分由所有连接到因特网上、供用户直接使用的主机组成，用来进行通信(如传输数据、音频或视频）和资源共享;核心部分由大量的网络和连接这些网络的路由器组成，它为边缘部分提供连通性和交换服务。图1.1给出了这两部分的示意图。</p>
<p>3）从功能组成上看，计算机网络由$\color{green}{\text{通信子网}}$和$\color{green}{\text{资源子网}}$组成。通信子网由各种传输介质、通信设备和相应的网络协议组成，它使网络具有数据传输、交换、控制和存储的能力，实现联网计算机之间的数据通信。资源子网是实现资源共享功能的设备及其软件的集合，向网络用户提供共享其他计算机上的硬件资源、软件资源和数据资源的服务。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210724094736.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210724094736.png';" /></details>

<h4 id="计算机网络的功能"><a href="#计算机网络的功能" class="headerlink" title="计算机网络的功能"></a>计算机网络的功能</h4><p>计算机网络的功能很多，现今的很多应用都与网络有关。主要有以下五大功能。</p>
<h5 id="数据通信"><a href="#数据通信" class="headerlink" title="数据通信"></a>数据通信</h5><p>它是计算机网络最基本和最重要的功能，用来实现联网计算机之间各种信息的传输，并将分散在不同地理位置的计算机联系起来，进行统一的调配、控制和管理。例如，文件传输、电子邮件等应用，离开了计算机网络将无法实现。</p>
<h5 id="资源共享"><a href="#资源共享" class="headerlink" title="资源共享"></a>资源共享</h5><p>资源共享可以是软件共享、数据共享，也可以是硬件共享。它使计算机网络中的资源互通有无、分工协作，从而极大地提高硬件资源、软件资源和数据资源的利用率。</p>
<h5 id="分布式处理"><a href="#分布式处理" class="headerlink" title="分布式处理"></a>分布式处理</h5><p>当计算机网络中的某个计算机系统负荷过重时，可以将其处理的某个复杂任务分配给网络中的其他计算机系统，从而利用空闲计算机资源以提高整个系统的利用率。</p>
<h5 id="提高可靠性"><a href="#提高可靠性" class="headerlink" title="提高可靠性"></a>提高可靠性</h5><p>计算机网络中的各台计算机可以通过网络互为替代机。</p>
<h5 id="负载均衡"><a href="#负载均衡" class="headerlink" title="负载均衡"></a>负载均衡</h5><p>将工作任务均衡地分配给计算机网络中的各台计算机。</p>
<p>除以上几大主要功能外，计算机网络还可以实现电子化办公与服务、远程教育、娱乐等功能，满足了社会的需求，方便了人们学习、工作和生活，具有巨大的经济效益。</p>
<h4 id="计算机网络的分类"><a href="#计算机网络的分类" class="headerlink" title="计算机网络的分类"></a>计算机网络的分类</h4><h5 id="按分布范围分类"><a href="#按分布范围分类" class="headerlink" title="按分布范围分类"></a>按分布范围分类</h5><p>1)广域网（WAN)。广域网的任务是提供长距离通信，运送主机所发送的数据，其覆盖范围通常是直径为几十千米到几千千米的区域，因而有时也称远程网。广域网是因特网的核心部分。连接广域网的各结点交换机的链路一般都是高速链路，具有较大的通信容量。</p>
<p>2）城域网(MAN)。城域网的覆盖范围可以跨越几个街区甚至整个城市，覆盖区域的直径范围是5~50km。城域网大多采用以太网技术，因此有时也常并入局域网的范围讨论。</p>
<p>3）局域网（LAN)。局域网一般用微机或工.作站通过高速线路相连，覆盖范围较小，通常是直径为几十米到几千米的区域。局域网在计算机配置的数量上没有太多的限制，少的可以只有两台，多的可达几百台。传统上，局域网使用广播技术，而广域网使用交换技术。</p>
<p>4）个人区域网(PAN)。个人区域网是指在个人工作的地方将消费电子设备（如平板电脑、智能手机等）用无线技术连接起来的网络，也常称为无线个人区域网（WPAN)，覆盖区域的直径约为10m。</p>
<blockquote>
<p>注意:若中央处理器之间的距离非常近(如仅1m的数量级或甚至更小)，则一般称为多处理器系统，而不称为计算机网络。</p>
</blockquote>
<h5 id="按传输技术分类"><a href="#按传输技术分类" class="headerlink" title="按传输技术分类"></a>按传输技术分类</h5><p>1)广播式网络。所有联网计算机都共享一个公共通信信道。当一台计算机利用共享通信信道发送报文分组时，所有其他的计算机都会“收听”到这个分组。接收到该分组的计算机将通过检查目的地址来决定是否接收该分组。局域网基本上都采用广播式通信技术，广域网中的无线、卫星通信网络也采用广播式通信技术。</p>
<p>2）点对点网络。每条物理线路连接一对计算机。若通信的两台主机之间没有直接连接的线路，则它们之间的分组传输就要通过中间结点进行接收、存储和转发，直至目的结点。是否采用分组存储转发与路由选择机制是点对点式网络与广播式网络的重要区别，广域网基本都属于点对点网络。</p>
<h5 id="按拓扑结构分类"><a href="#按拓扑结构分类" class="headerlink" title="按拓扑结构分类"></a>按拓扑结构分类</h5><p>网络拓扑结构是指由网中结点（路由器、主机等）与通信线路（网线）之间的几何关系（如总线形、环形)表示的网络结构，主要指通信子网的拓扑结构。</p>
<p>网络拓扑结构是指由网中结点（路由器、主机等）与通信线路（网线）之间的几何关系（如总线形、环形)表示的网络结构，主要指通信子网的拓扑结构。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210724095447.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210724095447.png';" /></details>

<p>1）总线形网络。用单根传输线把计算机连接起来。总线形网络的优点是建网容易、增/减结点方便、节省线路。缺点是重负载时通信效率不高、总线任意一处对故障敏感。</p>
<p>2）星形网络。每个终端或计算机都以单独的线路与中央设备相连。中央设备早期是计算机，现在一般是交换机或路由器。星形网络便于集中控制和管理，因为端用户之间的通信必须经过中央设备。缺点是成本高、中心设备对故障敏感。</p>
<p>3）环形网络。所有计算机接口设备连接成一个环。环形网络最典型的例子是$\color{green}{\text{令牌环局域网}}$。环可以是单环，也可以是双环，环中信号是单向传输的。</p>
<p>4）网状网络。一般情况下，每个结点至少有两条路径与其他结点相连，多用在广域网中。其有规则型和非规则型两种。其优点是可靠性高，缺点是控制复杂、线路成本高。以上4种基本的网络拓扑结构可以互联为更复杂的网络。</p>
<p>以上4种基本的网络拓扑结构可以互联为更复杂的网络。</p>
<h5 id="按使用者分类"><a href="#按使用者分类" class="headerlink" title="按使用者分类"></a>按使用者分类</h5><p>1）公用网（Public Network)。指电信公司出资建造的大型网络。“公用”的意思是指所有愿意按电信公司的规定交纳费用的人都可以使用这种网络，因此也称公众网。</p>
<p>2）专用网（Private Network)。指某个部门为满足本单位特殊业务的需要而建造的网络。这种网络不向本单位以外的人提供服务。例如铁路、电力、军队等部门的专用网。</p>
<h5 id="按交换技术分类"><a href="#按交换技术分类" class="headerlink" title="按交换技术分类"></a>按交换技术分类</h5><p>交换技术是指各台主机之间、各通信设备之间或主机与通信设备之间为交换信息所采用的数据格式和交换装置的方式。按交换技术可将网络分为如下几种。</p>
<p>1)电路交换网络。在源结点和目的结点之间建立一条专用的通路用于传送数据，包括建立连接、传输数据和断开连接三个阶段。最典型的电路交换网是传统电话网络。该类网络的主要特点是整个报文的比特流连续地从源点直达终点，好像是在一条管道中传送。优点是数据直接传送、时延小。缺点是线路利用率低、不能充分利用线路容量、不便于进行差错控制。</p>
<p>2)报文交换网络。用户数据加上源地址、目的地址、校验码等辅助信息，然后封装成报文。整个报文传送到相邻结点，全部存储后，再转发给下一个结点，重复这一过程直到到达目的结点。每个报文可以单独选择到达目的结点的路径。报文交换网络也称存储-转发网络，主要特点是整个报文先传送到相邻结点，全部存储后查找转发表，转发到下一个结点。优点是可以较为充分地利用线路容量，可以实现不同链路之间不同数据传输速率的转换，可以实现格式转换，可以实现一对多、多对一的访问，可以实现差错控制。缺点是增大了资源开销（如辅助信息导致处理时间和存储资源的开销)，增加了缓冲时延，需要额外的控制机制来保证多个报文的顺序不乱序，缓冲区难以管理（因为报文的大小不确定，接收方在接收到报文之前不能预知报文的大小)。</p>
<p>3）分组交换网络，也称包交换网络。其原理是，将数据分成较短的固定长度的数据块，在每个数据块中加上目的地址、源地址等辅助信息组成分组（包)，以存储-转发方式传输。其主要特点是单个分组（它只是整个报文的一部分）传送到相邻结点，存储后查找转发表，转发到下一个结点。除具备报文交换网络的优点外，分组交换网络还具有自身的优点:缓冲易于管理;包的平均时延更小，网络占用的平均缓冲区更少;更易于标准化;更适合应用。现在的主流网络基本上都可视为分组交换网络。</p>
<blockquote>
<p>报文交换和分组交换的区别：整个报文在网络中传递，分组交换报文被分割了</p>
</blockquote>
<h5 id="按传输介质分类"><a href="#按传输介质分类" class="headerlink" title="按传输介质分类"></a>按传输介质分类</h5><p>传输介质可分为有线和无线两大类，因此网络可以分为有线网络和无线网络。有线网络又分为双绞线网络、同轴电缆网络等。无线网络又可分为蓝牙、微波、无线电等类型。</p>
<h4 id="计算机网络的标准化工作及相关组织"><a href="#计算机网络的标准化工作及相关组织" class="headerlink" title="*计算机网络的标准化工作及相关组织"></a>*计算机网络的标准化工作及相关组织</h4><p>计算机网络的标准化对计算机网络的发展和推广起到了极为重要的作用。</p>
<p>因特网的所有标准都以RFC (Request For Comments）的形式在因特网上发布，但并非每个RFC都是因特网标准，RFC要上升为因特网的正式标准需经过以下4个阶段。</p>
<p>1)）因特网草案（ Internet Draft)。这个阶段还不是RFC文档。</p>
<p>2）建议标准（Proposed Standard)。从这个阶段开始就成为RFC文档。3）草案标准（Draft Standard)。</p>
<p>4）因特网标准(Internet Standard)。</p>
<p>此外，还有试验的RFC和提供信息的RFC。各种RFC之间的关系如图1.3所示。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210724095943.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210724095943.png';" /></details>

<p>在国际上，负责制定、实施相关网络标准的标准化组织众多，主要有如下几个:</p>
<ul>
<li>国际标准化组织(ISO)。其制定的主要网络标准或规范有OSI参考模型、HDLC等。</li>
<li>国际电信联盟(ITU)。其前身为国际电话电报咨询委员会(CCITT)，其下属机构ITU-T制定了大量有关远程通信的标准。</li>
<li>国际电气电子工程师协会（IEEE)。世界上最大的专业技术团体，由计算机和工程学专业人士组成。IEEE在通信领域最著名的研究成果是802标准。</li>
</ul>
<h4 id="计算机网络的性能指标"><a href="#计算机网络的性能指标" class="headerlink" title="计算机网络的性能指标"></a>计算机网络的性能指标</h4><p>性能指标从不同方面度量计算机网络的性能。常用的性能指标如下。</p>
<p>1）带宽（Bandwidth)。本来表示通信线路允许通过的信号频带范围，单位是赫兹（Hz)。而在计算机网络中，带宽表示网络的通信线路所能传送数据的能力，是数字信道所能传送的“最高数据传输速率”的同义语，单位是比特/秒( b/s）。</p>
<p>2）时延(Delay)。指数据（一个报文或分组）从网络（或链路）的一端传送到另一端所需要的总时间，它由4部分构成:发送时延、传播时延、处理时延和排队时延。</p>
<ul>
<li> $\color{green}{\text{发送时延}}$ 。结点将分组的所有比特推向（传输）链路所需的时间，即从发送分组的第一个比特算起，到该分组的最后一个比特发送完毕所需的时间，因此也称 $\color{green}{\text{传输时延}}$ 。计算公式为：发送时延=分组长度/信道宽度</li>
<li> $\color{green}{\text{传播时延}}$ 。电磁波在信道中传播一定的距离需要花费的时间，即一个比特从链路的一端传播到另一端所需的时间。计算公式为：传播时延=信道长度/电磁波在信道上的传播速率</li>
<li> $\color{green}{\text{处理时延}}$ 。数据在交换结点为存储转发而进行的一些必要的处理所花费的时间。例如，分析分组的首部、从分组中提取数据部分、进行差错检验或查找适当的路由等。</li>
<li> $\color{green}{\text{排队时延}}$ 。分组在进入路由器后要先在输入队列中排队等待处理。路由器确定转发端口后，还要在输出队列中排队等待转发，这就产生了排队时延。</li>
</ul>
<p>因此，数据在网络中经历的总时延就是以上4部分时延之和:</p>
<p>总时延一发送时延＋传播时延＋处理时延＋排队时延</p>
<blockquote>
<p>注意:做题时，排队时延和处理时延一般可忽略不计（除非题目另有说明)。另外，对于高速链路，提高的仅是数据发送速率而非比特在链路上的传播速率。提高数据的发送速率只是为了减少数据的发送时延。</p>
</blockquote>
<p>3）时延带宽积。指发送端发送的第一个比特即将到达终点时，发送端已经发出了多少个比特，因此又称以比特为单位的链路长度，即时延带宽积=传播时延×信道带宽。</p>
<p>如图1.4所示，考虑一个代表链路的圆柱形管道，其长度表示链路的传播时延，横截面积表示链路带宽，则时延带宽积表示该管道可以容纳的比特数量。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210724100452.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210724100452.png';" /></details>

<p>4)）往返时延(Round-Trip Time，RTT)。指从发送端发送数据开始，到发送端收到来自接收端的确认（接收端收到数据后立即发送确认)，总共经历的时延。在互联网中，往返时延还包括各中间结点的处理时延、排队时延及转发数据时的发送时延。</p>
<p>5）吞吐量(Throughput)。指单位时间内通过某个网络（或信道、接口）的数据量。吞吐量受网络带宽或网络额定速率的限制。</p>
<p>6）速率(Speed)。网络中的速率是指连接到计算机网络上的主机在数字信道上传送数据的速率，也称数据传输速率、数据率或比特率，单位为 bl/s （比特/秒)(或bit/s，有时也写为bps)。数据率较高时，可用kb/s (k = 103)、Mb/s (M = 10°）或Gb/s (G = 10’)表示。在计算机网络中，通常把最高数据传输速率称为带宽。</p>
<p>7)信道利用率。指出某一信道有百分之多少的时间是有数据通过的，即信道利用率=有数据通过时间/(有+无)数据通过时间。</p>
<blockquote>
<p>结合计网期末考的概念：传输时延，重点在<code>输送</code>，传播实验，重点在<code>广播</code>（$\color{red}{\text{Q}}$：好像王道的传播和自顶向下的传播不是指一个东西）<br>注意带宽的单位是$\text{比特}/s=b/s$,不可以是$\text{字节}/s=B/s$！！</p>
</blockquote>
<p>网络的<a target="_blank" rel="noopener" href="https://blog.csdn.net/wanghaobo920/article/details/11877589">报文流和字节流</a>，程序中的<a target="_blank" rel="noopener" href="https://zhidao.baidu.com/question/89316568.html">字符流和字节流</a></p>
<p><a target="_blank" rel="noopener" href="https://www.javapedia.net/JMS/1106">字节流和报文流</a></p>
<p><a target="_blank" rel="noopener" href="https://bbs.csdn.net/topics/100037179">jms和socket</a></p>
<p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/hzq3554055/p/12004633.html">jms和amqp和rabbitmq的区别</a></p>
<h3 id="计算机网络体系结构与参考模型"><a href="#计算机网络体系结构与参考模型" class="headerlink" title="计算机网络体系结构与参考模型"></a>计算机网络体系结构与参考模型</h3><h4 id="计算机网络分层结构"><a href="#计算机网络分层结构" class="headerlink" title="计算机网络分层结构"></a>计算机网络分层结构</h4><p>两个系统中实体间的通信是一个很复杂的过程，为了降低协议设计和调试过程的复杂性，也为了便于对网络进行研究、实现和维护，促进标准化工作，通常对计算机网络的体系结构以分层的方式进行建模。</p>
<p>我们把计算机网络的各层及其协议的集合称为网络的体系结构（Architecture)。换言之，计算机网络的体系结构就是这个计算机网络及其所应完成的功能的精确定义，它是计算机网络中的层次、各层的协议及层间接口的集合。需要强调的是，这些功能究竟是用何种硬件或软件完成的,则是一个遵循这种体系结构的实现(Implementation)问题。体系结构是抽象的，而实现是具体的，是真正在运行的计算机硬件和软件。</p>
<p>计算机网络的体系结构通常都具有可分层的特性，它将复杂的大系统分成若干较容易实现的层次。分层的基本原则如下:</p>
<p>1）每层都实现一种相对独立的功能，降低大系统的复杂度。</p>
<p>2）各层之间界面自然清晰，易于理解，相互交流尽可能少。</p>
<p>3）各层功能的精确定义独立于具体的实现方法，可以采用最合适的技术来实现。</p>
<p>4）保持下层对上层的独立性，上层单向使用下层提供的服务。</p>
<p>5）整个分层结构应能促进标准化工作。</p>
<p>由于分层后各层之间相对独立，灵活性好，因而分层的体系结构易于更新（替换单个模块)易于调试，易于交流，易于抽象，易于标准化。但层次越多，有些功能在不同层中难免重复出现，产生额外的开销，导致整体运行效率越低。层次越少，就会使每层的协议太复杂。因此，在分层时应考虑层次的清晰程度与运行效率间的折中、层次数量的折中。</p>
<p>依据一定的规则，将分层后的网络从低层到高层依次称为第1层、第2层……第n层，通常还为每层取一个特定的名称，如第1层的名称为$\color{green}{\text{物理层}}$。</p>
<p>在计算机网络的分层结构中，第n层中的活动元素通常称为第n层实体。具体来说，实体指任何可发送或接收信息的硬件或软件进程，通常是一个特定的软件模块。不同机器上的同一层称为$\color{green}{\text{对等层}}$，同一层的实体称为$\color{green}{\text{对等实体}}$。第n层实体实现的服务为第n +1层所利用。在这种情况下，第n层称为$\color{green}{\text{服务提供者}}$，第n+1层则服务于$\color{green}{\text{用户}}$。</p>
<p>每一层还有自己传送的数据单位，其名称、大小、含义也各有不同。</p>
<p>在计算机网络体系结构的各个层次中，每个报文都分为两部分:一是数据部分，即SDU;二是控制信息部分，即 PCI，它们共同组成PDU。</p>
<p>服务数据单元(SDU):为完成用户所要求的功能而应传送的数据。第n层的服务数据单元记为n-SDU。</p>
<p>协议控制信息(PCI):控制协议操作的信息。第n层的协议控制信息记为n-PCI。</p>
<p>协议数据单元(PDU):对等层次之间传送的数据单位称为该层的PDU。第n层的协议数据单元记为n-PDU。在实际的网络中，每层的协议数据单元都有一个通俗的名称，如物理层的PDU称为$\color{green}{\text{比特}}$，数据链路层的PDU称为$\color{green}{\text{帧}}$，网络层的PDU称为$\color{green}{\text{分组}}$，传输层的PDU称为$\color{green}{\text{报文段}}$。</p>
<p>在各层间传输数据时，把从第n+1层收到的PDU作为第n层的SDU，加上第n层的PCI,就变成了第n层的PDU，交给第n-1层后作为SDU发送，接收方接收时做相反的处理，因此可知三者的关系为n-SDU + n-PCI = n-PDU=(n- 1)-SDU，其变换过程如图1.5所示。</p>
<details><summary>图1.5网络各层数据单元的联系</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210724162854.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210724162854.png';" /></details>

<p>1)第n层的实体不仅要使用第n-1层的服务来实现自身定义的功能，还要向第n+1层提供本层的服务，该服务是第n层及其下面各层提供的服务总和。</p>
<p>2）最低层只提供服务，是整个层次结构的基础;中间各层既是下一层的服务使用者，又是上一层的服务提供者;最高层面向用户提供服务。</p>
<p>3)上一层只能通过相邻层间的接口使用下一层的服务，而不能调用其他层的服务;下一层所提供服务的实现细节对上一层透明。</p>
<p>4）两台主机通信时，对等层在逻辑上有一条直接信道，表现为不经过下层就把信息传送到对方。</p>
<blockquote>
<p>也就是在物理上还是经过了下一层</p>
</blockquote>
<h4 id="计算机网络协议、接口、服务的概念"><a href="#计算机网络协议、接口、服务的概念" class="headerlink" title="计算机网络协议、接口、服务的概念"></a>计算机网络协议、接口、服务的概念</h4><h5 id="协议"><a href="#协议" class="headerlink" title="协议"></a>协议</h5><p>协议，就是规则的集合。在网络中要做到有条不紊地交换数据，就必须遵循一些事先约定好的规则。这些规则明确规定了所交换的数据的格式及有关的同步问题。这些为进行网络中的数据交换而建立的规则、标准或约定称为网络协议(Network Protocol)，它是控制两个(或多个）对等实体进行通信的规则的集合，是水平的。不对等实体之间是没有协议的，比如用TCP/IP 协议栈通信的两个结点，结点A的传输层和结点B的传输层之间存在协议，但结点A的传输层和结点B的网络层之间不存在协议。网络协议也简称为协议。</p>
<p>协议由语法、语义和同步三部分组成。语法规定了传输数据的格式;语义规定了所要完成的功能，即需要发出何种控制信息、完成何种动作及做出何种应答;同步规定了执行各种操作的条件、时序关系等，即事件实现顺序的详细说明。一个完整的协议通常应具有线路管理（建立、释放连接)、差错控制、数据转换等功能。</p>
<h5 id="接口"><a href="#接口" class="headerlink" title="接口"></a>接口</h5><p>接口是同一结点内相邻两层间交换信息的连接点，是一个系统内部的规定。每层只能为紧邻的层次之间定义接口，不能跨层定义接口。在典型的接口上，同一结点相邻两层的实体通过服务访问点(Service Access Point，SAP）进行交互。服务是通过SAP提供给上层使用的，第n层的SAP就是第n+1层可以访问第n层服务的地方。每个SAP都有一个能够标识它的地址。SAP是一个抽象的概念，它实际上是一个逻辑接口（类似于邮政信箱)，但和通常所说的两个设备之间的硬件接口是很不一样的。</p>
<h5 id="服务"><a href="#服务" class="headerlink" title="服务"></a>服务</h5><p>服务是指下层为紧邻的上层提供的功能调用，它是垂直的。对等实体在协议的控制下，使得本层能为上一层提供服务，但要实现本层协议还需要使用下一层所提供的服务。</p>
<p>上层使用下层所提供的服务时必须与下层交换一些命令，这些命令在OSI参考模型中称为服务原语。OSI参考模型将原语划分为4类:(原语的主语用绿色高亮)</p>
<p>1）请求(Request)。由$\color{green}{\text{服务用户}}$发往服务提供者，请求完成某项工作。</p>
<p>2）指示(Indication)。由$\color{green}{\text{服务提供者}}$发往服务用户，指示用户做某件事情。</p>
<p>3）响应(Response)。由$\color{green}{\text{服务用户}}$发往服务提供者，作为对指示的响应。</p>
<p>4）证实(Confirmation)。由$\color{green}{\text{服务提供者}}$发往服务用户，作为对请求的证实。</p>
<p>这4类原语用于不同的功能，如建立连接、传输数据和断开连接等。有应答服务包括全部4类原语，而无应答服务则只有请求和指示两类原语。<br>4类原语的关系如图1.6所示。</p>
<details><summary>4类原语的关系</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210724163436.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210724163436.png';" /></details>

<p>一定要注意，协议和服务在概念上是不一样的。首先，只有本层协议的实现才能保证向上一层提供服务。本层的服务用户只能看见服务而无法看见下面的协议，即下面的协议对上层的服务用户是透明的。其次，协议是“$\color{green}{\text{水平的}}$”，即协议是控制对等实体之间通信的规则。但服务是“$\color{green}{\text{垂直的}}$”，即服务是由下层通过层间接口向上层提供的。另外，并非在一层内完成的全部功能都称为服务，只有那些能够被高一层实体“看得见”的功能才称为服务。</p>
<p>协议、接口、服务三者之间的关系如图1.7所示。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210724163750.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210724163750.png';" /></details>

<p>计算机网络提供的服务可按以下三种方式分类。</p>
<p>( 1)面向连接服务与无连接服务</p>
<p>在面向连接服务中，通信前双方必须先建立连接，分配相应的资源（如缓冲区)，以保证通信能正常进行，传输结束后释放连接和所占用的资源。因此这种服务可以分为连接建立、数据传输和连接释放三个阶段。例如TCP就是一种面向连接服务的协议。</p>
<p>在无连接服务中，通信前双方不需要先建立连接，需要发送数据时可直接发送，把每个带有目的地址的包（报文分组）传送到线路上，由系统选定路线进行传输。这是一种不可靠的服务。这种服务常被描述为“尽最大努力交付”(Best-Effort-Delivery)，它并不保证通信的可靠性。例如$\color{green}{\text{IP}}$、$\color{green}{\text{UDP}}$就是一种无连接服务的协议。</p>
<p>(2)可靠服务和不可靠服务</p>
<p>可靠服务是指网络具有纠错、检错、应答机制，能保证数据正确、可靠地传送到目的地。</p>
<p>不可靠服务是指网络只是尽量正确、可靠地传送，而不能保证数据正确、可靠地传送到目的地，是一种尽力而为的服务。</p>
<p>对于提供不可靠服务的网络，其网络的正确性、可靠性要由应用或用户来保障。例如，用户收到信息后要判断信息的正确性，如果不正确，那么用户要把出错信息报告给信息的发送者，以便发送者采取纠正措施。通过用户的这些措施，可以把不可靠的服务变成可靠的服务。</p>
<blockquote>
<p>注意:在一层内完成的全部功能并非都称为服务，只有那些能够被高一层实体“看得见”的功能才能称为服务。</p>
</blockquote>
<p>( 3）有应答服务和无应答服务</p>
<p>有应答服务是指接收方在收到数据后向发送方给出相应的应答，该应答由传输系统内部自动实现，而不由用户实现。所发送的应答既可以是肯定应答，也可以是否定应答，通常在接收到的数据有错误时发送否定应答。例如，文件传输服务就是一种有应答服务。</p>
<p>无应答服务是指接收方收到数据后不自动给出应答。若需要应答，则由高层实现。例如，对于WWW服务，客户端收到服务器发送的页面文件后不给出应答。</p>
<h4 id="ISO-OSI参考模型和TCP-IP模型"><a href="#ISO-OSI参考模型和TCP-IP模型" class="headerlink" title="ISO/OSI参考模型和TCP/IP模型"></a>ISO/OSI参考模型和TCP/IP模型</h4><h5 id="OSI参考模型"><a href="#OSI参考模型" class="headerlink" title="OSI参考模型"></a>OSI参考模型</h5><p>国际标准化组织(ISO）提出的网络体系结构模型，称为开放系统互连参考模型（OSI/RM),通常简称为OSI参考模型。OSI参考模型有7层，自下而上依次为物理层、数据链路层、网络层、传输层、会话层、表示层、应用层。低三层统称为通信子网，它是为了联网而附加的通信设备，完成数据的传输功能;高三层统称为资源子网，它相当于计算机系统，完成数据的处理等功能。传输层承上启下。OSI参考模型的层次结构如图1.8所示。</p>
<details><summary>图1.8 OSI参考模型的层次结构</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210724164119.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210724164119.png';" /></details>

<p>下面详述OSI参考模型各层的功能。</p>
<p>(1）物理层（Physical Layer)</p>
<p>物理层的传输单位是比特，任务是透明的传输比特流，功能是在物理媒体上为数据端设备透明地传输原始比特流。</p>
<p>物理层主要定义数据终端设备（DTE）和数据通信设备（DCE）的物理与逻辑连接方法，所以物理层协议也称物理层接口标准。由于在通信技术的早期阶段,通信规则称为规程(Procedure),因此物理层协议也称物理层规程。</p>
<p>物理层接口标准很多，如EIA-232C、EIA/TIA RS-449、CCITT的X.21等。在计算机网络的复习过程中，不要忽略对各层传输协议的记忆，到了后期，读者对数据链路层、网络层、传输层和应用层的协议会比较熟悉，但往往容易忽视物理层的协议。</p>
<p>图1.9表示的是两个通信结点及它们间的一段通信链路，物理层主要研究以下内容:</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210724164314.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210724164314.png';" /></details>

<p>${\textstyle\unicode{x2460}}$  通信链路与通信结点的连接需要一些电路接口，物理层规定了这些接口的一些参数，如机械形状和尺寸、交换电路的数量和排列等，例如，笔记本电脑上的网线接口，就是物理层规定的内容之一。</p>
<p>${\textstyle\unicode{x2461}}$  物理层也规定了通信链路上传输的信号的意义和电气特征。例如物理层规定信号A代表数字0，那么当结点要传输数字0时，就会发出信号A，当结点接收到信号A时，就知道自己接收到的实际上是数字0。</p>
<blockquote>
<p>注意，传输信息所利用的一些物理媒体，如双绞线、光缆、无线信道等，并不在物理层协议之内而在物理层协议下面。因此，有人把物理媒体当作第О层。</p>
</blockquote>
<p>(2）数据链路层(Data Link Layer)</p>
<p>数据链路层的传输单位是帧，任务是将网络层传来的P数据报组装成帧。数据链路层的功能可以概括为成帧、差错控制、流量控制和传输管理等。</p>
<p>由于外界噪声的干扰，原始的物理连接在传输比特流时可能发生错误。如图1.8所示，左边结点想向右边结点传输数字0，于是发出了信号A;但传输过程中受到干扰，信号A变成了信号B，而信号B又刚好代表1，右边结点接收到信号B时，就会误以为左边结点传送了数字1，从而发生差错。两个结点之间如果规定了数据链路层协议，那么可以检测出这些差错，然后把收到的错误信息丢弃，这就是差错控制功能。</p>
<p>如图1.9所示，在两个相邻结点之间传送数据时，由于两个结点性能的不同，可能结点A发送数据的速率会比结点B接收数据的速率快，如果不加控制，那么结点B就会丢弃很多来不及接收的正确数据，造成传输线路效率的下降。流量控制可以协调两个结点的速率，使结点A发送数据的速率刚好是结点B可以接收的速率。</p>
<p>广播式网络在数据链路层还要处理新的问题，即如何控制对共享信道的访问。数据链路层的一个特殊的子层—介质访问子层，就是专门处理这个问题的。</p>
<p>典型的数据链路层协议有SDLC、HDLC、PPP、STP和帧中继等。</p>
<p>(3）网络层(Network Layer)</p>
<p>网络层的传输单位是数据报，它关心的是通信子网的运行控制，主要任务是把网络层的协议数据单元（分组）从源端传到目的端，为分组交换网上的不同主机提供通信服务。关键问题是对分组进行路由选择，并实现流量控制、拥塞控制、差错控制和网际互联等功能。</p>
<p>如图1.10所示，结点A向结点B传输一个分组时，既可经过边a、c、g，也可经过边b、h，有很多条可以选择的路由，而网络层的作用就是根据网络的情况，利用相应的路由算法计算出一条合适的路径，使这个分组可以顺利到达结点B。</p>
<details><summary>图1.10某网络结构图</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210724164514.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210724164514.png';" /></details>

<p>流量控制与数据链路层的流量控制含义一样,都是协调A的发送速率和B的接收速率。</p>
<p>差错控制是通信两结点之间约定的特定检错规则，如奇偶校验码，接收方根据这个规则检查接收到的分组是否出现差错，如果出现了差错，那么能纠错就纠错，不能纠错就丢弃，确保向上层提交的数据都是无误的。</p>
<p>如果图1.10中的结点都处于来不及接收分组而要丢弃大量分组的情况,那么网络就处于拥塞状态，拥塞状态使得网络中的两个结点无法正常通信。网络层要采取一定的措施来缓解这种拥塞，这就是拥塞控制。</p>
<p>因特网是一个很大的互联网，它由大量异构网络通过路由器（(Router）相互连接起来。因特网的主要网络层协议是无连接的网际协议（Internet Protocol，P)和许多路由选择协议，因此因特网的网络层也称网际层或IP层。</p>
<p>注意，网络层中的“网络”一词并不是我们通常谈及的具体网络，而是在计算机网络体系结构中使用的专有名词。</p>
<p>网络层的协议有IP、IPX、ICMP、IGMP、ARP、RARP和OSPF等。</p>
<p>(4)传输层（Transport Layer)</p>
<p>传输层也称运输层，传输单位是报文段(TCP）或用户数据报（UDP)，传输层负责主机中两个进程之间的通信，功能是为端到端连接提供可靠的传输服务，为端到端连接提供流量控制、差错控制、服务质量、数据传输管理等服务。</p>
<p>数据链路层提供的是点到点的通信，传输层提供的是端到端的通信，两者不同。通俗地说，点到点可以理解为主机到主机之间的通信，一个点是指一个硬件地址或P地址，网络中参与通信的主机是通过硬件地址或P地址标识的;端到端的通信是指运行在不同主机内的两个进程之间的通信，一个进程由一个端口来标识，所以称为端到端通信。</p>
<p>使用传输层的服务，高层用户可以直接进行端到端的数据传输，从而忽略通信子网的存在。通过传输层的屏蔽，高层用户看不到子网的交替和变化。由于一台主机可同时运行多个进程，因此传输层具有复用和分用的功能。复用是指多个应用层进程可同时使用下面传输层的服务，分用是指传输层把收到的信息分别交付给上面应用层中相应的进程。</p>
<p>传输层的协议有TCP、UDP。</p>
<p>(5）会话层(Session Layer)</p>
<p>会话层允许不同主机上的各个进程之间进行会话。会话层利用传输层提供的端到端的服务，向表示层提供它的增值服务。这种服务主要为表示层实体或用户进程建立连接并在连接上有序地传输数据，这就是会话，也称建立同步(SYN)。</p>
<p>会话层负责管理主机间的会话进程，包括建立、管理及终止进程间的会话。会话层可以使用校验点使通信会话在通信失效时从校验点继续恢复通信，实现数据同步。</p>
<p>(6）表示层（Presentation Layer)</p>
<p>表示层主要处理在两个通信系统中交换信息的表示方式。不同机器采用的编码和表示方法不同，使用的数据结构也不同。为了使不同表示方法的数据和信息之间能互相交换，表示层采用抽象的标准方法定义数据结构，并采用标准的编码形式。数据压缩、加密和解密也是表示层可提供的数据表示变换功能。</p>
<p>(7）应用层（Application Layer）</p>
<p>应用层是OSI参考模型的最高层，是用户与网络的界面。应用层为特定类型的网络应用提供访问OSI参考模型环境的手段。因为用户的实际应用多种多样，这就要求应用层采用不同的应用协议来解决不同类型的应用要求，因此应用层是最复杂的一层，使用的协议也最多。典型的协议有用于文件传送的FTP、用于电子邮件的SMTP、用于万维网的HTTP等。</p>
<h5 id="TCP-IP模型"><a href="#TCP-IP模型" class="headerlink" title="TCP/IP模型"></a>TCP/IP模型</h5><p>ARPA在研究ARPAnet时提出了TCP/IP模型，模型从低到高依次为网络接口层（对应OSI参考模型中的物理层和数据链路层)、网际层、传输层和应用层（对应OSI参考模型中的会话层、表示层和应用层)。TCP/P由于得到广泛应用而成为事实上的国际标准。TCP/P模型的层次结构及各层的主要协议如图1.11所示。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210724164734.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210724164734.png';" /></details>

<p>网络接口层的功能类似于OSI参考模型的物理层和数据链路层。它表示与物理网络的接口，但实际上TCP/IP本身并未真正描述这一部分，只是指出主机必须使用某种协议与网络连接，以便在其上传递IP分组。具体的物理网络既可以是各种类型的局域网，如以太网、令牌环网、令牌总线网等，也可以是诸如电话网、SDH、X.25、帧中继和 ATM等公共数据网络。网络接口层的作用是从主机或结点接收IP分组，并把它们发送到指定的物理网络上。</p>
<p>网际层（主机-主机）是TCP/IP体系结构的关键部分。它和OSI参考模型的网络层在功能上非常相似。网际层将分组发往任何网络，并为之独立地选择合适的路由，但它不保证各个分组有序地到达，各个分组的有序交付由高层负责。网际层定义了标准的分组格式和协议，即P。当前采用的P协议是第4版，即IPv4，它的下一版本是IPv6。</p>
<p>传输层（应用-应用或进程-进程）的功能同样和OSI参考模型中的传输层类似，即使得发送端和目的端主机上的对等实体进行会话。传输层主要使用以下两种协议:</p>
<p>1）传输控制协议（Transmission Control Protocol，TCP)。它是面向连接的，数据传输的单位是报文段，能够提供可靠的交付。</p>
<p>2）用户数据报协议（User Datagram Protocol，UDP)。它是无连接的，数据传输的单位是用户数据报，不保证提供可靠的交付，只能提供“尽最大努力交付”。应用层(用户-用户)包含所有的高层协议，如虚拟终端协议(Telnet)、文件传输协议(FTP)、域名解析服务(DNS)、电子邮件协议(SMTP）和超文本传输协议（HTTP)。</p>
<p>由图1.10可以看出，IP 协议是因特网中的核心协议;TCP/IP可以为各式各样的应用提供服务(所谓的everything over IP)，同时TCP/P 也允许IP协议在由各种网络构成的互联网上运行(所谓的IP over everything)。正因为如此，因特网才会发展到今天的规模。</p>
<h5 id="TCP-IP模型与OSl参考模型的比较"><a href="#TCP-IP模型与OSl参考模型的比较" class="headerlink" title="TCP/IP模型与OSl参考模型的比较"></a>TCP/IP模型与OSl参考模型的比较</h5><p>TCP/IP模型与OSI参考模型有许多相似之处。</p>
<p>首先，二者都采取分层的体系结构，将庞大且复杂的问题划分为若干较容易处理的、范围较小的问题，而且分层的功能也大体相似。</p>
<p>其次，二者都是基于独立的协议栈的概念。</p>
<p>最后，二者都可以解决异构网络的互联，实现世界上不同厂家生产的计算机之间的通信。它们之间的比较如图1.12所示。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210724164828.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210724164828.png';" /></details>

<p>两个模型除具有这些基本的相似之处外，也有很多差别。</p>
<p>第一，OSI参考模型的最大贡献就是精确地定义了三个主要概念:服务、协议和接口，这与现代的面向对象程序设计思想非常吻合。而TCP/IP模型在这三个概念上却没有明确区分，不符合软件工程的思想。</p>
<p>第二，OSI参考模型产生在协议发明之前，没有偏向于任何特定的协议，通用性良好。但设计者在协议方面没有太多经验，不知道把哪些功能放到哪一层更好。TCP/IP模型正好相反，首先出现的是协议，模型实际上是对已有协议的描述，因此不会出现协议不能匹配模型的情况，但该模型不适合于任何其他非TCP/IP的协议栈。</p>
<p>第三，TCP/IP模型在设计之初就考虑到了多种异构网的互联问题，并将网际协议(P）作为一个单独的重要层次。OSI参考模型最初只考虑到用一种标准的公用数据网将各种不同的系统互联。OSI参考模型认识到P的重要性后，只好在网络层中划分出一个子层来完成类似于TCP/IP模型中的IP的功能。</p>
<p>第四，OSI参考模型在网络层支持无连接和面向连接的通信，但在传输层仅有面向连接的通信。而TCP/IP模型认为可靠性是端到端的问题，因此它在网际层仅有一种无连接的通信模式，但传输层支持无连接和面向连接两种模式。这个不同点常常作为考查点。</p>
<p>无论是OSI参考模型还是TCP/IP模型，都不是完美的，对二者的讨论和批评都很多。OSI参考模型的设计者从工作的开始，就试图建立一个全世界的计算机网络都要遵循的统一标准。从技术角度来看，他们希望追求一种完美的理想状态，这也导致基于OSI参考模型的软件效率极低。OSI参考模型缺乏市场与商业动力，结构复杂，实现周期长，运行效率低，这是它未能达到预期目标的重要原因。</p>
<p>无论是OSI参考模型还是TCP/P模型，都不是完美的，对二者的讨论和批评都很多。OSI参考模型的设计者从工作的开始，就试图建立一个全世界的计算机网络都要遵循的统一标准。从技术角度来看，他们希望追求一种完美的理想状态，这也导致基于OSI参考模型的软件效率极低。OSI参考模型缺乏市场与商业动力，结构复杂，实现周期长，运行效率低，这是它未能达到预期目标的重要原因。</p>
<p>学习计算机网络时，我们往往采取折中的办法，即综合OSI参考模型和TCP/IP模型的优点，采用一种如图1.13所示的只有5层协议的体系结构，即我们所熟知的物理层、数据链路层、网络层、传输层和应用层。本书也采用这种体系结构进行讨论。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210724165119.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210724165119.png';" /></details>

<p>最后简单介绍使用通信协议栈进行通信的结点的数据传输过程。每个协议栈的最顶端都是一个面向用户的接口，下面各层是为通信服务的协议。用户传输一个数据报时，通常给出用户能够理解的自然语言，然后通过应用层，将自然语言会转化为用于通信的通信数据。通信数据到达传输层，作为传输层的数据部分（传输层SDU)，加上传输层的控制信息（传输层PCI)，组成传输层的PDU，然后交到网络层，传输层的PDU下放到网络层后，就成为网络层的SDU，然后加上网络层的PCI，又组成了网络层的PDU，下放到数据链路层，就这样层层下放，层层包裹，最后形成的数据报通过通信线路传输，到达接收方结点协议栈，接收方再逆向地逐层把“包裹”拆开,然后把收到的数据提交给用户，如图1.14所示。</p>
<details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210724165141.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210724165141.png';" /></details>

<blockquote>
<p><a target="_blank" rel="noopener" href="https://www.techopedia.com/definition/9005/protocol-stack">协议栈和分层的区别</a>，<a target="_blank" rel="noopener" href="https://en.wikipedia.org/wiki/Protocol_stack">维基百科协议栈的定义</a><br>为什么我感觉协议栈和分层是同一概念？见谢书</p>
</blockquote>
<h5 id="计算机网络与分布式计算机系统的主要区别是什么"><a href="#计算机网络与分布式计算机系统的主要区别是什么" class="headerlink" title="计算机网络与分布式计算机系统的主要区别是什么?"></a>计算机网络与分布式计算机系统的主要区别是什么?</h5><p>分布式系统最主要的特点是，整个系统中的各个计算机对用户都是透明的。用户通过输入命令就可以运行程序，但用户并不知道哪台计算机在为它运行程序。操作系统为用户选择一台最合适的计算机来运行其程序，并将运行的结果传送到合适的地方。</p>
<p>计算机网络则与之不同，用户必须先登录欲运行程序的计算机，然后按照计算机的地址，将程序通过计算机网络传送到该计算机中运行，最后根据用户的命令将结果传送到指定的计算机中。二者的区别主要是软件的不同。</p>
<blockquote>
<p>不是应该是计算机网路是网络的网络吗？</p>
</blockquote>
<h5 id="为什么一个网络协议必须考虑到各种不利的情况"><a href="#为什么一个网络协议必须考虑到各种不利的情况" class="headerlink" title="为什么一个网络协议必须考虑到各种不利的情况?"></a>为什么一个网络协议必须考虑到各种不利的情况?</h5><p>因为网络协议如果不全面考虑不利的情况，那么当情况发生变化时,协议就会保持理想状况，一直等下去!就如同两位朋友在电话中约好下午3点在公园见面，并且约定不见不散。这一―协议很不科学，因为任何一方如果有耽搁而来不了，且无法通知对方，那么另一方就要一直等下去!所以判断一个计算机网络是否正确，不能只看在正常情况下是否正确，还必须非常仔细地检查协议能否应付各种异常情况。</p>
<h5 id="因特网使用的P协议是无连接的，因此其传输是不可靠的。这样容易使人们感到因特网很不可靠。那么为什么当初不把因特网的传输设计为可靠的呢"><a href="#因特网使用的P协议是无连接的，因此其传输是不可靠的。这样容易使人们感到因特网很不可靠。那么为什么当初不把因特网的传输设计为可靠的呢" class="headerlink" title="因特网使用的P协议是无连接的，因此其传输是不可靠的。这样容易使人们感到因特网很不可靠。那么为什么当初不把因特网的传输设计为可靠的呢?"></a>因特网使用的P协议是无连接的，因此其传输是不可靠的。这样容易使人们感到因特网很不可靠。那么为什么当初不把因特网的传输设计为可靠的呢?</h5><p>传统电信网的主要用途是电话通信，并且普通电话机不是智能的，因此电信公司必须花费巨大的代价把电信网设计得非常好，以保证用户的通信质量。</p>
<p>数据的传送显然必须非常可靠。当初在设计ARPAnet时，很重要的讨论内容之一是:“谁应当负责数据传输的可靠性?”一种意见是主张应当像电信网那样，由通信网络负责数据传输的可靠性（因为电信网的发展历史及其技术水平已经证明，人们可以将网络设计得相当可靠);另一种意见则坚决主张由用户的主机负责数据传输的可靠性，理由是这样可使计算机网络便宜、灵活。</p>
<p>计算机网络的先驱认为，计算机网络和电信网的一个重大区别是终端设备的性能差别很大。于是，他们采用了“端到端的可靠传输”策略，即在传输层使用面向连接的TCP协议，这样既能使网络部分价格便宜且灵活可靠，又能保证端到端的可靠传输。</p>
<blockquote>
<p>计算机网络和电信网的一个重大区别是终端设备的性能差别很大。按照这层意思不就更应该把可靠传输这种繁琐的工作交给计算机网络更好了吗</p>
</blockquote>
<h5 id="有人说，宽带信道相当于高速公路车道数目增多了，可以同时并行地跑更多数量的汽车。虽然汽车的时速并没有提高（相当于比特在信道上的传播速率未提高-，但整个高速公路的运输能力却增多了，相当于能够传送更多数量的比特。这种比喻合适否"><a href="#有人说，宽带信道相当于高速公路车道数目增多了，可以同时并行地跑更多数量的汽车。虽然汽车的时速并没有提高（相当于比特在信道上的传播速率未提高-，但整个高速公路的运输能力却增多了，相当于能够传送更多数量的比特。这种比喻合适否" class="headerlink" title="有人说，宽带信道相当于高速公路车道数目增多了，可以同时并行地跑更多数量的汽车。虽然汽车的时速并没有提高（相当于比特在信道上的传播速率未提高)，但整个高速公路的运输能力却增多了，相当于能够传送更多数量的比特。这种比喻合适否?"></a>有人说，宽带信道相当于高速公路车道数目增多了，可以同时并行地跑更多数量的汽车。虽然汽车的时速并没有提高（相当于比特在信道上的传播速率未提高)，但整个高速公路的运输能力却增多了，相当于能够传送更多数量的比特。这种比喻合适否?</h5><p>可以这样比喻。但一定不能误认为“提高信道的速率是设法使比特并行地传输”。</p>
<p>如果一定要用汽车在高速公路上行驶和比特在通信线路上传输相比较，那么可以这样来想象:低速信道相当于汽车进入高速公路的时间间隔较长。例如，每隔1分钟有一辆汽车进入高速公路;“信道速率提高”相当于进入高速公路的汽车的时间间隔缩短了，例如，现在每隔6秒就有一辆汽车进入高速公路。虽然汽车在高速公路上行驶的速率无变化，但在同样的时间内，进入高速公路的汽车总数却增多了(每隔1分钟进入高速公路的汽车现在增加到10辆)，因而吞吐量也就增大了。</p>
<p>也就是说，当带宽或发送速率提高后，比特在链路上向前传播的速率并未提高，只是每秒注入链路的比特数增加了。“速率提高”就体现在单位时间内发送到链路上的比特数增多了，而并不是比特在链路上跑得更快了。</p>
<h5 id="端到端通信和点到点通信有什么区别"><a href="#端到端通信和点到点通信有什么区别" class="headerlink" title="端到端通信和点到点通信有什么区别?"></a>端到端通信和点到点通信有什么区别?</h5><p>从本质上说，由物理层、数据链路层和网络层组成的通信子网为网络环境中的主机提供点到点的服务，而传输层为网络中的主机提供端到端的通信。</p>
<p>直接相连的结点之间的通信称为点到点通信，它只提供一台机器到另一台机器之间的通信，不涉及程序或进程的概念。同时，点到点通信并不能保证数据传输的可靠性，也不能说明源主机与目的主机之间是哪两个进程在通信，这些工作都是由传输层来完成的。</p>
<p>端到端通信建立在点到点通信的基础上，它是由一段段的点到点通信信道构成的，是比点到点通信更高一级的通信方式，以完成应用程序（进程）之间的通信。“端”是指用户程序的端口，端口号标识了应用层中不同的进程。</p>
<h5 id="如何理解传输速率、带宽和传播速率"><a href="#如何理解传输速率、带宽和传播速率" class="headerlink" title="如何理解传输速率、带宽和传播速率?"></a>如何理解传输速率、带宽和传播速率?</h5><details><summary>图片详情</summary><img src="https://raw.githubusercontent.com/ednow/cloudimg/main/githubio/20210727094533.png" alt="找不到图片(Image not found)" onerror="this.onerror=null;this.src='https://gitee.com/ednow/cloudimg/raw/main/githubio/20210727094533.png';" /></details>

<blockquote>
<p>这个理解似乎与期末考的定义恰好相反</p>
</blockquote>
<h5 id="如何理解传输时延、发送时延和传播时延"><a href="#如何理解传输时延、发送时延和传播时延" class="headerlink" title="如何理解传输时延、发送时延和传播时延?"></a>如何理解传输时延、发送时延和传播时延?</h5><p>传输时延又称发送时延，是主机或路由器发送数据帧所需的时间，即从数据帧的第1比特算起，到该数据帧的最后1比特发送完毕所需要的时间。计算公式是</p>
<p>发送时延=数据帧长度/信道带宽</p>
<p>传播时延是电磁波在信道中传播一定的距离所花费的时间。计算公式是</p>
<p>传播时延=信道长度/电磁波在信道上的传播速率</p>

      
    </div>

    
    
    
      <footer class="post-footer">
        <div class="post-eof"></div>
      </footer>
  </article>
  
  
  


  
  <nav class="pagination">
    <a class="extend prev" rel="prev" href="/index/page/3/"><i class="fa fa-angle-left" aria-label="上一页"></i></a><a class="page-number" href="/index/">1</a><span class="space">&hellip;</span><a class="page-number" href="/index/page/3/">3</a><span class="page-number current">4</span><a class="page-number" href="/index/page/5/">5</a><span class="space">&hellip;</span><a class="page-number" href="/index/page/32/">32</a><a class="extend next" rel="next" href="/index/page/5/"><i class="fa fa-angle-right" aria-label="下一页"></i></a>
  </nav>



          </div>
          

<script>
  window.addEventListener('tabs:register', () => {
    let { activeClass } = CONFIG.comments;
    if (CONFIG.comments.storage) {
      activeClass = localStorage.getItem('comments_active') || activeClass;
    }
    if (activeClass) {
      let activeTab = document.querySelector(`a[href="#comment-${activeClass}"]`);
      if (activeTab) {
        activeTab.click();
      }
    }
  });
  if (CONFIG.comments.storage) {
    window.addEventListener('tabs:click', event => {
      if (!event.target.matches('.tabs-comment .tab-content .tab-pane')) return;
      let commentClass = event.target.classList[1];
      localStorage.setItem('comments_active', commentClass);
    });
  }
</script>

        </div>
          
  
  <div class="toggle sidebar-toggle">
    <span class="toggle-line toggle-line-first"></span>
    <span class="toggle-line toggle-line-middle"></span>
    <span class="toggle-line toggle-line-last"></span>
  </div>

  <aside class="sidebar">
    <div class="sidebar-inner">

      



      <ul class="sidebar-nav motion-element">
        <li class="sidebar-nav-toc">
          文章目录
        </li>
        <li class="sidebar-nav-overview">
          站点概览
        </li>
      </ul>

      <!--noindex-->
      <div class="post-toc-wrap sidebar-panel">
      </div>
      <!--/noindex-->

      <div class="site-overview-wrap sidebar-panel">
        <div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person">
  <p class="site-author-name" itemprop="name">ednow</p>
  <div class="site-description" itemprop="description"></div>
</div>
<div class="site-state-wrap motion-element">
  <nav class="site-state">
      <div class="site-state-item site-state-posts">
          <a href="/archives/">
        
          <span class="site-state-item-count">317</span>
          <span class="site-state-item-name">日志</span>
        </a>
      </div>
      <div class="site-state-item site-state-categories">
            <a href="/categories/">
          
        <span class="site-state-item-count">1</span>
        <span class="site-state-item-name">分类</span></a>
      </div>
      <div class="site-state-item site-state-tags">
            <a href="/tags/">
          
        <span class="site-state-item-count">57</span>
        <span class="site-state-item-name">标签</span></a>
      </div>
  </nav>
</div>



      </div>

    </div>
  </aside>
  <div id="sidebar-dimmer"></div>


      </div>
    </main>

    <footer class="footer">
      <div class="footer-inner">
        

        

<div class="copyright">
  
  &copy; 
  <span itemprop="copyrightYear">2022</span>
  <span class="with-love">
    <i class="fa fa-heart"></i>
  </span>
  <span class="author" itemprop="copyrightHolder">ednow</span>
</div>
  <div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动
  </div>

        








      </div>
    </footer>
  </div>

  
  <script src="/lib/anime.min.js"></script>
  <script src="/lib/velocity/velocity.min.js"></script>
  <script src="/lib/velocity/velocity.ui.min.js"></script>

<script src="/js/utils.js"></script>

<script src="/js/motion.js"></script>


<script src="/js/schemes/muse.js"></script>


<script src="/js/next-boot.js"></script>




  















  

  
      

<script>
  if (typeof MathJax === 'undefined') {
    window.MathJax = {
      loader: {
        source: {
          '[tex]/amsCd': '[tex]/amscd',
          '[tex]/AMScd': '[tex]/amscd'
        }
      },
      tex: {
        inlineMath: {'[+]': [['$', '$']]},
        tags: 'ams'
      },
      options: {
        renderActions: {
          findScript: [10, doc => {
            document.querySelectorAll('script[type^="math/tex"]').forEach(node => {
              const display = !!node.type.match(/; *mode=display/);
              const math = new doc.options.MathItem(node.textContent, doc.inputJax[0], display);
              const text = document.createTextNode('');
              node.parentNode.replaceChild(text, node);
              math.start = {node: text, delim: '', n: 0};
              math.end = {node: text, delim: '', n: 0};
              doc.math.push(math);
            });
          }, '', false],
          insertedScript: [200, () => {
            document.querySelectorAll('mjx-container').forEach(node => {
              let target = node.parentNode;
              if (target.nodeName.toLowerCase() === 'li') {
                target.parentNode.classList.add('has-jax');
              }
            });
          }, '', false]
        }
      }
    };
    (function () {
      var script = document.createElement('script');
      script.src = '//cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js';
      script.defer = true;
      document.head.appendChild(script);
    })();
  } else {
    MathJax.startup.document.state(0);
    MathJax.texReset();
    MathJax.typeset();
  }
</script>

    

  


</body>
</html>
