<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.3.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"ednow.github.io",root:"/",scheme:"Muse",version:"7.8.0",exturl:!1,sidebar:{position:"left",width:400,display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="模型模型管理平台：ollamaollama的模型路径默认情况下，ollama模型的存储目录如下： macOS: ~&#x2F;.ollama&#x2F;models Linux: &#x2F;usr&#x2F;share&#x2F;ollama&#x2F;.ollama&#x2F;models Windows: C:\Users\&lt;username&gt;\.ollama\models ollama的配置文件参考文献 ollama修改模型的位置系统环境变量增加"><meta property="og:type" content="article"><meta property="og:title" content="chat-gpt"><meta property="og:url" content="http://ednow.github.io/2023/08/15/chat-gpt/index.html"><meta property="og:site_name" content="ednow"><meta property="og:description" content="模型模型管理平台：ollamaollama的模型路径默认情况下，ollama模型的存储目录如下： macOS: ~&#x2F;.ollama&#x2F;models Linux: &#x2F;usr&#x2F;share&#x2F;ollama&#x2F;.ollama&#x2F;models Windows: C:\Users\&lt;username&gt;\.ollama\models ollama的配置文件参考文献 ollama修改模型的位置系统环境变量增加"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2023-08-15T11:22:03.000Z"><meta property="article:modified_time" content="2024-10-29T08:30:48.210Z"><meta property="article:author" content="ednow"><meta name="twitter:card" content="summary"><link rel="canonical" href="http://ednow.github.io/2023/08/15/chat-gpt/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>chat-gpt | ednow</title><script async src="https://www.googletagmanager.com/gtag/js?id=G-XQGJ63ZD9Y"></script><script>function gtag(){dataLayer.push(arguments)}CONFIG.hostname===location.hostname&&(window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-XQGJ63ZD9Y"))</script><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script"),t=(e.src="https://hm.baidu.com/hm.js?83f04257c97e81cca692d7c4c7fbbc9a",document.getElementsByTagName("script")[0]);t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">ednow</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://ednow.github.io/2023/08/15/chat-gpt/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="ednow"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="ednow"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">chat-gpt</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2023-08-15 11:22:03" itemprop="dateCreated datePublished" datetime="2023-08-15T11:22:03+00:00">2023-08-15</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2024-10-29 08:30:48" itemprop="dateModified" datetime="2024-10-29T08:30:48+00:00">2024-10-29</time> </span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>6.5k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>6 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="模型"><a href="#模型" class="headerlink" title="模型"></a>模型</h1><h2 id="模型管理平台：ollama"><a href="#模型管理平台：ollama" class="headerlink" title="模型管理平台：ollama"></a>模型管理平台：ollama</h2><h3 id="ollama的模型路径"><a href="#ollama的模型路径" class="headerlink" title="ollama的模型路径"></a>ollama的模型路径</h3><p>默认情况下，ollama模型的存储目录如下：</p><p>macOS: <code>~/.ollama/models</code></p><p>Linux: <code>/usr/share/ollama/.ollama/models</code></p><p>Windows: <code>C:\Users\&lt;username&gt;\.ollama\models</code></p><h3 id="ollama的配置文件"><a href="#ollama的配置文件" class="headerlink" title="ollama的配置文件"></a>ollama的配置文件</h3><p><a target="_blank" rel="noopener" href="https://github.com/datawhalechina/handy-ollama/blob/main/docs/C2/2.%20Ollama%20%E5%9C%A8%20Windows%20%E4%B8%8B%E7%9A%84%E5%AE%89%E8%A3%85%E4%B8%8E%E9%85%8D%E7%BD%AE.md">参考文献</a></p><h3 id="ollama修改模型的位置"><a href="#ollama修改模型的位置" class="headerlink" title="ollama修改模型的位置"></a>ollama修改模型的位置</h3><p>系统环境变量增加 <code>OLLAMA_MODELS</code></p><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/weixin_46124467/article/details/136781232">参考文献</a></p><h3 id="别的机器监听地址需要改成0-0-0-0"><a href="#别的机器监听地址需要改成0-0-0-0" class="headerlink" title="别的机器监听地址需要改成0.0.0.0"></a>别的机器监听地址需要改成0.0.0.0</h3><p>不然会出现networkerror</p><h3 id="maxkb"><a href="#maxkb" class="headerlink" title="maxkb"></a>maxkb</h3><p>当maxkb和ollama部署在同一台主机上时</p><p>maxkb中可以配置的ollama地址:<code>http://host.docker.internal:11434</code>, 访问宿主机的ip端口</p><h3 id="ollama模型的支持长度"><a href="#ollama模型的支持长度" class="headerlink" title="ollama模型的支持长度"></a>ollama模型的支持长度</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/jining11/article/details/140846446">参考文献</a></p><h3 id="查看ollama模型的版本"><a href="#查看ollama模型的版本" class="headerlink" title="查看ollama模型的版本"></a>查看ollama模型的版本</h3><p><code>ollama show llava:34B</code></p><h3 id="查看ollama的版本"><a href="#查看ollama的版本" class="headerlink" title="查看ollama的版本"></a>查看ollama的版本</h3><p><code>ollama -v</code></p><h3 id="调用ollama的时候使用apikey"><a href="#调用ollama的时候使用apikey" class="headerlink" title="调用ollama的时候使用apikey"></a>调用ollama的时候使用apikey</h3><p><a target="_blank" rel="noopener" href="https://github.com/ollama/ollama/issues/849">参考文献</a></p><h2 id="ollama-vs-vllm"><a href="#ollama-vs-vllm" class="headerlink" title="ollama vs vllm"></a><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Bw4m1D7K3/?spm_id_from=333.788.recommend_more_video.0">ollama vs vllm</a></h2><p>vllm有paged attention，如果prefix相同会只算一份，kv占的显存也只一份</p><h2 id="模型微调"><a href="#模型微调" class="headerlink" title="模型微调"></a>模型微调</h2><p>TODO</p><h2 id="其他模型"><a href="#其他模型" class="headerlink" title="其他模型"></a>其他模型</h2><ul><li>ai语音模拟生成</li><li>ai音频解析</li><li>ai音频解析</li></ul><h2 id="从hugging-face上下载模型"><a href="#从hugging-face上下载模型" class="headerlink" title="从hugging face上下载模型"></a>从hugging face上下载模型</h2><p>linux设置镜像：<code>export HF_ENDPOINT=https://hf-mirror.com</code></p><p>Windows设置镜像：<code>set HF_ENDPOINT=https://hf-mirror.com</code></p><p>下载模型：</p><p><code>huggingface-cli download InstantX/InstantID --local-dir ./checkpoints</code></p><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/watson2017/article/details/136330899">参考文献</a></p><h2 id="mteb排行榜"><a href="#mteb排行榜" class="headerlink" title="mteb排行榜"></a><a target="_blank" rel="noopener" href="https://huggingface.co/spaces/mteb/leaderboard">mteb排行榜</a></h2><h2 id="向量模型"><a href="#向量模型" class="headerlink" title="向量模型"></a>向量模型</h2><h3 id="TencentBAC-Conan-embedding-v1"><a href="#TencentBAC-Conan-embedding-v1" class="headerlink" title="TencentBAC/Conan-embedding-v1"></a><a target="_blank" rel="noopener" href="https://huggingface.co/TencentBAC/Conan-embedding-v1">TencentBAC/Conan-embedding-v1</a></h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/hawk2014bj/article/details/142265387">HuggingFace Embedding 转为 Ollama Embedding</a></p><h2 id="多模态模型"><a href="#多模态模型" class="headerlink" title="多模态模型"></a>多模态模型</h2><h3 id="llava"><a href="#llava" class="headerlink" title="llava"></a><a target="_blank" rel="noopener" href="https://ollama.com/library/llava">llava</a></h3><h2 id="coder模型"><a href="#coder模型" class="headerlink" title="coder模型"></a>coder模型</h2><h3 id="deepseek-coder"><a href="#deepseek-coder" class="headerlink" title="deepseek-coder"></a><a target="_blank" rel="noopener" href="https://ollama.com/library/deepseek-coder">deepseek-coder</a></h3><h2 id="LLM模型"><a href="#LLM模型" class="headerlink" title="LLM模型"></a>LLM模型</h2><h3 id="qwen2-5"><a href="#qwen2-5" class="headerlink" title="qwen2.5"></a><a target="_blank" rel="noopener" href="https://qwenlm.github.io/zh/blog/qwen2.5-llm/">qwen2.5</a></h3><h2 id="模型排行"><a href="#模型排行" class="headerlink" title="模型排行"></a>模型排行</h2><h3 id="qwen模型的版本区别"><a href="#qwen模型的版本区别" class="headerlink" title="qwen模型的版本区别"></a>qwen模型的版本区别</h3><table><thead><tr><th>特性</th><th>Qwen-2.5-14B</th><th>Qwen-2.5-14B-Instruct</th></tr></thead><tbody><tr><td><strong>用途</strong></td><td>- 这是一个通用的大型语言模型，适用于各种自然语言处理任务，如文本生成、翻译、问答等。<code>&lt;br&gt;</code> - 它在大规模的文本数据上进行了预训练，能够生成高质量的文本。</td><td>- 这是一个经过指令微调的版本，专门用于理解和执行特定的指令。<code>&lt;br&gt;</code> - 它在通用语言模型的基础上，增加了对指令的理解和执行能力，使其更适合处理指令式的任务，如生成特定格式的文本、回答特定类型的问题等。</td></tr><tr><td><strong>训练方式</strong></td><td>- 这个版本主要通过无监督学习的方式在大规模的文本数据上进行预训练。<code>&lt;br&gt;</code> - 它通过预测下一个词或掩码词的任务来学习语言的结构和模式。</td><td>- 这个版本在通用语言模型的基础上，进行了指令微调。<code>&lt;br&gt;</code> - 微调过程中，模型会接触到大量的指令-响应对，从而学会如何根据指令生成相应的输出。这种微调使得模型在处理特定任务时更加高效和准确。</td></tr><tr><td><strong>应用场景</strong></td><td>- 适用于广泛的自然语言处理任务，如文本生成、翻译、摘要、情感分析等。<code>&lt;br&gt;</code> - 它在这些任务中表现出色，但可能在处理特定指令时不如 Qwen-2.5-14B-Instruct 那么精确。</td><td>- 特别适合处理指令式的任务，如生成特定格式的文档、回答特定类型的问题、完成特定的编辑任务等。<code>&lt;br&gt;</code> - 它在这些任务中表现更为出色，能够更准确地理解用户的意图并生成相应的输出。</td></tr><tr><td><strong>性能</strong></td><td>- 在通用任务上表现出色，但由于没有专门的指令微调，可能在处理特定指令时表现稍逊一筹。</td><td>- 在处理特定指令时表现更佳，能够更准确地理解指令并生成高质量的输出。</td></tr></tbody></table><p>总结：Qwen-2.5-14B 是一个通用的大型语言模型，适用于各种自然语言处理任务。<code>&lt;br&gt;</code><br>Qwen-2.5-14B-Instruct 是在通用模型基础上经过指令微调的版本，特别适合处理指令式的任务。<br>选择哪个版本取决于你的具体需求。如果你需要一个通用的模型来处理多种任务，可以选择 Qwen-2.5-14B。如果你的任务涉及大量指令式的操作，建议使用 Qwen-2.5-14B-Instruct。</p><h2 id="模型转化、量化"><a href="#模型转化、量化" class="headerlink" title="模型转化、量化"></a><a target="_blank" rel="noopener" href="https://www.bilibili.com/video/BV1Mm42157HB/?spm_id_from=333.337.search-card.all.click&vd_source=6c3bcde1a56014266e8b0e632f67c1eb">模型转化、量化</a></h2><h2 id="打标模型"><a href="#打标模型" class="headerlink" title="打标模型"></a>打标模型</h2><p>图片的分类标签非常多样，可以根据不同的标准进行分类。例如，根据场景可以分为自然风光、城市街景、室内环境等；根据物体类型可以分为动物、植物、交通工具等；根据用途可以分为医学影像、遥感图像、艺术作品等。</p><p>一些常见的分类标签包括但不限于：</p><ol><li>通用视觉类：如CIFAR-10、CIFAR-100、STL-10等 。</li><li>手写体&amp;单通道类：如MNIST、Fashion-MNIST等 。</li><li>细粒度图像识别类：如SUN Attribute、Oxford-IIIT Pet等 。</li><li>自然界图像和场景类：如IP102、Places365等 。</li><li>遥感类：如EuroSAT、BigEarthNet等 。</li><li>医疗健康类：如HErlev、BBBC041等 。</li><li>科学教育类：如PlantVillage、PlantDoc等 。</li><li>艺术类：如iCartoonFace、KaoKore等 。</li><li>食物类：如Food-101、ECUSTFD等 。</li><li>生活场景类：如SVHN、Clothing1M等 。</li></ol><p>此外，还可以根据图像的内容进行分类，如物体、场景、行为等，并返回对应的标签信息，例如花、鸟、鱼、虫、汽车、建筑等 。</p><p>对于具体的分类标签，可以通过查看相关的数据集资源来获取更详细的信息。例如，CSDN的博客文章分享了64个图像分类任务相关的热门公开数据集资源，并粗略分为10类，每个类别下都有具体的数据集和对应的标签信息 。此外，还可以参考TensorFlow Lite提供的图像分类模型，这些模型可以识别不同的类别，如兔子、仓鼠和狗等 。</p><p>如果想要查找更多的分类标签，可以访问一些提供图像分类数据集的网站，如Kaggle、Google TensorFlow等，这些平台通常会提供详细的标签信息和数据集下载链接。</p><h2 id="用于发票识别的微调-Transformer-模型"><a href="#用于发票识别的微调-Transformer-模型" class="headerlink" title="用于发票识别的微调 Transformer 模型"></a><a target="_blank" rel="noopener" href="https://blog.csdn.net/deephub/article/details/118667533">用于发票识别的微调 Transformer 模型</a></h2><h2 id="TrOCR模型微调实战【微软光学字符识别】"><a href="#TrOCR模型微调实战【微软光学字符识别】" class="headerlink" title="TrOCR模型微调实战【微软光学字符识别】"></a><a target="_blank" rel="noopener" href="https://zhuanlan.zhihu.com/p/666687605">TrOCR模型微调实战【微软光学字符识别】</a></h2><h1 id="llm应用平台"><a href="#llm应用平台" class="headerlink" title="llm应用平台"></a>llm应用平台</h1><h2 id="workflow平台推荐"><a href="#workflow平台推荐" class="headerlink" title="workflow平台推荐"></a>workflow平台推荐</h2><p><a target="_blank" rel="noopener" href="https://www.cnblogs.com/ting1/p/18332412">RAG+AI工作流+Agent：LLM框架该如何选择，全面对比MaxKB、Dify、FastGPT、RagFlow、Anything-LLM,以及更多推荐</a></p><h2 id="ragflow"><a href="#ragflow" class="headerlink" title="ragflow"></a>ragflow</h2><h3 id="npm下载时报错unable-to-verify-the-first-certificate解决方法"><a href="#npm下载时报错unable-to-verify-the-first-certificate解决方法" class="headerlink" title="npm下载时报错unable to verify the first certificate解决方法"></a>npm下载时报错unable to verify the first certificate解决方法</h3><p>在dockerfile中新加 <code>npm config set strict-ssl false</code></p><h2 id="Ollama-结合-maxkb做本地知识库"><a href="#Ollama-结合-maxkb做本地知识库" class="headerlink" title="Ollama 结合 maxkb做本地知识库"></a>Ollama 结合 maxkb做本地知识库</h2><h3 id="【教程】Windows系统本地部署Ollama-MaxKB安装教程"><a href="#【教程】Windows系统本地部署Ollama-MaxKB安装教程" class="headerlink" title="【教程】Windows系统本地部署Ollama+MaxKB安装教程"></a><a target="_blank" rel="noopener" href="https://bbs.fit2cloud.com/t/topic/5701">【教程】Windows系统本地部署Ollama+MaxKB安装教程</a></h3><h3 id="maxkb去掉社区版限制"><a href="#maxkb去掉社区版限制" class="headerlink" title="maxkb去掉社区版限制"></a>maxkb去掉社区版限制</h3><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/taotao_guiwang/article/details/141320454">参考文献</a></p><h3 id="索引时间测试"><a href="#索引时间测试" class="headerlink" title="索引时间测试"></a>索引时间测试</h3><table><thead><tr><th>字符数</th><th>分段数</th><th>cpu型号</th><th>时间</th></tr></thead><tbody><tr><td>738.6k</td><td>1478</td><td>i9 13900k</td><td>5min</td></tr><tr><td>738.6k</td><td>1478</td><td>CPU Intel Celeron J4125(DS920+)</td><td>1.5h</td></tr></tbody></table><h3 id="目录挂载"><a href="#目录挂载" class="headerlink" title="目录挂载"></a>目录挂载</h3><p>maxkb知识库：/var/lib/postgresql/data</p><p>向量模型：</p><p>推理模型：</p><h2 id="RAG技术"><a href="#RAG技术" class="headerlink" title="RAG技术"></a>RAG技术</h2><p><a target="_blank" rel="noopener" href="https://baike.baidu.com/item/%E6%A3%80%E7%B4%A2%E5%A2%9E%E5%BC%BA%E7%94%9F%E6%88%90/64380539?fromtitle=RAG&fromid=64380547&fr=aladdin">检索增强生成（Retrieval-augmented Generation）</a></p><p>检索增强生成模型结合了语言模型和信息检索技术。具体来说，当模型需要生成文本或者回答问题时，它会先从一个庞大的文档集合中检索出相关的信息，然后利用这些检索到的信息来指导文本的生成，从而提高预测的质量和准确性。</p><h3 id="稀疏检索-Sparse-Retrieval-和稠密检索-Dense-Retrieval-的区别"><a href="#稀疏检索-Sparse-Retrieval-和稠密检索-Dense-Retrieval-的区别" class="headerlink" title="稀疏检索(Sparse Retrieval)和稠密检索(Dense Retrieval)的区别"></a>稀疏检索(Sparse Retrieval)和稠密检索(Dense Retrieval)的区别</h3><p>在信息检索领域，“稀疏”和“稠密”这两个形容词主要用来描述向量表示的特性。具体来说，它们形容的是向量的<strong>稀疏性和稠密性</strong>，即向量中非零元素的相对数量和分布情况。</p><p><strong>稀疏（Sparse）</strong></p><ul><li><strong>特点</strong>：稀疏向量中大多数元素是零，只有少数几个元素是非零的。换句话说，向量的绝大部分维度没有激活。</li><li><strong>形象例子</strong>：假设有一个向量表示包含10,000个词汇的词汇表中的词频向量，如果一个文档中只有50个词出现，那么这个文档的词频向量将有9,950个零值和50个非零值。这个向量被称为稀疏向量。</li><li><strong>用途</strong>：在稀疏检索中，比如使用TF-IDF或BM25算法，文档和查询通常表示为这种稀疏向量。它们明确地表示了文本中的具体词汇，但大多数词汇在单个文档或查询中是不存在的（值为零）。</li></ul><p><strong>稠密（Dense）</strong></p><ul><li><strong>特点</strong>：稠密向量中大多数元素都是非零的，每个维度都有实际的数值。向量的每个维度都有值，虽然不一定是大的值。</li><li><strong>形象例子</strong>：假设有一个向量表示包含768个维度的BERT嵌入，这个向量中的每一个维度通常都有非零值。尽管这些值的大小会有所不同，但几乎没有维度是完全零值的。这个向量被称为稠密向量。</li><li><strong>用途</strong>：在稠密检索中，使用深度学习模型（如BERT、GPT）把文档和查询编码成这种稠密向量。这些向量捕捉了文本的复杂语义和上下文信息，尽管这些信息不是显式的词汇频率，但通过向量的形式隐式地表示出来。</li></ul><p><strong>小结</strong></p><ul><li><strong>稀疏（Sparse）</strong>：指向量中大部分维度是零，只有少数维度是非零。这类向量便于从高维数据中提取特定的显式特征，计算效率高。</li><li><strong>稠密（Dense）</strong>：指向量中大部分维度都是非零，每个维度都有实际值。这类向量捕捉到更多复杂的隐式信息，如上下文和语义，是丰富语义表示的结果。</li></ul><p>这两个术语帮助区分了两种不同的向量表示方法的特性和适用场景。</p><h3 id="参考文献"><a href="#参考文献" class="headerlink" title="参考文献"></a>参考文献</h3><ul><li><a target="_blank" rel="noopener" href="https://developer.aliyun.com/article/1605343">rag文档解析技术</a></li><li><a target="_blank" rel="noopener" href="https://new.qq.com/rain/a/20240627A019OY00">一文梳理有效提升RAG效果的方法</a></li><li><a target="_blank" rel="noopener" href="https://new.qq.com/rain/a/20240611A0118C00#:~:text=%E5%99%A8%E7%9A%84%E8%A1%8C%E4%B8%BA%E3%80%82-,%E5%9B%9B%E3%80%81RAG%E9%80%9A%E7%94%A8%E8%8C%83%E5%BC%8F%E7%9A%84%E5%B7%A5%E7%A8%8B%E5%AE%9E%E8%B7%B5,-4.1%20%E6%8A%80%E6%9C%AF%E6%9E%B6%E6%9E%84">RAG通用范式的工程实践</a></li><li><a target="_blank" rel="noopener" href="https://www.163.com/dy/article/JF8OTQV80511N33R.html">各类开源模型</a></li></ul><h2 id="dify"><a href="#dify" class="headerlink" title="dify"></a>dify</h2><h3 id="OpenAPI-Swagger标准是什么"><a href="#OpenAPI-Swagger标准是什么" class="headerlink" title="OpenAPI Swagger标准是什么"></a>OpenAPI Swagger标准是什么</h3><p>OpenAPI Swagger标准是一个用于描述RESTful API的规范，它允许你以一种标准化的方式来创建、描述、调用和发现API。这个标准最初由Swagger规范发展而来，后来由OpenAPI Initiative（OAI）接管并继续发展，目前最新的正式版本是OpenAPI 3.0.0。</p><p>OpenAPI规范的主要特点包括：</p><ol><li><strong>机器可读</strong>：规范定义了一种机器可读的格式，允许工具能够读取API描述并执行各种操作，如生成文档、代码等。</li><li><strong>人类可读</strong>：规范同样强调人类可读性，使得API的描述易于理解。</li><li><strong>语言无关性</strong>：规范与具体的编程语言无关，可以用于描述任何语言实现的API。</li><li><strong>丰富的功能</strong>：包括请求和响应的验证、参数传递方式等高级功能。</li><li><strong>扩展性</strong>：规范支持通过”x-“前缀的字段进行扩展，以适应特定的需求。</li></ol><p>OpenAPI文档通常以YAML或JSON格式编写，包含了API的详细信息，如API的路径、请求参数、响应参数、错误码等。文档的结构由以下几个主要部分组成：</p><ul><li><strong>openapi</strong>：指定OpenAPI规范的版本。</li><li><strong>info</strong>：提供API的元数据，如标题、描述、版本等。</li><li><strong>paths</strong>：描述API的路径和操作（如GET、POST、PUT、DELETE等）。</li><li><strong>components</strong>：包含可重用的构件，如参数、响应、安全方案等。</li><li><strong>security</strong>：定义API的安全要求。</li><li><strong>tags</strong>：用于对API进行逻辑分组。</li><li><strong>externalDocs</strong>：提供对外部文档的引用。</li></ul><p>使用OpenAPI规范，开发者可以更容易地理解API的工作原理，自动化工具可以根据规范生成文档、代码、测试脚本等，从而提高开发效率和API的可用性。此外，API管理平台和工具也广泛支持OpenAPI规范，使得API的管理和使用更加规范化和便捷化。</p><p><a target="_blank" rel="noopener" href="https://editor.swagger.io/">在线生成OpenAPI Swagger</a></p><h1 id="硬件"><a href="#硬件" class="headerlink" title="硬件"></a>硬件</h1><p><a target="_blank" rel="noopener" href="https://www.chiphell.com/thread-2506408-1-1.html">想自己组个或者4块A6000或者A100的服务器有点无从下手</a></p><p><a target="_blank" rel="noopener" href="https://new.qq.com/rain/a/20221119A06ZJI00">显卡为什么不能扩充升级显存？用最浅显的语言回答你，全是干货</a></p><h2 id="ollama-cpu占用高，gpu占用低"><a href="#ollama-cpu占用高，gpu占用低" class="headerlink" title="ollama cpu占用高，gpu占用低"></a>ollama cpu占用高，gpu占用低</h2><p><a target="_blank" rel="noopener" href="https://github.com/ollama/ollama/issues/1663#issuecomment-1946904131">模型需要全部载入到gpu的缓存中，才能运行</a>，而模型载入、<a href="#%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8%E8%AF%84%E4%BC%B0">推理时需要的显存如下</a></p><h2 id="显存占用评估"><a href="#显存占用评估" class="headerlink" title="显存占用评估"></a>显存占用评估</h2><p><a target="_blank" rel="noopener" href="https://qwen.readthedocs.io/zh-cn/latest/benchmark/speed_benchmark.html">参考文献</a></p><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/sinat_39620217/article/details/136910771">如何选择GPU显卡，带你对比A100/H100/4090性价比、训练/推理该使用谁？</a></p><p>实测自己的ollama可以运行qwen72b，这是为什么？虽然layer载入是39/81，本机内存64GB。</p><h1 id="chatGPT"><a href="#chatGPT" class="headerlink" title="chatGPT"></a>chatGPT</h1><h2 id="chatgpt"><a href="#chatgpt" class="headerlink" title="chatgpt"></a>chatgpt</h2><p><a target="_blank" rel="noopener" href="https://help.openai.com/en/articles/7102672-how-can-i-access-gpt-4">申请gpt4</a></p><p><a target="_blank" rel="noopener" href="https://help.openai.com/en/articles/7102672-how-can-i-access-gpt-4">参考文献</a></p><h2 id="MaxKB-Ollama"><a href="#MaxKB-Ollama" class="headerlink" title="MaxKB+Ollama"></a><a target="_blank" rel="noopener" href="https://blog.fit2cloud.com/?p=3fc407b2-1962-4196-b5b8-c7d27684b446">MaxKB+Ollama</a></h2><h2 id="扣子工作流"><a href="#扣子工作流" class="headerlink" title="扣子工作流"></a><a target="_blank" rel="noopener" href="https://www.coze.cn/store/workflow">扣子工作流</a></h2><h1 id="ubuntu安装"><a href="#ubuntu安装" class="headerlink" title="ubuntu安装"></a>ubuntu安装</h1><p>1、在windows系统下，首先下载磁盘分区工具diskgenius并安装。</p><p>2、下载并安装 启动引导软件 easyUEFI （试用版）。</p><p>3、选择并下载合适的ubuntu版本。</p><p>4、使用diskgenius分出一个区，格式化为FAT32，并将ubuntu解压至这个盘里。</p><p>5、启动easyUEFI，新建一个启动项，选择刚解压的那个ubuntu，设置成第一个启动项，然后重启。</p><h1 id="MISC"><a href="#MISC" class="headerlink" title="MISC"></a>MISC</h1><h2 id="深度学习PyTorch，TensorFlow中GPU利用率较低，CPU利用率很低，且模型训练速度很慢的问题总结与分析"><a href="#深度学习PyTorch，TensorFlow中GPU利用率较低，CPU利用率很低，且模型训练速度很慢的问题总结与分析" class="headerlink" title="深度学习PyTorch，TensorFlow中GPU利用率较低，CPU利用率很低，且模型训练速度很慢的问题总结与分析"></a><a target="_blank" rel="noopener" href="https://blog.csdn.net/qq_32998593/article/details/92849585">深度学习PyTorch，TensorFlow中GPU利用率较低，CPU利用率很低，且模型训练速度很慢的问题总结与分析</a></h2><h2 id="llama-cpp"><a href="#llama-cpp" class="headerlink" title="llama.cpp"></a>llama.cpp</h2><h3 id="编译llama-cpp出问题"><a href="#编译llama-cpp出问题" class="headerlink" title="编译llama.cpp出问题"></a>编译llama.cpp出问题</h3><p>我觉得版本不对，删掉了这个路径，为什么w64devkit还是回去这个目录去找。本地有一个cahe文件</p><details><summary>代码详情</summary><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">-- The CXX compiler identification is unknown</span><br><span class="line">CMake Error at CMakeLists.txt:2 (project):</span><br><span class="line">  The CMAKE_C_COMPILER:</span><br><span class="line"></span><br><span class="line">    C:&#x2F;Program Files&#x2F;mingw-w64&#x2F;x86_64-8.1.0-win32-seh-rt_v6-rev0&#x2F;mingw64&#x2F;bin&#x2F;gcc.exe</span><br><span class="line"></span><br><span class="line">  is not a full path to an existing compiler tool.</span><br><span class="line"></span><br><span class="line">  Tell CMake where to find the compiler by setting either the environment</span><br><span class="line">  variable &quot;CC&quot; or the CMake cache entry CMAKE_C_COMPILER to the full path to</span><br><span class="line">  the compiler, or to the compiler name if it is in the PATH.</span><br><span class="line"></span><br><span class="line"></span><br><span class="line">CMake Error at CMakeLists.txt:2 (project):</span><br><span class="line">  The CMAKE_CXX_COMPILER:</span><br><span class="line"></span><br><span class="line">    C:&#x2F;Program Files&#x2F;mingw-w64&#x2F;x86_64-8.1.0-win32-seh-rt_v6-rev0&#x2F;mingw64&#x2F;bin&#x2F;g++.exe</span><br><span class="line"></span><br><span class="line">  is not a full path to an existing compiler tool.</span><br><span class="line"></span><br><span class="line">  Tell CMake where to find the compiler by setting either the environment</span><br><span class="line">  variable &quot;CXX&quot; or the CMake cache entry CMAKE_CXX_COMPILER to the full path</span><br><span class="line">  to the compiler, or to the compiler name if it is in the PATH.</span><br><span class="line"></span><br></pre></td></tr></table></figure></details><h3 id="wsl-ubuntu安装"><a href="#wsl-ubuntu安装" class="headerlink" title="wsl-ubuntu安装"></a>wsl-ubuntu安装</h3><h4 id="使用代理"><a href="#使用代理" class="headerlink" title="使用代理"></a>使用代理</h4><p><code>export http_proxy=http://192.168.1.111:10810;</code></p><p><code>export https_proxy=http://192.168.1.111:10810;</code></p><h4 id="cc-not-found"><a href="#cc-not-found" class="headerlink" title="cc: not found"></a>cc: not found</h4><p>安装gcc g++</p><p><code>sudo apt install gcc</code></p><p><code>sudo apt-get update</code></p><details><summary>代码详情</summary><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">I LDFLAGS:</span><br><span class="line">&#x2F;bin&#x2F;sh: 1: cc: not found</span><br><span class="line">I CC:</span><br><span class="line">&#x2F;bin&#x2F;sh: 1: c++: not found</span><br><span class="line">I CXX:</span><br></pre></td></tr></table></figure></details><h4 id="ubuntu安装ananconda"><a href="#ubuntu安装ananconda" class="headerlink" title="ubuntu安装ananconda"></a>ubuntu安装ananconda</h4><p><code>apt install libgl1-mesa-glx libegl1-mesa libxrandr2 libxrandr2 libxss1 libxcursor1 libxcomposite1 libasound2 libxi6 libxtst6</code></p><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/JineD/article/details/129507719">参考文献</a></p><h2 id="ollama输出不稳定"><a href="#ollama输出不稳定" class="headerlink" title="ollama输出不稳定"></a>ollama输出不稳定</h2><p>输出不可以复现是为什么，待补充</p><h1 id="rag测试用例"><a href="#rag测试用例" class="headerlink" title="rag测试用例"></a>rag测试用例</h1><p>curl如何设置代理,linux运维,## 使用代理</p></div><footer class="post-footer"><div class="post-nav"><div class="post-nav-item"><a href="/2023/08/15/iot/" rel="prev" title="iot"><i class="fa fa-chevron-left"></i> iot</a></div><div class="post-nav-item"><a href="/2023/08/16/stuff-manage/" rel="next" title="stuff-manage">stuff-manage <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="gitalk-container"></div><script>window.addEventListener("tabs:register",()=>{let t=CONFIG.comments["activeClass"];var e;(t=CONFIG.comments.storage?localStorage.getItem("comments_active")||t:t)&&(e=document.querySelector(`a[href="#comment-${t}"]`))&&e.click()}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{t.target.matches(".tabs-comment .tab-content .tab-pane")&&(t=t.target.classList[1],localStorage.setItem("comments_active",t))})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B"><span class="nav-text">模型</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E7%AE%A1%E7%90%86%E5%B9%B3%E5%8F%B0%EF%BC%9Aollama"><span class="nav-text">模型管理平台：ollama</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#ollama%E7%9A%84%E6%A8%A1%E5%9E%8B%E8%B7%AF%E5%BE%84"><span class="nav-text">ollama的模型路径</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ollama%E7%9A%84%E9%85%8D%E7%BD%AE%E6%96%87%E4%BB%B6"><span class="nav-text">ollama的配置文件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ollama%E4%BF%AE%E6%94%B9%E6%A8%A1%E5%9E%8B%E7%9A%84%E4%BD%8D%E7%BD%AE"><span class="nav-text">ollama修改模型的位置</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%88%AB%E7%9A%84%E6%9C%BA%E5%99%A8%E7%9B%91%E5%90%AC%E5%9C%B0%E5%9D%80%E9%9C%80%E8%A6%81%E6%94%B9%E6%88%900-0-0-0"><span class="nav-text">别的机器监听地址需要改成0.0.0.0</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#maxkb"><span class="nav-text">maxkb</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#ollama%E6%A8%A1%E5%9E%8B%E7%9A%84%E6%94%AF%E6%8C%81%E9%95%BF%E5%BA%A6"><span class="nav-text">ollama模型的支持长度</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9F%A5%E7%9C%8Bollama%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%88%E6%9C%AC"><span class="nav-text">查看ollama模型的版本</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%9F%A5%E7%9C%8Bollama%E7%9A%84%E7%89%88%E6%9C%AC"><span class="nav-text">查看ollama的版本</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B0%83%E7%94%A8ollama%E7%9A%84%E6%97%B6%E5%80%99%E4%BD%BF%E7%94%A8apikey"><span class="nav-text">调用ollama的时候使用apikey</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ollama-vs-vllm"><span class="nav-text">ollama vs vllm</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83"><span class="nav-text">模型微调</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%85%B6%E4%BB%96%E6%A8%A1%E5%9E%8B"><span class="nav-text">其他模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E4%BB%8Ehugging-face%E4%B8%8A%E4%B8%8B%E8%BD%BD%E6%A8%A1%E5%9E%8B"><span class="nav-text">从hugging face上下载模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#mteb%E6%8E%92%E8%A1%8C%E6%A6%9C"><span class="nav-text">mteb排行榜</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%90%91%E9%87%8F%E6%A8%A1%E5%9E%8B"><span class="nav-text">向量模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#TencentBAC-Conan-embedding-v1"><span class="nav-text">TencentBAC&#x2F;Conan-embedding-v1</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%A4%9A%E6%A8%A1%E6%80%81%E6%A8%A1%E5%9E%8B"><span class="nav-text">多模态模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#llava"><span class="nav-text">llava</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#coder%E6%A8%A1%E5%9E%8B"><span class="nav-text">coder模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#deepseek-coder"><span class="nav-text">deepseek-coder</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#LLM%E6%A8%A1%E5%9E%8B"><span class="nav-text">LLM模型</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#qwen2-5"><span class="nav-text">qwen2.5</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E6%8E%92%E8%A1%8C"><span class="nav-text">模型排行</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#qwen%E6%A8%A1%E5%9E%8B%E7%9A%84%E7%89%88%E6%9C%AC%E5%8C%BA%E5%88%AB"><span class="nav-text">qwen模型的版本区别</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A8%A1%E5%9E%8B%E8%BD%AC%E5%8C%96%E3%80%81%E9%87%8F%E5%8C%96"><span class="nav-text">模型转化、量化</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%89%93%E6%A0%87%E6%A8%A1%E5%9E%8B"><span class="nav-text">打标模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%94%A8%E4%BA%8E%E5%8F%91%E7%A5%A8%E8%AF%86%E5%88%AB%E7%9A%84%E5%BE%AE%E8%B0%83-Transformer-%E6%A8%A1%E5%9E%8B"><span class="nav-text">用于发票识别的微调 Transformer 模型</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#TrOCR%E6%A8%A1%E5%9E%8B%E5%BE%AE%E8%B0%83%E5%AE%9E%E6%88%98%E3%80%90%E5%BE%AE%E8%BD%AF%E5%85%89%E5%AD%A6%E5%AD%97%E7%AC%A6%E8%AF%86%E5%88%AB%E3%80%91"><span class="nav-text">TrOCR模型微调实战【微软光学字符识别】</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#llm%E5%BA%94%E7%94%A8%E5%B9%B3%E5%8F%B0"><span class="nav-text">llm应用平台</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#workflow%E5%B9%B3%E5%8F%B0%E6%8E%A8%E8%8D%90"><span class="nav-text">workflow平台推荐</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ragflow"><span class="nav-text">ragflow</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#npm%E4%B8%8B%E8%BD%BD%E6%97%B6%E6%8A%A5%E9%94%99unable-to-verify-the-first-certificate%E8%A7%A3%E5%86%B3%E6%96%B9%E6%B3%95"><span class="nav-text">npm下载时报错unable to verify the first certificate解决方法</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#Ollama-%E7%BB%93%E5%90%88-maxkb%E5%81%9A%E6%9C%AC%E5%9C%B0%E7%9F%A5%E8%AF%86%E5%BA%93"><span class="nav-text">Ollama 结合 maxkb做本地知识库</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E3%80%90%E6%95%99%E7%A8%8B%E3%80%91Windows%E7%B3%BB%E7%BB%9F%E6%9C%AC%E5%9C%B0%E9%83%A8%E7%BD%B2Ollama-MaxKB%E5%AE%89%E8%A3%85%E6%95%99%E7%A8%8B"><span class="nav-text">【教程】Windows系统本地部署Ollama+MaxKB安装教程</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#maxkb%E5%8E%BB%E6%8E%89%E7%A4%BE%E5%8C%BA%E7%89%88%E9%99%90%E5%88%B6"><span class="nav-text">maxkb去掉社区版限制</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%B4%A2%E5%BC%95%E6%97%B6%E9%97%B4%E6%B5%8B%E8%AF%95"><span class="nav-text">索引时间测试</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%9B%AE%E5%BD%95%E6%8C%82%E8%BD%BD"><span class="nav-text">目录挂载</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#RAG%E6%8A%80%E6%9C%AF"><span class="nav-text">RAG技术</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%A8%80%E7%96%8F%E6%A3%80%E7%B4%A2-Sparse-Retrieval-%E5%92%8C%E7%A8%A0%E5%AF%86%E6%A3%80%E7%B4%A2-Dense-Retrieval-%E7%9A%84%E5%8C%BA%E5%88%AB"><span class="nav-text">稀疏检索(Sparse Retrieval)和稠密检索(Dense Retrieval)的区别</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%8F%82%E8%80%83%E6%96%87%E7%8C%AE"><span class="nav-text">参考文献</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#dify"><span class="nav-text">dify</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#OpenAPI-Swagger%E6%A0%87%E5%87%86%E6%98%AF%E4%BB%80%E4%B9%88"><span class="nav-text">OpenAPI Swagger标准是什么</span></a></li></ol></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#%E7%A1%AC%E4%BB%B6"><span class="nav-text">硬件</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#ollama-cpu%E5%8D%A0%E7%94%A8%E9%AB%98%EF%BC%8Cgpu%E5%8D%A0%E7%94%A8%E4%BD%8E"><span class="nav-text">ollama cpu占用高，gpu占用低</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%98%BE%E5%AD%98%E5%8D%A0%E7%94%A8%E8%AF%84%E4%BC%B0"><span class="nav-text">显存占用评估</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#chatGPT"><span class="nav-text">chatGPT</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#chatgpt"><span class="nav-text">chatgpt</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#MaxKB-Ollama"><span class="nav-text">MaxKB+Ollama</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%89%A3%E5%AD%90%E5%B7%A5%E4%BD%9C%E6%B5%81"><span class="nav-text">扣子工作流</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#ubuntu%E5%AE%89%E8%A3%85"><span class="nav-text">ubuntu安装</span></a></li><li class="nav-item nav-level-1"><a class="nav-link" href="#MISC"><span class="nav-text">MISC</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0PyTorch%EF%BC%8CTensorFlow%E4%B8%ADGPU%E5%88%A9%E7%94%A8%E7%8E%87%E8%BE%83%E4%BD%8E%EF%BC%8CCPU%E5%88%A9%E7%94%A8%E7%8E%87%E5%BE%88%E4%BD%8E%EF%BC%8C%E4%B8%94%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E9%80%9F%E5%BA%A6%E5%BE%88%E6%85%A2%E7%9A%84%E9%97%AE%E9%A2%98%E6%80%BB%E7%BB%93%E4%B8%8E%E5%88%86%E6%9E%90"><span class="nav-text">深度学习PyTorch，TensorFlow中GPU利用率较低，CPU利用率很低，且模型训练速度很慢的问题总结与分析</span></a></li><li class="nav-item nav-level-2"><a class="nav-link" href="#llama-cpp"><span class="nav-text">llama.cpp</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BC%96%E8%AF%91llama-cpp%E5%87%BA%E9%97%AE%E9%A2%98"><span class="nav-text">编译llama.cpp出问题</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#wsl-ubuntu%E5%AE%89%E8%A3%85"><span class="nav-text">wsl-ubuntu安装</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BD%BF%E7%94%A8%E4%BB%A3%E7%90%86"><span class="nav-text">使用代理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#cc-not-found"><span class="nav-text">cc: not found</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#ubuntu%E5%AE%89%E8%A3%85ananconda"><span class="nav-text">ubuntu安装ananconda</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#ollama%E8%BE%93%E5%87%BA%E4%B8%8D%E7%A8%B3%E5%AE%9A"><span class="nav-text">ollama输出不稳定</span></a></li></ol></li><li class="nav-item nav-level-1"><a class="nav-link" href="#rag%E6%B5%8B%E8%AF%95%E7%94%A8%E4%BE%8B"><span class="nav-text">rag测试用例</span></a></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">ednow</p><div class="site-description" itemprop="description"></div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">306</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">1</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">62</span> <span class="site-state-item-name">标签</span></a></div></nav></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">ednow</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">7.7m</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">116:01</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动</div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-divider">|</span> <span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>NexT.utils.loadComments(document.querySelector("#gitalk-container"),()=>{NexT.utils.getScript("//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js",()=>{new Gitalk({clientID:"04b9fe1c5636beb4acc4",clientSecret:"8ccb8829887eac219a8fdb018878fd0cf088a7ac",repo:"gittalk-comment",owner:"ednow",admin:["ednow"],id:"f2fc5b440f138a59f7f5a65ba38d4bd4",language:"zh-CN",distractionFreeMode:!0}).render("gitalk-container")},window.Gitalk)})</script><style>[bg-lazy]{background-image:none!important;background-color:#eee!important}</style><script>window.imageLazyLoadSetting={isSPA:!1,preloadRatio:1,processImages:null}</script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})})</script><script>!function(r){r.imageLazyLoadSetting.processImages=t;var e=r.imageLazyLoadSetting.isSPA,n=r.imageLazyLoadSetting.preloadRatio||1,c=a();function a(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(){e&&(c=a());for(var t,o=0;o<c.length;o++)0<=(t=(t=c[o]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(r.innerHeight*n||document.documentElement.clientHeight*n)&&function(){var t,e,n,a=c[o],i=function(){c=c.filter(function(t){return a!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(a)};(t=a).hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),i()):(e=new Image,n=t.getAttribute("data-original"),e.onload=function(){t.src=n,t.removeAttribute("data-original"),i()},t.src!==n&&(e.src=n))}()}function i(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",i),r.addEventListener("resize",i),r.addEventListener("orientationchange",i)}(this)</script></body></html>