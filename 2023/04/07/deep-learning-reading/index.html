<!DOCTYPE html><html lang="zh-CN"><head><meta charset="UTF-8"><meta name="viewport" content="width=device-width,initial-scale=1,maximum-scale=2"><meta name="theme-color" content="#222"><meta name="generator" content="Hexo 5.3.0"><link rel="apple-touch-icon" sizes="180x180" href="/images/apple-touch-icon-next.png"><link rel="icon" type="image/png" sizes="32x32" href="/images/favicon-32x32-next.png"><link rel="icon" type="image/png" sizes="16x16" href="/images/favicon-16x16-next.png"><link rel="mask-icon" href="/images/logo.svg" color="#222"><link rel="stylesheet" href="/css/main.css"><link rel="stylesheet" href="//fonts.googleapis.com/css?family=Lato:300,300italic,400,400italic,700,700italic&display=swap&subset=latin,latin-ext"><link rel="stylesheet" href="/lib/font-awesome/css/all.min.css"><script id="hexo-configurations">var NexT=window.NexT||{},CONFIG={hostname:"ednow.github.io",root:"/",scheme:"Muse",version:"7.8.0",exturl:!1,sidebar:{position:"left",width:400,display:"post",padding:18,offset:12,onmobile:!1},copycode:{enable:!1,show_result:!1,style:null},back2top:{enable:!0,sidebar:!1,scrollpercent:!1},bookmark:{enable:!1,color:"#222",save:"auto"},fancybox:!1,mediumzoom:!1,lazyload:!1,pangu:!1,comments:{style:"tabs",active:null,storage:!0,lazyload:!1,nav:null},algolia:{hits:{per_page:10},labels:{input_placeholder:"Search for Posts",hits_empty:"We didn't find any results for the search: ${query}",hits_stats:"${hits} results found in ${time} ms"}},localsearch:{enable:!0,trigger:"auto",top_n_per_article:1,unescape:!1,preload:!1},motion:{enable:!0,async:!1,transition:{post_block:"fadeIn",post_header:"slideDownIn",post_body:"slideDownIn",coll_header:"slideLeftIn",sidebar:"slideUpIn"}},path:"search.xml"}</script><meta name="description" content="阅读花书github 答疑标量、向量、矩阵、张量之间的区别和联系 TODO图像补充图像补充一律以如下标准进行标注 【page:other(方便找到具体是什么图像)】 例如【62:图3.3: logistic sigmoid函数。】,代表在62页的一副下标为图3.3: logistic sigmoid函数。的图片。 并且这个page号一定是能通过pdf阅读器直接导航到的页码数 可以用【.*?:.*?"><meta property="og:type" content="article"><meta property="og:title" content="deep-learning-reading"><meta property="og:url" content="http://ednow.github.io/2023/04/07/deep-learning-reading/index.html"><meta property="og:site_name" content="ednow"><meta property="og:description" content="阅读花书github 答疑标量、向量、矩阵、张量之间的区别和联系 TODO图像补充图像补充一律以如下标准进行标注 【page:other(方便找到具体是什么图像)】 例如【62:图3.3: logistic sigmoid函数。】,代表在62页的一副下标为图3.3: logistic sigmoid函数。的图片。 并且这个page号一定是能通过pdf阅读器直接导航到的页码数 可以用【.*?:.*?"><meta property="og:locale" content="zh_CN"><meta property="article:published_time" content="2023-04-07T06:46:40.000Z"><meta property="article:modified_time" content="2023-08-13T14:32:49.095Z"><meta property="article:author" content="ednow"><meta property="article:tag" content="深度学习"><meta property="article:tag" content="神经网络"><meta name="twitter:card" content="summary"><link rel="canonical" href="http://ednow.github.io/2023/04/07/deep-learning-reading/"><script id="page-configurations">CONFIG.page={sidebar:"",isHome:!1,isPost:!0,lang:"zh-CN"}</script><title>deep-learning-reading | ednow</title><script async src="https://www.googletagmanager.com/gtag/js?id=G-XQGJ63ZD9Y"></script><script>function gtag(){dataLayer.push(arguments)}CONFIG.hostname===location.hostname&&(window.dataLayer=window.dataLayer||[],gtag("js",new Date),gtag("config","G-XQGJ63ZD9Y"))</script><script>var _hmt=_hmt||[];!function(){var e=document.createElement("script"),t=(e.src="https://hm.baidu.com/hm.js?83f04257c97e81cca692d7c4c7fbbc9a",document.getElementsByTagName("script")[0]);t.parentNode.insertBefore(e,t)}()</script><noscript><style>.sidebar-inner,.use-motion .brand,.use-motion .collection-header,.use-motion .comments,.use-motion .menu-item,.use-motion .pagination,.use-motion .post-block,.use-motion .post-body,.use-motion .post-header{opacity:initial}.use-motion .site-subtitle,.use-motion .site-title{opacity:initial;top:initial}.use-motion .logo-line-before i{left:initial}.use-motion .logo-line-after i{right:initial}</style></noscript></head><body itemscope itemtype="http://schema.org/WebPage"><div class="container use-motion"><div class="headband"></div><header class="header" itemscope itemtype="http://schema.org/WPHeader"><div class="header-inner"><div class="site-brand-container"><div class="site-nav-toggle"><div class="toggle" aria-label="切换导航栏"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div></div><div class="site-meta"><a href="/" class="brand" rel="start"><span class="logo-line-before"><i></i></span><h1 class="site-title">ednow</h1><span class="logo-line-after"><i></i></span></a></div><div class="site-nav-right"><div class="toggle popup-trigger"><i class="fa fa-search fa-fw fa-lg"></i></div></div></div><nav class="site-nav"><ul id="menu" class="main-menu menu"><li class="menu-item menu-item-home"><a href="/" rel="section"><i class="fa fa-home fa-fw"></i>首页</a></li><li class="menu-item menu-item-tags"><a href="/tags/" rel="section"><i class="fa fa-tags fa-fw"></i>标签</a></li><li class="menu-item menu-item-categories"><a href="/categories/" rel="section"><i class="fa fa-th fa-fw"></i>分类</a></li><li class="menu-item menu-item-archives"><a href="/archives/" rel="section"><i class="fa fa-archive fa-fw"></i>归档</a></li><li class="menu-item menu-item-search"><a role="button" class="popup-trigger"><i class="fa fa-search fa-fw"></i>搜索</a></li></ul></nav><div class="search-pop-overlay"><div class="popup search-popup"><div class="search-header"><span class="search-icon"><i class="fa fa-search"></i></span><div class="search-input-container"><input autocomplete="off" autocapitalize="off" placeholder="搜索..." spellcheck="false" type="search" class="search-input"></div><span class="popup-btn-close"><i class="fa fa-times-circle"></i></span></div><div id="search-result"><div id="no-result"><i class="fa fa-spinner fa-pulse fa-5x fa-fw"></i></div></div></div></div></div></header><div class="back-to-top"><i class="fa fa-arrow-up"></i> <span>0%</span></div><main class="main"><div class="main-inner"><div class="content-wrap"><div class="content post posts-expand"><article itemscope itemtype="http://schema.org/Article" class="post-block" lang="zh-CN"><link itemprop="mainEntityOfPage" href="http://ednow.github.io/2023/04/07/deep-learning-reading/"><span hidden itemprop="author" itemscope itemtype="http://schema.org/Person"><meta itemprop="image" content="/images/avatar.gif"><meta itemprop="name" content="ednow"><meta itemprop="description" content=""></span><span hidden itemprop="publisher" itemscope itemtype="http://schema.org/Organization"><meta itemprop="name" content="ednow"></span><header class="post-header"><h1 class="post-title" itemprop="name headline">deep-learning-reading</h1><div class="post-meta"><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar"></i> </span><span class="post-meta-item-text">发表于</span> <time title="创建时间：2023-04-07 14:46:40" itemprop="dateCreated datePublished" datetime="2023-04-07T14:46:40+08:00">2023-04-07</time> </span><span class="post-meta-item"><span class="post-meta-item-icon"><i class="far fa-calendar-check"></i> </span><span class="post-meta-item-text">更新于</span> <time title="修改时间：2023-08-13 22:32:49" itemprop="dateModified" datetime="2023-08-13T22:32:49+08:00">2023-08-13</time> </span><span class="post-meta-item" title="阅读次数" id="busuanzi_container_page_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="post-meta-item-text">阅读次数：</span> <span id="busuanzi_value_page_pv"></span></span><br><span class="post-meta-item" title="本文字数"><span class="post-meta-item-icon"><i class="far fa-file-word"></i> </span><span class="post-meta-item-text">本文字数：</span> <span>16k</span> </span><span class="post-meta-item" title="阅读时长"><span class="post-meta-item-icon"><i class="far fa-clock"></i> </span><span class="post-meta-item-text">阅读时长 &asymp;</span> <span>15 分钟</span></span></div></header><div class="post-body" itemprop="articleBody"><h1 id="阅读花书"><a href="#阅读花书" class="headerlink" title="阅读花书"></a>阅读花书</h1><p><a target="_blank" rel="noopener" href="https://github.com/exacity/deeplearningbook-chinese">github</a></p><h2 id="答疑"><a href="#答疑" class="headerlink" title="答疑"></a>答疑</h2><p><a target="_blank" rel="noopener" href="https://blog.csdn.net/sinat_29957455/article/details/117396685">标量、向量、矩阵、张量之间的区别和联系</a></p><h3 id="TODO"><a href="#TODO" class="headerlink" title="TODO"></a>TODO</h3><h4 id="图像补充"><a href="#图像补充" class="headerlink" title="图像补充"></a>图像补充</h4><p>图像补充一律以如下标准进行标注<code>【page:other(方便找到具体是什么图像)】</code></p><p>例如<code>【62:图3.3: logistic sigmoid函数。】</code>,代表在<code>62</code>页的一副下标为<code>图3.3: logistic sigmoid函数。</code>的图片。</p><p>并且这个page号一定是能通过pdf阅读器直接导航到的页码数</p><p>可以用<code>【.*?:.*?】</code>正则去寻找所有需要进行补充记录的截图</p><h4 id="张量元素如何用latex表示"><a href="#张量元素如何用latex表示" class="headerlink" title="张量元素如何用latex表示"></a>张量元素如何用latex表示</h4><h4 id="整流线性单元"><a href="#整流线性单元" class="headerlink" title="整流线性单元"></a>整流线性单元</h4><p>Rectified Linear Unit，ReLU</p><p>$f(x) = max(0, x)$</p><h4 id="贪婪逐层预训练"><a href="#贪婪逐层预训练" class="headerlink" title="贪婪逐层预训练"></a>贪婪逐层预训练</h4><details><summary>ChatGPT</summary><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">贪婪逐层预训练（Greedy Layer-wise Pretraining）是一种用于训练深度神经网络的方法。它通常用于解决训练深度神经网络时遇到的梯度消失和过拟合等问题。</span><br><span class="line"></span><br><span class="line">贪婪逐层预训练的基本思想是：将深度神经网络分成多个层，然后逐层进行训练，每次训练只使用当前层的输入和输出。在每个层的训练中，先训练一个浅层网络，然后将其参数传递给下一层网络，用作下一层网络的初始参数。这样，通过逐层训练，每一层网络都可以学习到比较好的参数，从而使整个深度神经网络可以更好地拟合数据。</span><br><span class="line"></span><br><span class="line">贪婪逐层预训练的过程分为两个阶段：</span><br><span class="line"></span><br><span class="line">1. 预训练阶段：逐层训练每个浅层网络，并将其参数传递给下一层网络，用作下一层网络的初始参数。这个过程可以使用无监督学习算法，如受限玻尔兹曼机（Restricted Boltzmann Machines，RBMs）等。</span><br><span class="line"></span><br><span class="line">2. 微调阶段：使用有监督学习算法，如反向传播（Back Propagation），对整个深度神经网络进行训练，调整网络的所有参数，以最小化损失函数。</span><br><span class="line"></span><br><span class="line">贪婪逐层预训练的优点是可以在训练深度神经网络时有效地避免梯度消失和过拟合等问题，提高了深度神经网络的泛化能力。但是，由于现在的深度神经网络通常使用更加先进的优化算法和初始化方法，因此贪婪逐层预训练已经不再是必需的步骤，而是一种可选的方法。</span><br></pre></td></tr></table></figure></details><details><summary>代码详情</summary><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br></pre></td><td class="code"><pre><span class="line">除了传统的梯度下降算法，现在有很多更先进的优化算法，下面列举几种比较常见的：</span><br><span class="line"></span><br><span class="line">1. 动量法（Momentum）：在梯度下降的基础上，加入了一个动量项，来增加学习的稳定性和速度。</span><br><span class="line"></span><br><span class="line">2. 自适应学习率优化算法：包括Adagrad、Adadelta、RMSprop等。这类算法可以自适应地调整学习率，以适应不同参数的更新需求，提高了模型的训练效率和准确性。</span><br><span class="line"></span><br><span class="line">3. Adam算法：结合了动量法和自适应学习率优化算法的优点，具有高效和准确的特点，广泛应用于深度学习中。</span><br><span class="line"></span><br><span class="line">4. Nadam算法：Adam算法的改进版，具有更快的收敛速度和更好的泛化性能。</span><br><span class="line"></span><br><span class="line">5. Adafactor算法：基于自适应学习率的优化算法，具有更快的收敛速度和更好的泛化性能，适用于大规模模型和数据集。</span><br><span class="line"></span><br><span class="line">6. L-BFGS算法：一种基于拟牛顿法的优化算法，特别适用于训练小型模型和对计算效率要求高的任务。</span><br><span class="line"></span><br><span class="line">7. SGDM算法：结合了随机梯度下降和动量法的优点，具有更高的训练速度和更好的泛化性能。</span><br><span class="line"></span><br><span class="line">这些优化算法都具有不同的特点和适用场景，在深度学习中广泛应用。在选择算法时，需要根据具体的任务和数据集来选择合适的优化算法。</span><br></pre></td></tr></table></figure></details><h2 id="前言"><a href="#前言" class="headerlink" title="前言"></a>前言</h2><p>本书讨论一种解决方案。该方案可以让计算机从经验中学习，并根据层次化的概念体系来理解世界，而每个概念则通过与某些相对简单的概念之间的关系来定义。让计算机从经验获取知识，可以避免由人类来给计算机形式化地指定它需要的所有知识。层次化的概念让计算机构建较简单的概念来学习复杂概念。如果绘制出这些概念如何建立在彼此之上的图，我们将得到一张‘‘深’’（层次很多）的图。基于这个原因，我们称这种方法为AI 深度学习（deep learning）。</p><p>抽象和形式化的任务对人类而言是最困难的脑力任务之一，但对计算机而言却属于最容易的。人工智能的一个关键挑战就是如何将这些非形式化的知识传达给计算机。</p><p>一些人工智能项目力求将关于世界的知识用形式化的语言进行硬编码(hard-code)。</p><p>AI 系统需要具备自己获取知识的能力，即从原始数据中提取模式的能力。这种能力被称为机器学习（machine learning）。引入机器学习使计算机能够解决涉及现实世界知识的问题，并能作出看似主观的决策。比如，一个被称为逻辑回归（logistic regression）的简单机器学习算法可以决定是否建议剖腹产(Mor-Yosef et al., 1990)。而同样是简单机器学习算法的朴素贝叶斯（naive Bayes）则可以区分垃圾电子邮件和合法电子邮件。</p><p>简单的机器学习算法的性能在很大程度上依赖于给定数据的表示（repre-sentation）。</p><p>许多人工智能任务都可以通过以下方式解决：先提取一个合适的特征集，然后将这些特征提供给简单的机器学习算法。</p><p>对于许多任务来说，我们很难知道应该提取哪些特征。解决这个问题的途径之一是使用机器学习来发掘表示本身，而不仅仅把表示映射到输出。这种方法我们称之为表示学习（representation learning）。表示学习算法的典型例子是自编码器（autoencoder）。自编码器由一个编码器（encoder）函数和一个解码器（decoder）函数组合而成。编码器函数将输入数据转换为一种不同的表示，而解码器函数则将这个新的表示转换到原来的形式。我们期望当输入数据经过编码器和解码器之后尽可能多地保留信息，同时希望新的表示有各种好的特性，这也是自编码器的训练目标。为了实现不同的特性，我们可以设计不同形式的自编码器。</p><p>当设计特征或设计用于学习特征的算法时，我们的目标通常是分离出能解释观察数据的变差因素（factors of variation）。在此背景下，‘‘因素’’ 这个词仅指代影响的不同来源；因素通常不是乘性组合。这些因素通常是不能被直接观察到的量。相反，它们可能是现实世界中观察不到的物体或者不可观测的力，但会影响可观测的量。为了对观察到的数据提供有用的简化解释或推断其原因，它们还可能以概念的形式存在于人类的思维中。它们可以被看作数据的概念或者抽象，帮助我们了解这些数据的丰富多样性。当分析语音记录时，变差因素包括说话者的年龄、性别、他们的口音和他们正在说的词语。当分析汽车的图像时，变差因素包括汽车的位置、它的颜色、太阳的角度和亮度。</p><p>深度学习（deep learning）通过其他较简单的表示来表达复杂表示，解决了表示学习中的核心问题。</p><p>深度学习将所需的复杂映射分解为一系列嵌套的简单映射（每个由模型的不同层描述）来解决这一难题。输入展示在可见层（visible layer），这样命名的原因是因为它包含我们能观察到的变量。然后是一系列从图像中提取越来越多抽象特征的<strong>隐藏层</strong>（hidden layer）。因为它们的值不在数据中给出，所以将这些层称为‘‘隐藏”; 模型必须确定哪些概念有利于解释观察数据中的关系。</p><p>深度学习模型的典型例子是前馈深度网络或<strong>多层感知机</strong>（multilayer perceptron, MLP）。多层感知机仅仅是一个将一组输入值映射到输出值的数学函数。该函数由许多较简单的函数复合而成。我们可以认为不同数学函数的每一次应用都为输入提供了新的表示。</p><p>学习数据的正确表示的想法是解释深度学习的一个视角。另一个视角是深度促使计算机学习一个多步骤的计算机程序。每一层表示都可以被认为是并行执行另一组指令之后计算机的存储器状态。更深的网络可以按顺序执行更多的指令。顺序指令提供了极大的能力，因为后面的指令可以参考早期指令的结果。从这个角度上看，在某层激活函数里，并非所有信息都蕴涵着解释输入的变差因素。表示还存储着状态信息，用于帮助程序理解输入。这里的状态信息类似于传统计算机程序中的计数器或指针。它与具体的输入内容无关，但有助于模型组织其处理过程。</p><p>目前主要有两种度量模型深度的方式。第一种方式是基于评估架构所需执行的顺序指令的数目。另一种是在深度概率模型中使用的方法，它不是将计算图的深度视为模型深度，而是将描述概念彼此如何关联的图的深度视为模型深度。</p><h3 id="深度学习的历史趋势"><a href="#深度学习的历史趋势" class="headerlink" title="深度学习的历史趋势"></a>深度学习的历史趋势</h3><h4 id="神经网络的众多名称和命运变迁"><a href="#神经网络的众多名称和命运变迁" class="headerlink" title="神经网络的众多名称和命运变迁"></a>神经网络的众多名称和命运变迁</h4><p>一般来说，目前为止深度学习已经经历了三次发展浪潮：20 世纪40 年代到60 年代深度学习的雏形出现在控制论（cybernetics）中，20 世纪80 年代到90 年代深度学习表现为联结主义（connectionism），直到2006 年，才真正以深度学习之名复兴。</p><p>现代术语“深度学习’’ 超越了目前机器学习模型的神经科学观点。它诉诸于学习多层次组合这一更普遍的原理，这一原理也可以应用于那些并非受神经科学启发的机器学习框架。</p><h5 id="第一阶段"><a href="#第一阶段" class="headerlink" title="第一阶段"></a>第一阶段</h5><p>现代深度学习的最早前身是从神经科学的角度出发的简单线性模型。这些模型被设计为使用一组 $n$ 个输入 $x_1,\cdots,x_n$ 并将它们与一个输出 $y$ 相关联。这些模型希望学习一组权重 $w_1,\cdots,w_n$ ，并计算它们的输出 $f(x, w) = x_1 w_1 + \cdots + x_n w_n$ 。这第一波神经网络研究浪潮被称为<strong>控制论</strong>。</p><p>McCulloch-Pitts 神经元(McCulloch and Pitts, 1943) 是脑功能的早期模型。该线性模型通过检验函数 $f(x,w)$ 的正负来识别两种不同类别的输入。显然，模型的权重需要正确设置后才能使模型的输出对应于期望的类别。这些权重可以由操作人员设定。在20 世纪50 年代，感知机(Rosenblatt, 1956, 1958) 成为第一个能根据每个类别的输入样本来学习权重的模型。约在同一时期，自适应线性单元(adaptivelinear element, ADALINE) 简单地返回函数 $f(x)$ 本身的值来预测一个实数(Widrowand Hoff, 1960)，并且它还可以学习从数据预测这些数。</p><p>用于调节ADALINE 权重的训练算法是被称为<strong>随机梯度下降</strong>（stochastic gradient descent）的一种特例。稍加改进后的随机梯度下降算法仍然是当今深度学习的主要训练算法。</p><p>基于感知机和ADALINE 中使用的函数 $f(x, w)$ 的模型被称为<strong>线性模型</strong>（linearmodel）。</p><p>线性模型有很多局限性。最著名的是，它们无法学习异或（XOR）函数。</p><h5 id="第二阶段"><a href="#第二阶段" class="headerlink" title="第二阶段"></a>第二阶段</h5><p>在20 世纪80 年代，神经网络研究的第二次浪潮在很大程度上是伴随一个被称为<strong>联结主义</strong>（connectionism）或<strong>并行分布处理</strong>( parallel distributed processing) 潮流而出现的。联结主义的中心思想是，当网络将大量简单的计算单元连接在一起时可以实现智能行为。这种见解同样适用于生物神经系统中的神经元，因为它和计算模型中隐藏单元起着类似的作用。</p><p>其中一个概念是<strong>分布式表示</strong>（distributed representation）(Hinton et al., 1986)。其思想是：系统的每一个输入都应该由多个特征表示，并且每一个特征都应该参与到多个可能输入的表示。例如，假设我们有一个能够识别红色、绿色、或蓝色的汽车、卡车和鸟类的视觉系统，表示这些输入的其中一个方法是将九个可能的组合：红卡车，红汽车，红鸟，绿卡车等等使用单独的神经元或隐藏单元激活。这需要九个不同的神经元，并且每个神经必须独立地学习颜色和对象身份的概念。改善这种情况的方法之一是使用分布式表示，即用三个神经元描述颜色，三个神经元描述对象身份。这仅仅需要6 个神经元而不是9 个，并且描述红色的神经元能够从汽车、卡车和鸟类的图像中学习红色，而不仅仅是从一个特定类别的图像中学习。分布式表示的概念是本书的核心，我们将在第十五章中更加详细地描述。</p><p>目前大多数神经网络是基于一个称为整流线性单元（rectified linear unit）的神经单元模型。</p><p>反向传播在训练具有内部表示的深度神经网络中的成功使用以及反向传播算法的普及(Rumelhart et al., 1986c; LeCun, 1987)。</p><p>长短期记忆（long short-term memory, LSTM）网络来解决这些难题。如今，LSTM 在许多序列建模任务中广泛应用，包括Google 的许多自然语言处理任务。</p><p>神经网络研究的第二次浪潮一直持续到上世纪90 年代中期。基于神经网络和其他AI技术的创业公司开始寻求投资，其做法野心勃勃但不切实际。当AI研究不能实现这些不合理的期望时，投资者感到失望。同时，机器学习的其他领域取得了进步。比如，核方法(Boser et al., 1992; Cortes and Vapnik, 1995; Schölkopf et al., 1999)和图模型(Jordan, 1998) 都在很多重要任务上实现了很好的效果。这两个因素导致了神经网络热潮的第二次衰退，并一直持续到2007 年。</p><h5 id="第三阶段"><a href="#第三阶段" class="headerlink" title="第三阶段"></a>第三阶段</h5><p>神经网络研究的第三次浪潮始于2006 年的突破。Geoffrey Hinton 表明名为深度信念网络的神经网络可以使用一种称为贪婪逐层预训练的策略来有效地训练(Hinton et al., 2006a)，我们将在第15.1 节中更详细地描述。其他CIFAR 附属研究小组很快表明，同样的策略可以被用来训练许多其他类型的深度网络(Bengio andLeCun, 2007a; Ranzato et al., 2007b)，并能系统地帮助提高在测试样例上的泛化能力。</p><p>第三次浪潮已开始着眼于新的无监督学习技术和深度模型在小数据集的泛化能力，但目前更多的兴趣点仍是比较传统的监督学习算法和深度模型充分利用大型标注数据集的能力。</p><p>目前在复杂的任务达到人类水平的学习算法，与20 世纪80 年代努力解决玩具问题(toyproblem) 的学习算法几乎是一样的，尽管我们使用这些算法训练的模型经历了变革，即简化了极深架构的训练。</p><h2 id="线性代数"><a href="#线性代数" class="headerlink" title="线性代数"></a>线性代数</h2><p>平方 $L^2$ 范数对 $x$ 中每个元素的导数只取决于对应的元素，而$L^2$ 范数对每个元素的导数却和整个向量相关。但是在很多情况下，平方$L^2$ 范数也可能不受欢迎，因为它在原点附近增长得十分缓慢。在某些机器学习应用中，区分恰好是零的元素和非零但值很小的元素是很重要的。在这些情况下，我们转而使用在各个位置斜率相同，同时保持简单的数学形式的函数： $L^1$ 范数。$L^1$ 范数可以简化如下：<br>$$\Vert x \Vert_1 = \displaystyle \sum_i \lvert x_i \rvert$$</p><p>有时候我们会统计向量中非零元素的个数来衡量向量的大小。有些作者将这种函数称为“ $L^0$ 范数’’，但是这个术语在数学意义上是不对的。向量的非零元素的数目不是范数，因为对向量缩放 $\alpha$ 倍不会改变该向量非零元素的数目。因此， $L^1$ 范数经常作为表示非零元素数目的替代函数。</p><p>另外一个经常在机器学习中出现的范数是 $L^1$ 范数，也被称为最大范数（max norm）。这个范数表示向量中具有最大幅值的元素的绝对值：</p><p>$$\Vert x \Vert_\infty = \max_i \lvert x_i \rvert$$</p><p>有时候我们可能也希望衡量矩阵的大小。在深度学习中，最常见的做法是使用Frobenius 范数（Frobenius norm），</p><p>$$\Vert A \Vert_F = \sqrt{\sum_{i,j} A_{i,j}^2} $$</p><p>在很多情况下，我们可以根据任意矩阵导出一些通用的机器学习算法；但通过将一些矩阵限制为对角矩阵，我们可以得到计算代价较低的（并且简明扼要的）算法。</p><p>当矩阵$A$的行数多于列数时，可能没有解。在这种情况下，通过伪逆得到的$x$使得$Ax$和$y$的欧几里得距离$\Vert Ax-y \Vert_2$最小。</p><h3 id="主成分分析的例子，综合运用线性代数的知识"><a href="#主成分分析的例子，综合运用线性代数的知识" class="headerlink" title="主成分分析的例子，综合运用线性代数的知识"></a>主成分分析的例子，综合运用线性代数的知识</h3><h2 id="概率与信息论"><a href="#概率与信息论" class="headerlink" title="概率与信息论"></a>概率与信息论</h2><h3 id="为什么要使用概率"><a href="#为什么要使用概率" class="headerlink" title="为什么要使用概率"></a>为什么要使用概率</h3><p>概率论是用于表示不确定性声明的数学框架。它不仅提供了量化不确定性的方法，也提供了用于导出新的不确定性声明（statement）的公理。在人工智能领域，概率论主要有两种用途。首先，概率法则告诉我们AI 系统如何推理，据此我们设计一些算法来计算或者估算由概率论导出的表达式。其次，我们可以用概率和统计从理论上分析我们提出的AI 系统的行为。</p><p>概率论使我们能够提出不确定的声明以及在不确定性存在的情况下进行推理，而信息论使我们能够量化概率分布中的不确定性总量。</p><p>不确定性有三种可能的来源：</p><ul><li>被建模系统内在的随机性。</li><li>不完全观测。</li><li>不完全建模。</li></ul><p>在很多情况下，使用一些简单而不确定的规则要比复杂而确定的规则更为实用，即使真正的规则是确定的并且我们建模的系统可以足够精确地容纳复杂的规则。例如，‘‘多数鸟儿都会飞’’ 这个简单的规则描述起来很简单很并且使用广泛，而正式的规则——‘‘除了那些还没学会飞翔的幼鸟，因为生病或是受伤而失去了飞翔能力的鸟，包括食火鸟(cassowary)、鸵鸟(ostrich)、几维(kiwi，一种新西兰产的无翼鸟)等不会飞的鸟类……以外，鸟儿会飞’’，很难应用、维护和沟通，即使经过这么多的努力，这个规则还是很脆弱而且容易失效。</p><p>尽管我们的确需要一种用以对不确定性进行表示和推理的方法，但是概率论并不能明显地提供我们在人工智能领域需要的所有工具。</p><p>我们用概率来表示一种<strong>信任度</strong>（degree of belief），其中1 表示非常肯定病人患有流感，而0 表示非常肯定病人没有流感。前面那种概率，直接与事件发生的频率相联系，被称为<strong>频率派概率</strong>（frequentist probability）；而后者，涉及到确定性水平，被称为<strong>贝叶斯概率</strong>（Bayesian probability）。</p><h3 id="随机变量"><a href="#随机变量" class="headerlink" title="随机变量"></a>随机变量</h3><p><strong>随机变量</strong>（random variable）是可以随机地取不同值的变量。我们通常用无格式字体(plain typeface) 中的小写字母来表示随机变量本身，而用手写体中的小写字母来表示随机变量能够取到的值。例如， $x_1$ 和 $x_2$ 都是随机变量 $\text{x}$ 可能的取值。对于向量值变量，我们会将随机变量写成$\text{x}$，它的一个可能取值为 $x$ 。就其本身而言，一个随机变量只是对可能的状态的描述；它必须伴随着一个<strong>概率分布</strong>来指定每个状态的可能性。</p><p>随机变量可以是离散的或者连续的。离散随机变量拥有有限或者可数无限多的状态。注意这些状态不一定非要是整数；它们也可能只是一些被命名的状态而没有数值。连续随机变量伴随着实数值。</p><p>这里需要注意的是，不要把条件概率和计算当采用某个动作后会发生什么相混淆。假定某个人说德语，那么他是德国人的条件概率是非常高的，但是如果随机选择的一个人会说德语，他的国籍不会因此而改变。计算一个行动的后果被称为干预查询（intervention query）。干预查询属于因果模型（causal modeling）的范畴，我们不会在本书中讨论。</p><p>通过组合一些简单的概率分布来定义新的概率分布也是很常见的。一种通用的组合方法是构造混合分布（mixture distribution）。</p><p>一个非常强大且常见的混合模型是<strong>高斯混合模型</strong>（Gaussian Mixture Model），它的组件$p(x \mid c= i)$是高斯分布。每个组件都有各自的参数，均值$\mu^{(i)}$ 和协方差矩阵 $\Sigma^{(i)}$ 。有一些混合可以有更多的限制。例如，协方差矩阵可以通过$\Sigma^{(i)}$ = $\Sigma, \forall i$的形式在组件之间共享参数。和单个高斯分布一样，高斯混合模型有时会限制每个组件的协方差矩阵为对角的或者各向同性的(标量乘以单位矩阵）。</p><p>除了均值和协方差以外，高斯混合模型的参数指明了给每个组件 $i$ 的先验概率（prior probability） $\alpha_i$ = $P(c = i)$ 。‘‘先验’’ 一词表明了在观测到 $\text{x}$ 之前传递给模型关于 $c$ 的信念。作为对比， $P(c \mid x)$ 是后验概率（posterior probability），因为它是在观测到x 之后进行计算的。高斯混合模型是概率密度的万能近似器（universal approximator），在这种意义下，任何平滑的概率密度都可以用具有足够多组件的高斯混合模型以任意精度来逼近。</p><p>sigmoid( $\sigma(x) = \frac{1}{1+\exp(-x)}$ ) 函数在变量取绝对值非常大的正值或负值时会出现饱和（saturate）现象，意味着函数会变得很平，并且对输入的微小改变会变得不敏感。</p><p>【62:图3.3: logistic sigmoid函数。】</p><p>另外一个经常遇到的函数是softplus ( $\zeta(x) = \log(1+\exp(x))$ )函数（softplus function）。softplus 函数名来源于它是另外一个函数的平滑（或‘‘软化’’）形式，这个函数是<code>x^+ = \max(0, x)</code></p><p>函数名“softplus’’ 提供了其他的正当理由。softplus 函数被设计成正部函数（positive part function）的平滑版本，这个正部函数是指$x^+ = \max { 0, x}$。与正部函数相对的是负部函数（negative part function）$x^- = \max{ 0, -x}$。为了获得类似负部函数的一个平滑函数，我们可以使用$\zeta(-x)$。就像$x$可以用它的正部和负部通过等式$x^+ - x^- = x$恢复一样，我们也可以用同样的方式对$\zeta(x)$和$\zeta(-x)$进行操作。</p><p>连续型随机变量和概率密度函数的深入理解需要用到数学分支测度论（measuretheory）的相关内容来扩展概率论。测度论超出了本书的范畴，但我们可以简要勾勒一些<strong>测度论</strong>用来解决的问题。</p><p>对于我们的目的，测度论更多的是用来描述那些适用于$\Set R^n$上的大多数点，却不适用于一些边界情况的定理。这种集合被称为“ <strong>零测度</strong>（measure zero）’’ 的。例如，在$\SetR^2$空间中，一条直线的测度为零，而填充的多边形具有正的测度。类似的，一个单独的点的测度为零。可数多个零测度集的并仍然是零测度的(所以所有有理数构成的集合测度为零)。</p><p>另外一个有用的测度论中的术语是“ 几乎处处（almost everywhere）’’。某个性质如果是几乎处处都成立的，那么它在整个空间中除了一个测度为零的集合以外都是成立的。因为这些例外只在空间中占有极其微小的量，它们在多数应用中都可以被放心地忽略。概率论中的一些重要结果对于离散值成立但对于连续值只能是‘‘几乎处处’’ 成立。</p><p>连续型随机变量的另一技术细节，涉及到处理那种相互之间有确定性函数关系的连续型变量。假设我们有两个随机变量 $x$ 和 $y$ 满足 $y = g(x)$ ，其中$g$是可逆的、连续可微的函数。可能有人会想$p_y(y) = p_x(g^{-1}(y))$。但实际上这并不对。（简单的例子没看懂）</p><p>在高维空间中，微分运算扩展为<strong>Jacobian 矩阵</strong>（Jacobian matrix）的行列式。</p><h3 id="信息论"><a href="#信息论" class="headerlink" title="信息论"></a>信息论</h3><p>信息论是应用数学的一个分支，主要研究的是对一个信号包含信息的多少进行量化。</p><p>信息论的基本想法是一个不太可能的事件居然发生了，要比一个非常可能的事件发生，能提供更多的信息。消息说：‘‘今天早上太阳升起’’ 信息量是如此之少以至于没有必要发送，但一条消息说：‘‘今天早上有日食’’ 信息量就很丰富。我们想要通过这种基本想法来量化信息。特别地，</p><ul><li>非常可能发生的事件信息量要比较少，并且极端情况下，确保能够发生的事件<br>应该没有信息量。</li><li>较不可能发生的事件具有更高的信息量。</li><li>独立事件应具有增量的信息。例如，投掷的硬币两次正面朝上传递的信息量，应该是投掷一次硬币正面朝上的信息量的两倍。</li></ul><p>自信息只处理单个的输出。我们可以用香农熵（Shannon entropy）来对整个概率分布中的不确定性总量进行量化：</p><p>如果我们对于同一个随机变量x 有两个单独的概率分布P(x) 和Q(x)，我们可以使用KL 散度（Kullback-Leibler (KL) divergence）来衡量这两个分布的差异</p><p>在离散型变量的情况下，KL 散度衡量的是，当我们使用一种被设计成能够使得概率分布 $Q$ 产生的消息的长度最小的编码，发送包含由概率分布 $P$ 产生的符号的消息时，所需要的额外信息量(如果我们使用底数为2 的对数时，信息量用比特衡量，但在机器学习中，我们通常用奈特和自然对数。)</p><p>一个和KL 散度密切联系的量是交叉熵（cross-entropy）。</p><h3 id="结构化概率模型"><a href="#结构化概率模型" class="headerlink" title="结构化概率模型"></a>结构化概率模型</h3><p>我们可以把概率分布分解成许多因子的乘积形式，而不是使用单一的函数来表示概率分布。</p><p>我们可以用图来描述这种分解。这里我们使用的是图论中的‘‘图’’ 的概念：由一些可以通过边互相连接的顶点的集合构成。当我们用图来表示这种概率分布的分解，我们把它称为<strong>结构化概率模型</strong>（structured probabilistic model）或者<strong>图模型</strong>（graphical model）。</p><p>请记住，这些图模型表示的分解仅仅是描述概率分布的一种语言。它们不是互相排斥的概率分布族。有向或者无向不是概率分布的特性；它是概率分布的一种特殊<strong>描述</strong>（description）所具有的特性，而任何概率分布都可以用这两种方式进行描述。</p><h3 id="数值计算"><a href="#数值计算" class="headerlink" title="数值计算"></a>数值计算</h3><p>机器学习算法通常需要大量的数值计算。这通常是指通过迭代过程更新解的估计值来解决数学问题的算法，而不是通过解析过程推导出公式来提供正确解的方法。常见的操作包括优化（找到最小化或最大化函数值的参数）和线性方程组的求解。对数字计算机来说实数无法在有限内存下精确表示，因此仅仅是计算涉及实数的函数也是困难的。</p><p>在大多数情况下，我们没有明确地对本书描述的各种算法所涉及的数值考虑进行详细说明。底层库的开发者在实现深度学习算法时应该牢记数值问题。本书的大多数读者可以简单地依赖保证数值稳定的底层库。</p><h3 id="病态条件"><a href="#病态条件" class="headerlink" title="病态条件"></a>病态条件</h3><p>条件数表征函数相对于输入的微小变化而变化的快慢程度。输入被轻微扰动而迅速改变的函数对于科学计算来说可能是有问题的，因为输入中的舍入误差可能导致输出的巨大变化。</p><h3 id="基于梯度的优化方法"><a href="#基于梯度的优化方法" class="headerlink" title="基于梯度的优化方法"></a>基于梯度的优化方法</h3><p>我们把要最小化或最大化的函数称为目标函数（objective function）或准则（criterion）。当我们对其进行最小化时，我们也把它称为代价函数（cost function）、损失函数（loss function）或误差函数（error function）。虽然有些机器学习著作赋予这些名称特殊的意义，但在这本书中我们交替使用这些术语。</p><p>这个函数的导数（derivative），它表明如何缩放输入的小变化才能在输出获得相应的变化。因此导数对于最小化一个函数很有用，因为它告诉我们如何更改 $x$ 来略微地改善 $y$ 。我们可以将 $x$ 往导数的反方向移动一小步来减小 $f(x)$ 。这种技术被称为梯度下降（gradient descent）。</p><p>仅使用梯度信息的优化算法被称为<strong>一阶优化算法</strong>(first-order optimization algorithms)，如梯度下降。使用Hessian 矩阵的优化算法被称为<strong>二阶最优化算法</strong>(second-order optimization algorithms)(Nocedal and Wright, 2006)，如牛顿法。</p><p>在本书大多数上下文中使用的优化算法适用于各种各样的函数，但几乎都没有保证。因为在深度学习中使用的函数族是相当复杂的，所以深度学习算法往往缺乏保证。在许多其他领域，优化的主要方法是为有限的函数族设计优化算法。</p><p>最成功的特定优化领域或许是<strong>凸优化</strong>（Convex optimization）。凸优化通过更强的限制提供更多的保证。凸优化算法只对凸函数适用，即Hessian 处处半正定的函数。因为这些函数没有鞍点而且其所有局部极小点必然是全局最小点，所以表现很好。然而，深度学习中的大多数问题都难以表示成凸优化的形式。凸优化仅用作一些深度学习算法的子程序。凸优化中的分析思路对证明深度学习算法的收敛性非常有用，然而一般来说，深度学习背景下凸优化的重要性大大减少。有关凸优化的详细信息，详见Boyd and Vandenberghe (2004) 或Rockafellar (1997)。</p><h3 id="约束优化"><a href="#约束优化" class="headerlink" title="约束优化"></a>约束优化</h3><p>有时候，在$x$的所有可能值下最大化或最小化一个函数$f(x)$不是我们所希望的。相反，我们可能希望在$x$的某些集合$\Set S$中找$f(x)$的最大值或最小值。这被称为约束优化。在约束优化术语中，集合$\Set S$内的点$x$被称为可行点。</p><p>Karush–Kuhn–Tucker（KKT）方法2是针对约束优化非常通用的解决方案。</p><h3 id="实例：线性最小二乘"><a href="#实例：线性最小二乘" class="headerlink" title="实例：线性最小二乘"></a>实例：线性最小二乘</h3><h2 id="机器学习基础"><a href="#机器学习基础" class="headerlink" title="机器学习基础"></a>机器学习基础</h2><h3 id="学习算法"><a href="#学习算法" class="headerlink" title="学习算法"></a>学习算法</h3><p>大部分机器学习算法都有超参数（必须在学习算法外设定）；我们将探讨如何使用额外的数据设置超参数。机器学习本质上属于应用统计学，更多地关注于如何用计算机统计地估计复杂函数，不太关注为这些函数提供置信区间。</p><p>‘‘学习’’ 是什么意思呢？Mitchell (1997) 提供了一个简洁的定义：‘‘对于某类任务 $T$ 和性能度量 $P$ ，一个计算机程序被认为可以从经验 $E$ 中学习是指，通过经验 $E$ 改进后，它在任务 $T$ 上由性能度量 $P$ 衡量的性能有所提升。”</p><h4 id="任务-T"><a href="#任务-T" class="headerlink" title="任务 $T$"></a>任务 $T$</h4><p>通常机器学习任务定义为机器学习系统应该如何处理样本（example）。样本是指我们从某些希望机器学习系统处理的对象或事件中收集到的已经量化的特征（feature）的集合。</p><p>一些非常常见的机器学习任务列举如下</p><ul><li>分类。识别人脸。</li><li>输入缺失分类。当一些输入可能丢失时，学习算法必须学习一组函数，而不是单个分类函数。使用 $n$ 个输入变量，我们现在可以获得每个可能的缺失输入集合所需的所有 $2^n$ 个不同的分类函数。</li><li>回归。在这类任务中，计算机程序需要对给定输入预测数值。预测投保人的索赔金额（用于设置保险费），或者预测证券未来的价格。这类预测也用在算法交易中。</li><li>转录。机器学习系统观测一些相对非结构化表示的数据，并转录信息为离散的文本形式。</li><li>机器翻译。在机器翻译任务中，输入是一种语言的符号序列，计算机程序必须将其转化成另一种语言的符号序列。这通常适用于自然语言，如将英语译成法语。</li><li>结构化输出。这类任务被称为结构化输出任务是因为输出值之间内部紧密相关。例如，为图片添加标题的程序输出的单词必须组合成一个通顺的句子。</li><li>异常检测：在这类任务中，计算机程序在一组事件或对象中筛选，并标记不正常或非典型的个体。信用卡公司可以检测到你的卡是否被滥用。</li><li>合成和采样：在这类任务中，机器学习程序生成一些和训练数据相似的新样本。</li><li>缺失值填补。</li><li>去噪。</li><li>密度估计或概率质量函数估计。</li></ul><h4 id="性能度量-P"><a href="#性能度量-P" class="headerlink" title="性能度量 $P$"></a>性能度量 $P$</h4><p>对于诸如分类、缺失输入分类和转录任务，我们通常度量模型的准确率（accuracy）。准确率是指该模型输出正确结果的样本比率。我们也可以通过错误率（errorrate）得到相同的信息。</p><p>通常，我们会更加关注机器学习算法在未观测数据上的性能如何，因为这将决定其在实际应用中的性能。因此，我们使用测试集（test set）数据来评估系统性能，将其与训练机器学习系统的训练集数据分开。</p><h4 id="经验-E"><a href="#经验-E" class="headerlink" title="经验 $E$"></a>经验 $E$</h4><p>强化学习（reinforcement learning）算法会和环境进行交互，所以学习系统和它的训练过程会有反馈回路。这类算法超出了本书的范畴。请参考Sutton and Barto (1998) 或Bertsekasand Tsitsiklis (1996) 了解强化学习相关知识，Mnih et al. (2013) 介绍了强化学习方向的深度学习方法。</p><h4 id="线性回归"><a href="#线性回归" class="headerlink" title="线性回归"></a>线性回归</h4><h3 id="容量、过拟合和欠拟合"><a href="#容量、过拟合和欠拟合" class="headerlink" title="容量、过拟合和欠拟合"></a>容量、过拟合和欠拟合</h3><p>机器学习的主要挑战是我们的算法必须能够在先前未观测的新输入上表现良好，而不只是在训练集上表现良好。在先前未观测到的输入上表现良好的能力被称为<strong>泛化</strong>（generalization）。</p><p>通常情况下，当我们训练机器学习模型时，我们可以使用某个训练集，在训练集上计算一些被称为<strong>训练误差</strong>（training error）的度量误差，目标是降低训练误差。目前为止，我们讨论的是一个简单的优化问题。机器学习和优化不同的地方在于，我们也希望<strong>泛化误差</strong>（generalization error）（也被称为测试误差（test error））很低。泛化误差被定义为新输入的误差期望。这里，期望的计算基于不同的可能输入，这些输入采自于系统在现实中遇到的分布。</p><p>统计学习理论（statistical learning theory）提供了一些答案。</p><p>训练集和测试集数据通过数据集上被称为数据生成过程（data generating process）的概率分布生成。通常，我们会做一系列被统称为独立同分布假设（i.i.d.assumption）的假设。该假设是说，每个数据集中的样本都是彼此相互独立的（in-dependent），并且训练集和测试集是同分布的（identically distributed），采样自相同的分布。</p><p>我们将这个共享的潜在分布称为<strong>数据生成分布</strong>（data generating distribution）。</p><p>以下是决定机器学习算法效果是否好的因素：</p><ul><li>降低训练误差。</li><li>缩小训练误差和测试误差的差距。</li></ul><p>这两个因素对应机器学习的两个主要挑战： 欠拟合（underfitting）和过拟合（overfitting）。</p><p>通过调整模型的<strong>容量</strong>（capacity），我们可以控制模型是否偏向于过拟合或者欠拟合。</p><p>一种控制训练算法容量的方法是选择<strong>假设空间</strong>（hypothesis space），即学习算法可以选择为解决方案的函数集。例如，线性回归算法将关于其输入的所有线性函数作为假设空间。广义线性回归的假设空间包括多项式函数，而非仅有线性函数。这样做就增加了模型的容量。</p><p>模型规定了调整参数降低训练目标时，学习算法可以从哪些函数族中选择函数。这被称为模型的<strong>表示容量</strong>（representational capacity）。在很多情况下，从这些函数中挑选出最优函数是非常困难的优化问题。实际中，学习算法不会真的找到最优函数，而仅是找到一个可以大大降低训练误差的函数。额外的限制因素，比如优化算法的不完美，意味着学习算法的<strong>有效容量</strong>（effective capacity）可能小于模型族的表示容量。</p><p>奥卡姆剃刀该原则指出，在同样能够解释已知观测现象的假设中，我们应该挑选‘‘最简单’’ 的那一个。</p><blockquote><p>初入机器学习领域的同学都知道机器学习中有一个普适的定理：没有免费的午餐(no free lunch)。<br>对它的简单易懂的解释就是：<br>1、一种算法（算法A）在特定数据集上的表现优于另一种算法（算法B）的同时，一定伴随着算法A在另外某一个特定的数据集上有着不如算法B的表现；<br>2、具体问题（机器学习领域内问题）具体分析（具体的机器学习算法选择）。</p></blockquote><p>统计学习理论提供了量化模型容量的不同方法。在这些中，最有名的是<strong>Vapnik-Chervonenkis 维度</strong>（Vapnik-Chervonenkis dimension, VC）。VC维度量二元分类器的容量。VC维定义为该分类器能够分类的训练样本的最大数目。假设存在m 个不同x 点的训练集，分类器可以任意地标记该m 个不同的x 点，VC维被定义为m的最大可能值。</p><p>统计学习理论中最重要的结论阐述了训练误差和泛化误差之间差异的上界随着模型容量增长而增长，但随着训练样本增多而下降。一部分原因是边界太松，另一部分原因是很难确定深度学习算法的容量。由于有效容量受限于优化算法的能力，确定深度学习模型容量的问题特别困难。而且对于深度学习中的一般非凸优化问题，我们只有很少的理论分析。</p><h4 id="没有免费午餐定理"><a href="#没有免费午餐定理" class="headerlink" title="没有免费午餐定理"></a>没有免费午餐定理</h4><p>这意味着机器学习研究的目标不是找一个通用学习算法或是绝对最好的学习算法。反之，我们的目标是理解什么样的分布与人工智能获取经验的‘‘真实世界’’ 相关，什么样的学习算法在我们关注的数据生成分布上效果最好。</p><h4 id="正则化"><a href="#正则化" class="headerlink" title="正则化"></a>正则化</h4><p>至此，我们具体讨论修改学习算法的方法只有，通过增加或减少学习算法可选假设空间的函数来增加或减少模型的表示容量。</p><p>算法的效果不仅很大程度上受影响于假设空间的函数数量，也取决于这些函数的具体形式。</p><p>因此我们可以通过两种方式控制算法的性能，一是允许使用的函数种类，二是这些函数的数量。</p><p>在假设空间中，相比于某一个学习算法，我们可能更偏好另一个学习算法。这意味着两个函数都是符合条件的，但是我们更偏好其中一个。只有非偏好函数比偏好函数在训练数据集上效果明显好很多时，我们才会考虑非偏好函数。</p><p>例如，我们可以加入权重衰减（weight decay）来修改线性回归的训练标准。带权重衰减的线性回归最小化训练集上的均方误差和正则项的和$J(w)$，其偏好于平方$L^2$范数较小的权重。</p><p>$$J(w) = \text{MSE}_{\text{train}} + \lambda w^\top w$$</p><p>最小化 $J(w)$ 可以看作是拟合训练数据和偏好小权重范数之间的权衡。这会使得解决方案的斜率较小，或是将权重放在较少的特征上。我们可以训练具有不同 $\lambda$ 值的高次多项式回归模型，来举例说明如何通过权重衰减控制模型欠拟合或过拟合的趋势。</p><p>更一般地，正则化一个学习函数$f(x;\theta)$的模型，我们可以给代价函数添加被称为正则化项的惩罚。在权重衰减的例子中，正则化项是$\Omega(w) = w^\top w$。</p><p>表示对函数的偏好是比增减假设空间的成员函数更一般的控制模型容量的方法。我们可以将去掉假设空间中的某个函数看作是对不赞成这个函数的无限偏好。</p><p>在我们权重衰减的示例中，通过在最小化的目标中额外增加一项，我们明确地表示了偏好权重较小的线性函数。有很多其他方法隐式或显式地表示对不同解的偏好。总而言之，这些不同的方法都被称为正则化（regularization）。正则化是指我们修改学习算法，使其降低泛化误差而非训练误差。正则化是机器学习领域的中心问题之一，只有优化能够与其重要性相媲。</p><p>没有免费午餐定理已经清楚地阐述了没有最优的学习算法，特别地，没有最优的正则化形式。反之，我们必须挑选一个非常适合于我们所要解决的任务的正则形式。深度学习中普遍的（特别是本书中的）理念是大量任务（例如所有人类能做的智能任务）也许都可以使用非常通用的正则化形式来有效解决。</p><h3 id="超参数和验证集"><a href="#超参数和验证集" class="headerlink" title="超参数和验证集"></a>超参数和验证集</h3><p>大多数机器学习算法都有超参数，可以设置来控制算法行为。超参数的值不是通过学习算法本身学习出来的（尽管我们可以设计一个嵌套的学习过程，一个学习算法为另一个学习算法学出最优超参数）。</p><p>有时一个选项被设为学习算法不用学习的超参数，是因为它太难优化了。更多的情况是，该选项必须是超参数，因为它不适合在训练集上学习。这适用于控制模型容量的所有超参数。如果在训练集上学习超参数，这些超参数总是趋向于最大可能的模型容量，导致过拟合（参考图5.3 ）。例如，相比低次多项式和正的权重衰减设定，更高次的多项式和权重衰减参数设定 $\lambda$ = 0 总能在训练集上更好地拟合。</p><p>为了解决这个问题，我们需要一个训练算法观测不到的<strong>验证集</strong>（validation set）样本。</p><p>用于挑选超参数的数据子集被称为验证集（validation set）。通常，80% 的训练数据用于训练，20% 用于验证。</p><h4 id="交叉验证"><a href="#交叉验证" class="headerlink" title="交叉验证"></a>交叉验证</h4><p>$k$ -折交叉验证过程,带来的一个问题是不存在平均误差方差的无偏估计(Bengio and Grandvalet, 2004)，但是我们通常会使用近似来解决。</p><h3 id="估计、偏差和方差"><a href="#估计、偏差和方差" class="headerlink" title="估计、偏差和方差"></a>估计、偏差和方差</h3><h4 id="点估计"><a href="#点估计" class="headerlink" title="点估计"></a>点估计</h4><p>点估计试图为一些感兴趣的量提供单个‘‘最优’’ 预测。一般地，感兴趣的量可以是单个参数，或是某些参数模型中的一个向量参数，例如第5.1.4 节线性回归中的权重，但是也有可能是整个函数。</p><p>令${x^{(1)},\dots,x^{(m)}}$是$m$个独立同分布（i.i.d.）的数据点。点估计（point esti-<br>mator）或统计量（statistics）是这些数据的任意函数：</p><p>$$ \hat{\theta}_m = g(x^{(1)}, \dots, x^{(m)})$$</p><p>点估计也可以指输入和目标变量之间关系的估计。我们将这种类型的点估计称为函数估计。</p><p>pp.108</p></div><footer class="post-footer"><div class="post-tags"><a href="/tags/%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0/" rel="tag"># 深度学习</a> <a href="/tags/%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C/" rel="tag"># 神经网络</a></div><div class="post-nav"><div class="post-nav-item"><a href="/2023/03/30/ai/" rel="prev" title="ai"><i class="fa fa-chevron-left"></i> ai</a></div><div class="post-nav-item"><a href="/2023/04/17/AI-topic/" rel="next" title="AI-topic">AI-topic <i class="fa fa-chevron-right"></i></a></div></div></footer></article></div><div class="comments" id="gitalk-container"></div><script>window.addEventListener("tabs:register",()=>{let t=CONFIG.comments["activeClass"];var e;(t=CONFIG.comments.storage?localStorage.getItem("comments_active")||t:t)&&(e=document.querySelector(`a[href="#comment-${t}"]`))&&e.click()}),CONFIG.comments.storage&&window.addEventListener("tabs:click",t=>{t.target.matches(".tabs-comment .tab-content .tab-pane")&&(t=t.target.classList[1],localStorage.setItem("comments_active",t))})</script></div><div class="toggle sidebar-toggle"><span class="toggle-line toggle-line-first"></span> <span class="toggle-line toggle-line-middle"></span> <span class="toggle-line toggle-line-last"></span></div><aside class="sidebar"><div class="sidebar-inner"><ul class="sidebar-nav motion-element"><li class="sidebar-nav-toc">文章目录</li><li class="sidebar-nav-overview">站点概览</li></ul><div class="post-toc-wrap sidebar-panel"><div class="post-toc motion-element"><ol class="nav"><li class="nav-item nav-level-1"><a class="nav-link" href="#%E9%98%85%E8%AF%BB%E8%8A%B1%E4%B9%A6"><span class="nav-text">阅读花书</span></a><ol class="nav-child"><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%AD%94%E7%96%91"><span class="nav-text">答疑</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#TODO"><span class="nav-text">TODO</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%9B%BE%E5%83%8F%E8%A1%A5%E5%85%85"><span class="nav-text">图像补充</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E5%BC%A0%E9%87%8F%E5%85%83%E7%B4%A0%E5%A6%82%E4%BD%95%E7%94%A8latex%E8%A1%A8%E7%A4%BA"><span class="nav-text">张量元素如何用latex表示</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%95%B4%E6%B5%81%E7%BA%BF%E6%80%A7%E5%8D%95%E5%85%83"><span class="nav-text">整流线性单元</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E8%B4%AA%E5%A9%AA%E9%80%90%E5%B1%82%E9%A2%84%E8%AE%AD%E7%BB%83"><span class="nav-text">贪婪逐层预训练</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E5%89%8D%E8%A8%80"><span class="nav-text">前言</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%B7%B1%E5%BA%A6%E5%AD%A6%E4%B9%A0%E7%9A%84%E5%8E%86%E5%8F%B2%E8%B6%8B%E5%8A%BF"><span class="nav-text">深度学习的历史趋势</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%A5%9E%E7%BB%8F%E7%BD%91%E7%BB%9C%E7%9A%84%E4%BC%97%E5%A4%9A%E5%90%8D%E7%A7%B0%E5%92%8C%E5%91%BD%E8%BF%90%E5%8F%98%E8%BF%81"><span class="nav-text">神经网络的众多名称和命运变迁</span></a></li></ol></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0"><span class="nav-text">线性代数</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BB%E6%88%90%E5%88%86%E5%88%86%E6%9E%90%E7%9A%84%E4%BE%8B%E5%AD%90%EF%BC%8C%E7%BB%BC%E5%90%88%E8%BF%90%E7%94%A8%E7%BA%BF%E6%80%A7%E4%BB%A3%E6%95%B0%E7%9A%84%E7%9F%A5%E8%AF%86"><span class="nav-text">主成分分析的例子，综合运用线性代数的知识</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%A6%82%E7%8E%87%E4%B8%8E%E4%BF%A1%E6%81%AF%E8%AE%BA"><span class="nav-text">概率与信息论</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%B8%BA%E4%BB%80%E4%B9%88%E8%A6%81%E4%BD%BF%E7%94%A8%E6%A6%82%E7%8E%87"><span class="nav-text">为什么要使用概率</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E9%9A%8F%E6%9C%BA%E5%8F%98%E9%87%8F"><span class="nav-text">随机变量</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BF%A1%E6%81%AF%E8%AE%BA"><span class="nav-text">信息论</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BB%93%E6%9E%84%E5%8C%96%E6%A6%82%E7%8E%87%E6%A8%A1%E5%9E%8B"><span class="nav-text">结构化概率模型</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E6%95%B0%E5%80%BC%E8%AE%A1%E7%AE%97"><span class="nav-text">数值计算</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%97%85%E6%80%81%E6%9D%A1%E4%BB%B6"><span class="nav-text">病态条件</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%9F%BA%E4%BA%8E%E6%A2%AF%E5%BA%A6%E7%9A%84%E4%BC%98%E5%8C%96%E6%96%B9%E6%B3%95"><span class="nav-text">基于梯度的优化方法</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E7%BA%A6%E6%9D%9F%E4%BC%98%E5%8C%96"><span class="nav-text">约束优化</span></a></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%9E%E4%BE%8B%EF%BC%9A%E7%BA%BF%E6%80%A7%E6%9C%80%E5%B0%8F%E4%BA%8C%E4%B9%98"><span class="nav-text">实例：线性最小二乘</span></a></li></ol></li><li class="nav-item nav-level-2"><a class="nav-link" href="#%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%9F%BA%E7%A1%80"><span class="nav-text">机器学习基础</span></a><ol class="nav-child"><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AD%A6%E4%B9%A0%E7%AE%97%E6%B3%95"><span class="nav-text">学习算法</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BB%BB%E5%8A%A1-T"><span class="nav-text">任务 $T$</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%80%A7%E8%83%BD%E5%BA%A6%E9%87%8F-P"><span class="nav-text">性能度量 $P$</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BB%8F%E9%AA%8C-E"><span class="nav-text">经验 $E$</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%BA%BF%E6%80%A7%E5%9B%9E%E5%BD%92"><span class="nav-text">线性回归</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E5%AE%B9%E9%87%8F%E3%80%81%E8%BF%87%E6%8B%9F%E5%90%88%E5%92%8C%E6%AC%A0%E6%8B%9F%E5%90%88"><span class="nav-text">容量、过拟合和欠拟合</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%B2%A1%E6%9C%89%E5%85%8D%E8%B4%B9%E5%8D%88%E9%A4%90%E5%AE%9A%E7%90%86"><span class="nav-text">没有免费午餐定理</span></a></li><li class="nav-item nav-level-4"><a class="nav-link" href="#%E6%AD%A3%E5%88%99%E5%8C%96"><span class="nav-text">正则化</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E8%B6%85%E5%8F%82%E6%95%B0%E5%92%8C%E9%AA%8C%E8%AF%81%E9%9B%86"><span class="nav-text">超参数和验证集</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E4%BA%A4%E5%8F%89%E9%AA%8C%E8%AF%81"><span class="nav-text">交叉验证</span></a></li></ol></li><li class="nav-item nav-level-3"><a class="nav-link" href="#%E4%BC%B0%E8%AE%A1%E3%80%81%E5%81%8F%E5%B7%AE%E5%92%8C%E6%96%B9%E5%B7%AE"><span class="nav-text">估计、偏差和方差</span></a><ol class="nav-child"><li class="nav-item nav-level-4"><a class="nav-link" href="#%E7%82%B9%E4%BC%B0%E8%AE%A1"><span class="nav-text">点估计</span></a></li></ol></li></ol></li></ol></li></ol></div></div><div class="site-overview-wrap sidebar-panel"><div class="site-author motion-element" itemprop="author" itemscope itemtype="http://schema.org/Person"><p class="site-author-name" itemprop="name">ednow</p><div class="site-description" itemprop="description"></div></div><div class="site-state-wrap motion-element"><nav class="site-state"><div class="site-state-item site-state-posts"><a href="/archives/"><span class="site-state-item-count">323</span> <span class="site-state-item-name">日志</span></a></div><div class="site-state-item site-state-categories"><a href="/categories/"><span class="site-state-item-count">1</span> <span class="site-state-item-name">分类</span></a></div><div class="site-state-item site-state-tags"><a href="/tags/"><span class="site-state-item-count">60</span> <span class="site-state-item-name">标签</span></a></div></nav></div></div></div></aside><div id="sidebar-dimmer"></div></div></main><footer class="footer"><div class="footer-inner"><div class="copyright">&copy; <span itemprop="copyrightYear">2024</span> <span class="with-love"><i class="fa fa-heart"></i> </span><span class="author" itemprop="copyrightHolder">ednow</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-chart-area"></i> </span><span class="post-meta-item-text">站点总字数：</span> <span title="站点总字数">7.5m</span> <span class="post-meta-divider">|</span> <span class="post-meta-item-icon"><i class="fa fa-coffee"></i> </span><span class="post-meta-item-text">站点阅读时长 &asymp;</span> <span title="站点阅读时长">112:55</span></div><div class="powered-by">由 <a href="https://hexo.io/" class="theme-link" rel="noopener" target="_blank">Hexo</a> & <a href="https://muse.theme-next.org/" class="theme-link" rel="noopener" target="_blank">NexT.Muse</a> 强力驱动</div><div class="busuanzi-count"><script async src="https://busuanzi.ibruce.info/busuanzi/2.3/busuanzi.pure.mini.js"></script><span class="post-meta-item" id="busuanzi_container_site_uv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-user"></i> </span><span class="site-uv" title="总访客量"><span id="busuanzi_value_site_uv"></span> </span></span><span class="post-meta-divider">|</span> <span class="post-meta-item" id="busuanzi_container_site_pv" style="display:none"><span class="post-meta-item-icon"><i class="fa fa-eye"></i> </span><span class="site-pv" title="总访问量"><span id="busuanzi_value_site_pv"></span></span></span></div></div></footer></div><script src="/lib/anime.min.js"></script><script src="/lib/velocity/velocity.min.js"></script><script src="/lib/velocity/velocity.ui.min.js"></script><script src="/js/utils.js"></script><script src="/js/motion.js"></script><script src="/js/schemes/muse.js"></script><script src="/js/next-boot.js"></script><script src="/js/local-search.js"></script><link rel="stylesheet" href="//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.css"><script>NexT.utils.loadComments(document.querySelector("#gitalk-container"),()=>{NexT.utils.getScript("//cdn.jsdelivr.net/npm/gitalk@1/dist/gitalk.min.js",()=>{new Gitalk({clientID:"04b9fe1c5636beb4acc4",clientSecret:"8ccb8829887eac219a8fdb018878fd0cf088a7ac",repo:"gittalk-comment",owner:"ednow",admin:["ednow"],id:"9ede13741c8683e2eea1fa3d0cec8520",language:"zh-CN",distractionFreeMode:!0}).render("gitalk-container")},window.Gitalk)})</script><style>[bg-lazy]{background-image:none!important;background-color:#eee!important}</style><script>window.imageLazyLoadSetting={isSPA:!1,preloadRatio:1,processImages:null}</script><script>window.addEventListener("load",function(){var t=/\.(gif|jpg|jpeg|tiff|png)$/i,r=/^data:image\/[a-z]+;base64,/;Array.prototype.slice.call(document.querySelectorAll("img[data-original]")).forEach(function(a){var e=a.parentNode;"A"===e.tagName&&(e.href.match(t)||e.href.match(r))&&(e.href=a.dataset.original)})})</script><script>!function(r){r.imageLazyLoadSetting.processImages=t;var e=r.imageLazyLoadSetting.isSPA,n=r.imageLazyLoadSetting.preloadRatio||1,c=a();function a(){var t=Array.prototype.slice.call(document.querySelectorAll("img[data-original]")),e=Array.prototype.slice.call(document.querySelectorAll("[bg-lazy]"));return t.concat(e)}function t(){e&&(c=a());for(var t,o=0;o<c.length;o++)0<=(t=(t=c[o]).getBoundingClientRect()).bottom&&0<=t.left&&t.top<=(r.innerHeight*n||document.documentElement.clientHeight*n)&&function(){var t,e,n,a=c[o],i=function(){c=c.filter(function(t){return a!==t}),r.imageLazyLoadSetting.onImageLoaded&&r.imageLazyLoadSetting.onImageLoaded(a)};(t=a).hasAttribute("bg-lazy")?(t.removeAttribute("bg-lazy"),i()):(e=new Image,n=t.getAttribute("data-original"),e.onload=function(){t.src=n,t.removeAttribute("data-original"),i()},t.src!==n&&(e.src=n))}()}function i(){clearTimeout(t.tId),t.tId=setTimeout(t,500)}t(),document.addEventListener("scroll",i),r.addEventListener("resize",i),r.addEventListener("orientationchange",i)}(this)</script></body></html>